"""
AI Fusionå·¥å…·å‡½æ•°å’Œé…ç½®ç®¡ç†
åŒ…å«LLMè°ƒç”¨ã€æ¨¡å‹é…ç½®ã€ç¯å¢ƒéªŒè¯ç­‰å®ç”¨åŠŸèƒ½
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from openai import AsyncOpenAI
from dotenv import load_dotenv

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

@dataclass
class ModelConfig:
    """LLMæ¨¡å‹é…ç½®ç±»"""
    name: str
    provider: str  # openai, anthropic, google, etc.
    api_key: str
    base_url: Optional[str] = None
    max_tokens: int = 2000
    temperature: float = 0.7

def get_available_models() -> List[ModelConfig]:
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„LLMæ¨¡å‹é…ç½®
    æ ¹æ®ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥ç¡®å®šå¯ç”¨æ¨¡å‹
    """
    models = []
    
    # OpenAIæ¨¡å‹ï¼ˆæ”¯æŒè‡ªå®šä¹‰base_urlï¼‰
    openai_key = os.environ.get("OPENAI_API_KEY")
    openai_base_url = os.environ.get("OPENAI_BASE_URL")
    
    if openai_key:
        # æ ¹æ®é”™è¯¯ä¿¡æ¯ä¸­å…è®¸çš„æ¨¡å‹åˆ—è¡¨æ·»åŠ æ‰€æœ‰å¯ç”¨æ¨¡å‹
        allowed_models = [
            "qwen-max", "qwen-plus", "claude_sonnet4", "gpt-41-0414-global", 
            "claude37_sonnet_new", "gpt-41-mini-0414-global", "glm-4.5", 
            "openmatrix-qwen3-235b-inst-fp8", "qwen3-max-preview", 
            "gpt-5-mini-0807-global", "qwen3-coder-480b-a35b-instruct", 
            "qwen3-coder-plus1", "qwen3-coder-plus"
        ]
        
        for model_name in allowed_models:
            models.append(ModelConfig(
                name=model_name,
                provider="openai",
                api_key=openai_key,
                base_url=openai_base_url,
                max_tokens=2000
            ))
    
    
    return models

async def call_llm_async(
    messages: List[Dict[str, str]], 
    model: str,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    max_tokens: int = 2000,
    temperature: float = 0.7
) -> str:
    """
    å¼‚æ­¥è°ƒç”¨LLMçš„é€šç”¨å‡½æ•°
    ç»Ÿä¸€ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨æ‰€æœ‰æ¨¡å‹ï¼ˆåŒ…æ‹¬Claudeç­‰ï¼‰
    """
    
    # ç»Ÿä¸€ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨æ‰€æœ‰æ¨¡å‹
    client = AsyncOpenAI(
        api_key=api_key,
        base_url=base_url
    )
    
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        raise Exception(f"è°ƒç”¨æ¨¡å‹ {model} å¤±è´¥: {str(e)}")


def validate_environment() -> bool:
    """
    éªŒè¯ç¯å¢ƒé…ç½®
    æ£€æŸ¥æ˜¯å¦é…ç½®äº†OpenAI APIå¯†é’¥
    """
    openai_key = os.environ.get("OPENAI_API_KEY")
    
    if not openai_key:
        print("âŒ ç¯å¢ƒé…ç½®æ£€æŸ¥å¤±è´¥ï¼")
        print("è¯·é…ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š")
        print("- OPENAI_API_KEY: OpenAI APIå¯†é’¥")
        print("- OPENAI_BASE_URL: è‡ªå®šä¹‰OpenAIå…¼å®¹æœåŠ¡åœ°å€ï¼ˆå¯é€‰ï¼‰")
        return False
    
    available_models = get_available_models()
    print(f"âœ… ç¯å¢ƒéªŒè¯é€šè¿‡ï¼å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹:")
    for model in available_models:
        print(f"  - {model.name} ({model.provider})")
    
    return len(available_models) >= 1

def setup_example_env():
    """
    è®¾ç½®ç¤ºä¾‹ç¯å¢ƒå˜é‡ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
    å®é™…ä½¿ç”¨æ—¶è¯·è®¾ç½®çœŸå®çš„APIå¯†é’¥
    """
    example_env = {
        "OPENAI_API_KEY": "your-openai-api-key-here",
        "OPENAI_BASE_URL": "https://api.openai.com/v1",  # å¯é€‰ï¼Œè‡ªå®šä¹‰OpenAIå…¼å®¹æœåŠ¡åœ°å€
    }
    
    print("ğŸ“ ç¤ºä¾‹ç¯å¢ƒå˜é‡é…ç½®:")
    for key, value in example_env.items():
        print(f"export {key}='{value}'")
    print("\nè¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µè®¾ç½®ç›¸åº”çš„APIå¯†é’¥")

async def test_model_connection(model_config: ModelConfig) -> bool:
    """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„è¿æ¥"""
    try:
        test_messages = [{"role": "user", "content": "Hello, please respond with 'OK'"}]
        
        response = await call_llm_async(
            messages=test_messages,
            model=model_config.name,
            api_key=model_config.api_key,
            base_url=model_config.base_url,
            max_tokens=10,
            temperature=0.1
        )
        
        print(f"âœ… {model_config.name} è¿æ¥æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ {model_config.name} è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def test_all_models():
    """æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„è¿æ¥"""
    models = get_available_models()
    if not models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®")
        return
    
    print(f"ğŸ§ª å¼€å§‹æµ‹è¯• {len(models)} ä¸ªæ¨¡å‹çš„è¿æ¥...")
    
    tasks = [test_model_connection(model) for model in models]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    successful = sum(1 for result in results if result is True)
    print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ: {successful}/{len(models)} ä¸ªæ¨¡å‹å¯ç”¨")

if __name__ == "__main__":
    # è¿è¡Œç¯å¢ƒéªŒè¯å’Œæ¨¡å‹æµ‹è¯•
    if validate_environment():
        print("\nğŸ§ª è¿è¡Œæ¨¡å‹è¿æ¥æµ‹è¯•...")
        asyncio.run(test_all_models())
    else:
        setup_example_env()