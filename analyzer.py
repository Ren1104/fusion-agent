"""
AI Fusion åˆ†ææ¨¡å—
åŒ…å«æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œè´¨é‡åˆ†æåŠŸèƒ½
"""

import json
import re
import asyncio
import hashlib
from typing import List, Dict, Any, Optional
from dataclasses import dataclass


# ============================================
# å‘åå…¼å®¹: ModelConfig å®šä¹‰
# ============================================

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®(å‘åå…¼å®¹)"""
    name: str
    provider: str = "unknown"
    api_key: str = ""
    base_url: Optional[str] = None
    max_tokens: int = 2000
    temperature: float = 0.7


# ============================================
# LLM è°ƒç”¨å‡½æ•°(å…¼å®¹æ—§ä»£ç )
# ============================================

async def call_llm_async(
    messages,
    model,
    max_tokens=2000,
    temperature=0.7,
    registry=None,
    trace_id=None,
    return_response_obj: bool = False,
    parent_observation_id: Optional[str] = None,
    langfuse_metadata: Optional[Dict[str, Any]] = None,
    **kwargs
):
    """
    å…¼å®¹çš„ LLM è°ƒç”¨å‡½æ•°
    å®é™…è°ƒç”¨ providers.ModelRegistry

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹ID
        max_tokens: æœ€å¤§tokenæ•°
        temperature: æ¸©åº¦å‚æ•°
        registry: ModelRegistry å®ä¾‹ (å¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šåˆ›å»ºæ–°å®ä¾‹)
        trace_id: Langfuse trace ID (å¯é€‰)
        return_response_obj: æ˜¯å¦è¿”å›å®Œæ•´å“åº”å¯¹è±¡ï¼ˆåŒ…å« usage ç­‰ä¿¡æ¯ï¼‰
        parent_observation_id: Langfuse çˆ¶ span ID
        langfuse_metadata: é™„åŠ çš„ Langfuse å…ƒæ•°æ®
        **kwargs: å…¶ä»–å‚æ•°
    """
    from providers import ModelRegistry

    # å¦‚æœæ²¡æœ‰æä¾› registryï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ï¼ˆå‘åå…¼å®¹ï¼‰
    if registry is None:
        registry = ModelRegistry()
        await registry.discover_all_models()

    response = await registry.call_model(
        model,
        messages,
        max_tokens=max_tokens,
        temperature=temperature,
        trace_id=trace_id,
        parent_observation_id=parent_observation_id,
        langfuse_metadata=langfuse_metadata,
        **kwargs
    )

    if return_response_obj:
        return response
    return response.text




import json
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
# Imports handled in header


@dataclass
class ModelCapability:
    """æ¨¡å‹èƒ½åŠ›æè¿°"""
    name: str
    provider: str
    strengths: List[str]
    suitable_tasks: List[str]
    performance_profile: Dict[str, str]
    special_features: List[str]


class AIFusionSmartSelector:
    """AI Fusionæ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨"""

    def __init__(self, registry=None):
        self.analyzer_model = "claude_sonnet4"  # ç”¨äºåˆ†æçš„æ¨¡å‹
        self.model_knowledge = self._build_model_knowledge()
        self.registry = registry  # ModelRegistry å®ä¾‹
    
    def _build_model_knowledge(self) -> Dict[str, ModelCapability]:
        """æ„å»ºæ¨¡å‹çŸ¥è¯†åº“ - åŒ…å«æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„è¯¦ç»†èƒ½åŠ›æè¿°"""
        return {
            # ============= Claude ç³»åˆ— =============
            "claude_sonnet4": ModelCapability(
                name="claude_sonnet4",
                provider="anthropic",
                strengths=[
                    "é€»è¾‘æ¨ç†èƒ½åŠ›å“è¶Š", "ä»£ç ç†è§£å’Œç”Ÿæˆé¡¶å°–", "å¤æ‚é—®é¢˜æ·±åº¦åˆ†æ",
                    "åˆ›æ„å†™ä½œä¼˜ç§€", "å¤šæ­¥éª¤ä»»åŠ¡å¤„ç†", "ç»“æ„åŒ–è¾“å‡ºç²¾å‡†",
                    "é•¿æ–‡æœ¬ç†è§£", "ç»†è‡´çš„ä¸Šä¸‹æ–‡æŠŠæ¡"
                ],
                suitable_tasks=[
                    "ç¼–ç¨‹å’ŒæŠ€æœ¯é—®é¢˜", "é€»è¾‘æ¨ç†", "åˆ›æ„å†™ä½œ", "å¤æ‚åˆ†æ",
                    "å­¦æœ¯ç ”ç©¶", "äº§å“è®¾è®¡", "ç­–ç•¥è§„åˆ’", "ä»£ç å®¡æŸ¥",
                    "ç³»ç»Ÿæ¶æ„è®¾è®¡", "æŠ€æœ¯æ–‡æ¡£ç¼–å†™"
                ],
                performance_profile={
                    "reasoning": "excellent",      # æ¨ç†èƒ½åŠ›ï¼šå“è¶Š
                    "creativity": "excellent",     # åˆ›é€ åŠ›ï¼šå“è¶Š
                    "coding": "excellent",         # ç¼–ç¨‹èƒ½åŠ›ï¼šå“è¶Š
                    "factual": "good",             # äº‹å®å‡†ç¡®æ€§ï¼šè‰¯å¥½
                    "speed": "medium",             # å“åº”é€Ÿåº¦ï¼šä¸­ç­‰
                    "context": "excellent"         # ä¸Šä¸‹æ–‡ç†è§£ï¼šå“è¶Š
                },
                special_features=["æ”¯æŒ200K+é•¿æ–‡æœ¬", "å¼ºé€»è¾‘æ¨ç†é“¾", "åˆ›æ„æ€§å¼º", "é“å¾·å®‰å…¨æ„è¯†é«˜"]
            ),

            "claude37_sonnet_new": ModelCapability(
                name="claude37_sonnet_new",
                provider="anthropic",
                strengths=[
                    "å¹³è¡¡çš„ç»¼åˆèƒ½åŠ›", "å¿«é€Ÿå“åº”", "æ—¥å¸¸å¯¹è¯æµç•…",
                    "ä¿¡æ¯æ•´ç†æ¸…æ™°", "ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡", "ç¨³å®šæ€§å¥½"
                ],
                suitable_tasks=[
                    "æ—¥å¸¸é—®ç­”", "ä¿¡æ¯æ€»ç»“", "ç¿»è¯‘", "ç®€å•åˆ†æ",
                    "æ–‡æ¡£æ•´ç†", "åŸºç¡€ç¼–ç¨‹", "é€šç”¨ä»»åŠ¡", "å®¢æœå¯¹è¯",
                    "å†…å®¹æ¶¦è‰²", "å¿«é€ŸåŸå‹å¼€å‘"
                ],
                performance_profile={
                    "reasoning": "good",
                    "creativity": "good",
                    "coding": "good",
                    "factual": "good",
                    "speed": "fast",
                    "context": "good"
                },
                special_features=["å“åº”å¿«é€Ÿ", "ç¨³å®šå¯é ", "å¹³è¡¡å‘å±•", "æˆæœ¬æ•ˆç›Šå¥½"]
            ),

            # ============= GPT ç³»åˆ— =============
            "gpt-41-0414-global": ModelCapability(
                name="gpt-41-0414-global",
                provider="openai",
                strengths=[
                    "æ•°å­¦å’Œç§‘å­¦è®¡ç®—å¼º", "é€»è¾‘æ¨ç†ä¸¥è°¨", "ç»“æ„åŒ–åˆ†æç²¾ç¡®",
                    "ä»£ç ä¼˜åŒ–èƒ½åŠ›", "æŠ€æœ¯æ–‡æ¡£å¤„ç†", "å¤šè¯­è¨€æ”¯æŒ",
                    "å·¥å…·è°ƒç”¨èƒ½åŠ›", "å‡½æ•°è°ƒç”¨ç²¾å‡†"
                ],
                suitable_tasks=[
                    "æ•°å­¦é—®é¢˜", "ç§‘å­¦è®¡ç®—", "ç®—æ³•è®¾è®¡", "æ•°æ®åˆ†æ",
                    "æŠ€æœ¯æ–‡æ¡£", "ç³»ç»Ÿè®¾è®¡", "å·¥ç¨‹é—®é¢˜", "APIå¼€å‘",
                    "æ•°æ®å¯è§†åŒ–", "ç»Ÿè®¡åˆ†æ"
                ],
                performance_profile={
                    "reasoning": "excellent",
                    "creativity": "good",
                    "coding": "excellent",
                    "factual": "excellent",
                    "speed": "medium",
                    "context": "excellent"
                },
                special_features=["æ•°å­¦èƒ½åŠ›å¼º", "é€»è¾‘ä¸¥è°¨", "æŠ€æœ¯å¯¼å‘", "128Kä¸Šä¸‹æ–‡", "å·¥å…·ä½¿ç”¨"]
            ),

            "gpt-41-mini-0414-global": ModelCapability(
                name="gpt-41-mini-0414-global",
                provider="openai",
                strengths=[
                    "æé€Ÿå“åº”", "è½»é‡çº§ä»»åŠ¡", "æ—¥å¸¸å¯¹è¯",
                    "ç®€å•é—®ç­”é«˜æ•ˆ", "æˆæœ¬ä¼˜åŒ–", "æ‰¹é‡å¤„ç†é€‚åˆ"
                ],
                suitable_tasks=[
                    "å¿«é€Ÿé—®ç­”", "ç®€å•ç¿»è¯‘", "åŸºç¡€å¯¹è¯", "ä¿¡æ¯æŸ¥æ‰¾",
                    "è½»é‡çº§ç¼–ç¨‹", "æ ¼å¼è½¬æ¢", "æ•ˆç‡ä»»åŠ¡", "æ‰¹é‡å¤„ç†",
                    "å®æ—¶å¯¹è¯", "å¿«é€ŸåŸå‹"
                ],
                performance_profile={
                    "reasoning": "medium",
                    "creativity": "medium",
                    "coding": "good",
                    "factual": "good",
                    "speed": "very_fast",
                    "context": "good"
                },
                special_features=["æé€Ÿå“åº”", "èµ„æºèŠ‚çº¦", "ç®€æ´é«˜æ•ˆ", "æ€§ä»·æ¯”æé«˜"]
            ),

            "gpt-5-mini-0807-global": ModelCapability(
                name="gpt-5-mini-0807-global",
                provider="openai",
                strengths=[
                    "æ–°ä¸€ä»£è½»é‡æ¨¡å‹", "é€Ÿåº¦ä¸è´¨é‡å¹³è¡¡", "æ”¹è¿›çš„æ¨ç†èƒ½åŠ›",
                    "æ›´å¥½çš„æŒ‡ä»¤éµå¾ª", "å¤šä»»åŠ¡å¤„ç†", "æˆæœ¬æ•ˆç›Šä¼˜"
                ],
                suitable_tasks=[
                    "é€šç”¨é—®ç­”", "æ–‡æœ¬ç”Ÿæˆ", "ç®€å•ç¼–ç¨‹", "ä¿¡æ¯æå–",
                    "å¯¹è¯ç³»ç»Ÿ", "å†…å®¹å®¡æ ¸", "åˆ†ç±»ä»»åŠ¡", "æ‘˜è¦ç”Ÿæˆ",
                    "æƒ…æ„Ÿåˆ†æ", "å®ä½“è¯†åˆ«"
                ],
                performance_profile={
                    "reasoning": "good",
                    "creativity": "good",
                    "coding": "good",
                    "factual": "good",
                    "speed": "very_fast",
                    "context": "good"
                },
                special_features=["GPT-5ç³»åˆ—", "é€Ÿåº¦å¿«", "è´¨é‡æå‡", "æŒ‡ä»¤éµå¾ªå¥½"]
            ),

            # ============= é€šä¹‰åƒé—®ç³»åˆ— =============
            "qwen-max": ModelCapability(
                name="qwen-max",
                provider="alibaba",
                strengths=[
                    "ä¸­æ–‡ç†è§£é¡¶å°–", "çŸ¥è¯†è¦†ç›–å¹¿", "å¤šè¯­è¨€æ”¯æŒå¼º",
                    "æ–‡åŒ–è¯­å¢ƒç†è§£æ·±", "æœ¬åœŸåŒ–å†…å®¹ç²¾å‡†", "æ¨ç†èƒ½åŠ›å¼º",
                    "é•¿æ–‡æœ¬å¤„ç†", "ä¸“ä¸šé¢†åŸŸçŸ¥è¯†"
                ],
                suitable_tasks=[
                    "ä¸­æ–‡å†…å®¹å¤„ç†", "ç¿»è¯‘ä»»åŠ¡", "æ–‡åŒ–ç›¸å…³é—®é¢˜",
                    "æœ¬åœŸåŒ–å†…å®¹", "å¤šè¯­è¨€å¯¹è¯", "çŸ¥è¯†é—®ç­”",
                    "ä¸“ä¸šæ–‡æ¡£", "å­¦æœ¯ç ”ç©¶", "å•†ä¸šåˆ†æ"
                ],
                performance_profile={
                    "reasoning": "excellent",
                    "creativity": "good",
                    "coding": "good",
                    "factual": "excellent",
                    "speed": "medium",
                    "context": "excellent"
                },
                special_features=["ä¸­æ–‡ä¼˜åŒ–", "çŸ¥è¯†ä¸°å¯Œ", "æ–‡åŒ–ç†è§£", "32Kä¸Šä¸‹æ–‡"]
            ),

            "qwen-plus": ModelCapability(
                name="qwen-plus",
                provider="alibaba",
                strengths=[
                    "ä¸­æ–‡å¤„ç†ä¼˜ç§€", "å¹³è¡¡æ€§èƒ½å¥½", "å¤šé¢†åŸŸçŸ¥è¯†",
                    "å®ç”¨æ€§å¼º", "æ€§ä»·æ¯”é«˜", "å“åº”ç¨³å®š"
                ],
                suitable_tasks=[
                    "ä¸­æ–‡é—®ç­”", "é€šç”¨ä»»åŠ¡", "çŸ¥è¯†æ•´ç†",
                    "å®ç”¨å·¥å…·", "æ—¥å¸¸åŠ©æ‰‹", "ä¿¡æ¯å¤„ç†",
                    "å†…å®¹ç”Ÿæˆ", "æ–‡æœ¬åˆ†æ"
                ],
                performance_profile={
                    "reasoning": "good",
                    "creativity": "good",
                    "coding": "good",
                    "factual": "good",
                    "speed": "fast",
                    "context": "good"
                },
                special_features=["ä¸­æ–‡å‹å¥½", "å®ç”¨å¯¼å‘", "æ€§ä»·æ¯”é«˜", "ç¨³å®šå¯é "]
            ),

            "qwen3-max-preview": ModelCapability(
                name="qwen3-max-preview",
                provider="alibaba",
                strengths=[
                    "ç¬¬ä¸‰ä»£æ¶æ„", "æ¨ç†èƒ½åŠ›å¢å¼º", "å¤šæ¨¡æ€ç†è§£",
                    "ä»£ç èƒ½åŠ›æå‡", "çŸ¥è¯†æ›´æ–°", "é•¿æ–‡æœ¬ä¼˜åŒ–"
                ],
                suitable_tasks=[
                    "å¤æ‚æ¨ç†", "ä»£ç ç”Ÿæˆ", "å¤šæ¨¡æ€åˆ†æ",
                    "é•¿æ–‡æ¡£å¤„ç†", "ä¸“ä¸šé—®ç­”", "åˆ›æ–°è®¾è®¡",
                    "æŠ€æœ¯ç ”ç©¶", "æ•°æ®åˆ†æ"
                ],
                performance_profile={
                    "reasoning": "excellent",
                    "creativity": "excellent",
                    "coding": "excellent",
                    "factual": "excellent",
                    "speed": "medium",
                    "context": "excellent"
                },
                special_features=["Qwen3æ¶æ„", "å¤šæ¨¡æ€", "é•¿æ–‡æœ¬", "æœ€æ–°æŠ€æœ¯"]
            ),

            # ============= æ™ºè°±AIç³»åˆ— =============
            "glm-4.5": ModelCapability(
                name="glm-4.5",
                provider="zhipu",
                strengths=[
                    "å¤šæ¨¡æ€èƒ½åŠ›å¼º", "è§†è§‰ç†è§£å¥½", "ç»¼åˆåˆ†æèƒ½åŠ›",
                    "åˆ›æ–°æ€§æ€ç»´", "ä¸­æ–‡ä¼˜åŒ–", "è·¨åª’ä½“å¤„ç†"
                ],
                suitable_tasks=[
                    "å¤šæ¨¡æ€ä»»åŠ¡", "åˆ›æ–°è®¾è®¡", "ç»¼åˆåˆ†æ",
                    "å›¾æ–‡ç†è§£", "è·¨é¢†åŸŸé—®é¢˜", "åˆ›æ„é¡¹ç›®",
                    "è§†è§‰é—®ç­”", "å†…å®¹åˆ›ä½œ"
                ],
                performance_profile={
                    "reasoning": "good",
                    "creativity": "excellent",
                    "coding": "good",
                    "factual": "good",
                    "speed": "medium",
                    "context": "good"
                },
                special_features=["å¤šæ¨¡æ€", "åˆ›æ–°æ€ç»´", "ç»¼åˆèƒ½åŠ›", "ä¸­æ–‡ä¼˜ç§€"]
            ),

            # ============= Qwen3-Coderç³»åˆ— (ä»£ç ä¸“ç²¾) =============
            "qwen3-coder-480b-a35b-instruct": ModelCapability(
                name="qwen3-coder-480b-a35b-instruct",
                provider="alibaba",
                strengths=[
                    "ä»£ç ç”Ÿæˆé¡¶å°–", "ç®—æ³•ç†è§£æ·±", "å¤šè¯­è¨€ç¼–ç¨‹",
                    "ä»£ç è¡¥å…¨ç²¾å‡†", "bugä¿®å¤èƒ½åŠ›", "æ¶æ„è®¾è®¡",
                    "480Bå‚æ•°è§„æ¨¡", "æŒ‡ä»¤éµå¾ªå¥½"
                ],
                suitable_tasks=[
                    "ä»£ç ç”Ÿæˆ", "ç®—æ³•å®ç°", "ä»£ç å®¡æŸ¥", "bugä¿®å¤",
                    "é‡æ„ä¼˜åŒ–", "æŠ€æœ¯æ–‡æ¡£", "APIè®¾è®¡", "æµ‹è¯•ç¼–å†™",
                    "æ€§èƒ½ä¼˜åŒ–", "æ¶æ„è®¾è®¡"
                ],
                performance_profile={
                    "reasoning": "excellent",
                    "creativity": "good",
                    "coding": "outstanding",     # ç¼–ç¨‹èƒ½åŠ›ï¼šæ°å‡º
                    "factual": "excellent",
                    "speed": "medium",
                    "context": "excellent"
                },
                special_features=["480Bè¶…å¤§è§„æ¨¡", "ä»£ç ä¸“ç²¾", "å¤šè¯­è¨€", "æŒ‡ä»¤éµå¾ª"]
            ),

            "qwen3-coder-plus1": ModelCapability(
                name="qwen3-coder-plus1",
                provider="alibaba",
                strengths=[
                    "ä»£ç ç”Ÿæˆä¼˜ç§€", "ç¼–ç¨‹æ•ˆç‡é«˜", "å¤šè¯­è¨€æ”¯æŒ",
                    "å¿«é€Ÿå¼€å‘", "å®ç”¨å¯¼å‘", "æ€§ä»·æ¯”å¥½"
                ],
                suitable_tasks=[
                    "å¿«é€Ÿå¼€å‘", "ä»£ç è¡¥å…¨", "ç®€å•é‡æ„", "è„šæœ¬ç¼–å†™",
                    "å·¥å…·å¼€å‘", "è‡ªåŠ¨åŒ–ä»»åŠ¡", "åŸå‹å¼€å‘", "ä»£ç è½¬æ¢"
                ],
                performance_profile={
                    "reasoning": "good",
                    "creativity": "medium",
                    "coding": "excellent",
                    "factual": "good",
                    "speed": "fast",
                    "context": "good"
                },
                special_features=["ä»£ç ä¼˜åŒ–", "å¿«é€Ÿå“åº”", "å®ç”¨æ€§å¼º", "æ€§ä»·æ¯”é«˜"]
            ),

            "qwen3-coder-plus": ModelCapability(
                name="qwen3-coder-plus",
                provider="alibaba",
                strengths=[
                    "ä»£ç èƒ½åŠ›å¼º", "å¹³è¡¡æ€§èƒ½", "å¤šåœºæ™¯é€‚ç”¨",
                    "å¼€å‘æ•ˆç‡", "ç¨³å®šå¯é "
                ],
                suitable_tasks=[
                    "é€šç”¨ç¼–ç¨‹", "ä»£ç ç”Ÿæˆ", "æŠ€æœ¯é—®ç­”", "å¼€å‘è¾…åŠ©",
                    "ä»£ç è§£é‡Š", "å­¦ä¹ è¾…å¯¼", "ä»£ç ä¼˜åŒ–"
                ],
                performance_profile={
                    "reasoning": "good",
                    "creativity": "medium",
                    "coding": "excellent",
                    "factual": "good",
                    "speed": "fast",
                    "context": "good"
                },
                special_features=["ä»£ç èƒ½åŠ›", "å¹³è¡¡å‘å±•", "ç¨³å®šæ€§å¥½", "å®ç”¨"]
            ),

            # ============= OpenMatrixç³»åˆ— (å¼€æºç”Ÿæ€) =============
            "openmatrix-qwen3-235b-inst-fp8": ModelCapability(
                name="openmatrix-qwen3-235b-inst-fp8",
                provider="openmatrix",
                strengths=[
                    "è¶…å¤§è§„æ¨¡235B", "æŒ‡ä»¤éµå¾ªç²¾å‡†", "FP8ä¼˜åŒ–",
                    "æ¨ç†æ•ˆç‡é«˜", "çŸ¥è¯†å¹¿åš", "å¤šä»»åŠ¡èƒ½åŠ›"
                ],
                suitable_tasks=[
                    "å¤æ‚æ¨ç†", "ä¸“ä¸šé—®ç­”", "æ·±åº¦åˆ†æ", "çŸ¥è¯†æ•´åˆ",
                    "å¤šæ­¥éª¤ä»»åŠ¡", "ç³»ç»Ÿè®¾è®¡", "ç ”ç©¶è¾…åŠ©", "åˆ›æ–°æ€è€ƒ"
                ],
                performance_profile={
                    "reasoning": "excellent",
                    "creativity": "good",
                    "coding": "good",
                    "factual": "excellent",
                    "speed": "medium",
                    "context": "excellent"
                },
                special_features=["235Bè§„æ¨¡", "FP8é‡åŒ–", "å¼€æºç”Ÿæ€", "é«˜æ•ˆæ¨ç†"]
            )
        }
    
    async def intelligent_model_selection(
        self,
        question: str,
        available_models: List[ModelConfig],
        trace_id: Optional[str] = None,
        parent_observation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ¨¡å‹é€‰æ‹©
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            available_models: å¯ç”¨æ¨¡å‹åˆ—è¡¨
            
        Returns:
            é€‰æ‹©ç»“æœåŒ…å«æ¨èæ¨¡å‹å’Œåˆ†æç†ç”±
        """
        print("ğŸ§  æ­£åœ¨è¿›è¡Œæ™ºèƒ½æ¨¡å‹åˆ†æ...")
        
        # 1. æ„å»ºæ¨¡å‹çŸ¥è¯†æç¤º
        model_descriptions = self._build_model_descriptions(available_models)
        
        # 2. åˆ›å»ºåˆ†ææç¤º
        analysis_prompt = self._create_analysis_prompt(question, model_descriptions)
        
        # 3. LLMåˆ†æå’Œæ¨è
        try:
            response = await call_llm_async(
                messages=[{"role": "user", "content": analysis_prompt}],
                model=self.analyzer_model,
                max_tokens=1500,
                temperature=0.3,
                registry=self.registry,  # ä¼ é€’ registry
                trace_id=trace_id,
                parent_observation_id=parent_observation_id,
                langfuse_metadata={
                    "component": "model_selector",
                    "stage": "intelligent_selection"
                },
            )
            
            # 4. è§£ææ¨èç»“æœ
            recommendation = self._parse_recommendation(response, available_models)
            
            return recommendation
            
        except Exception as e:
            print(f"âš ï¸ æ™ºèƒ½é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥: {str(e)}")
            return self._fallback_selection(question, available_models)
    
    def _build_model_descriptions(self, available_models: List[ModelConfig]) -> str:
        """æ„å»ºå¯ç”¨æ¨¡å‹çš„æè¿°"""
        descriptions = []
        
        for model in available_models:
            model_name = model.name
            if model_name in self.model_knowledge:
                capability = self.model_knowledge[model_name]
                
                desc = f"""
**{model_name}**:
- æ ¸å¿ƒä¼˜åŠ¿: {', '.join(capability.strengths)}
- é€‚åˆä»»åŠ¡: {', '.join(capability.suitable_tasks)}
- æ€§èƒ½ç‰¹ç‚¹: æ¨ç†èƒ½åŠ›{capability.performance_profile['reasoning']}, åˆ›é€ åŠ›{capability.performance_profile['creativity']}, ç¼–ç¨‹èƒ½åŠ›{capability.performance_profile['coding']}, äº‹å®å‡†ç¡®æ€§{capability.performance_profile['factual']}, å“åº”é€Ÿåº¦{capability.performance_profile['speed']}
- ç‰¹æ®ŠåŠŸèƒ½: {', '.join(capability.special_features)}
"""
                descriptions.append(desc)
            else:
                # å¯¹äºæœªçŸ¥æ¨¡å‹ï¼Œæä¾›åŸºæœ¬ä¿¡æ¯
                descriptions.append(f"**{model_name}**: é€šç”¨AIæ¨¡å‹ï¼Œå…·å¤‡åŸºç¡€çš„é—®ç­”å’Œåˆ†æèƒ½åŠ›")
        
        return "\n".join(descriptions)
    
    def _create_analysis_prompt(self, question: str, model_descriptions: str) -> str:
        """åˆ›å»ºåˆ†ææç¤º"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ¨¡å‹é€‰æ‹©ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜å¹¶ä»å¯ç”¨æ¨¡å‹ä¸­æ¨èæœ€é€‚åˆçš„3ä¸ªæ¨¡å‹ç»„åˆã€‚

ç”¨æˆ·é—®é¢˜ï¼š
{question}

å¯ç”¨æ¨¡å‹åŠå…¶èƒ½åŠ›ï¼š
{model_descriptions}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š

1. **é—®é¢˜åˆ†æ**ï¼šåˆ†æé—®é¢˜çš„ç±»å‹ã€å¤æ‚åº¦ã€æ‰€éœ€èƒ½åŠ›
2. **éœ€æ±‚åŒ¹é…**ï¼šç¡®å®šè§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦ä»€ä¹ˆæ ·çš„AIèƒ½åŠ›
3. **æ¨¡å‹è¯„ä¼°**ï¼šè¯„ä¼°æ¯ä¸ªæ¨¡å‹åœ¨è¿™ä¸ªé—®é¢˜ä¸Šçš„é€‚åˆåº¦
4. **ç»„åˆæ¨è**ï¼šé€‰æ‹©3ä¸ªæœ€é€‚åˆçš„æ¨¡å‹ï¼Œè€ƒè™‘èƒ½åŠ›äº’è¡¥

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š

```json
{{
    "problem_analysis": {{
        "question_type": "é—®é¢˜ç±»å‹",
        "complexity_level": "å¤æ‚åº¦ç­‰çº§(ç®€å•/ä¸­ç­‰/å¤æ‚)",
        "required_capabilities": ["æ‰€éœ€èƒ½åŠ›1", "æ‰€éœ€èƒ½åŠ›2", "æ‰€éœ€èƒ½åŠ›3"],
        "key_challenges": ["ä¸»è¦æŒ‘æˆ˜1", "ä¸»è¦æŒ‘æˆ˜2"]
    }},
    "recommended_models": [
        {{
            "model_name": "æ¨¡å‹åç§°",
            "rank": 1,
            "suitability_score": 9.5,
            "reasons": ["é€‰æ‹©ç†ç”±1", "é€‰æ‹©ç†ç”±2"],
            "expected_contribution": "é¢„æœŸè´¡çŒ®"
        }},
        {{
            "model_name": "æ¨¡å‹åç§°", 
            "rank": 2,
            "suitability_score": 8.8,
            "reasons": ["é€‰æ‹©ç†ç”±1", "é€‰æ‹©ç†ç”±2"],
            "expected_contribution": "é¢„æœŸè´¡çŒ®"
        }},
        {{
            "model_name": "æ¨¡å‹åç§°",
            "rank": 3, 
            "suitability_score": 8.2,
            "reasons": ["é€‰æ‹©ç†ç”±1", "é€‰æ‹©ç†ç”±2"],
            "expected_contribution": "é¢„æœŸè´¡çŒ®"
        }}
    ],
    "combination_strategy": "ç»„åˆç­–ç•¥è¯´æ˜",
    "confidence_level": "é«˜/ä¸­/ä½"
}}
```

æ³¨æ„ï¼š
- åªä»æä¾›çš„å¯ç”¨æ¨¡å‹ä¸­é€‰æ‹©
- ä¼˜å…ˆè€ƒè™‘èƒ½åŠ›äº’è¡¥çš„ç»„åˆ
- ç¡®ä¿æ¨èçš„æ¨¡å‹åç§°å®Œå…¨åŒ¹é…å¯ç”¨æ¨¡å‹åˆ—è¡¨
- é€‚åˆåº¦è¯„åˆ†èŒƒå›´ä¸º0-10åˆ†
"""
    
    def _parse_recommendation(
        self, 
        response: str, 
        available_models: List[ModelConfig]
    ) -> Dict[str, Any]:
        """è§£æLLMæ¨èç»“æœ"""
        
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                recommendation = json.loads(json_str)
                
                # éªŒè¯æ¨èçš„æ¨¡å‹æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
                available_model_names = [m.name for m in available_models]
                valid_models = []
                
                for rec_model in recommendation.get('recommended_models', []):
                    model_name = rec_model.get('model_name', '')
                    if model_name in available_model_names:
                        valid_models.append(rec_model)
                
                # å¦‚æœæœ‰æ•ˆæ¨¡å‹å°‘äº3ä¸ªï¼Œç”¨å›é€€ç­–ç•¥è¡¥å……
                if len(valid_models) < 3:
                    print(f"âš ï¸ æ™ºèƒ½æ¨èçš„æ¨¡å‹æ•°é‡ä¸è¶³({len(valid_models)})ï¼Œä½¿ç”¨å›é€€ç­–ç•¥è¡¥å……")
                    fallback = self._fallback_selection("", available_models)
                    
                    # è¡¥å……æ¨¡å‹
                    existing_names = [m['model_name'] for m in valid_models]
                    for model_name in fallback['selected_models'][:3]:
                        if model_name not in existing_names and len(valid_models) < 3:
                            valid_models.append({
                                'model_name': model_name,
                                'rank': len(valid_models) + 1,
                                'suitability_score': 7.0,
                                'reasons': ['å›é€€é€‰æ‹©'],
                                'expected_contribution': 'æä¾›å¤‡é€‰æ–¹æ¡ˆ'
                            })
                
                recommendation['recommended_models'] = valid_models[:3]
                recommendation['selected_models'] = [m['model_name'] for m in valid_models[:3]]
                recommendation['analysis_method'] = 'intelligent_llm'
                
                return recommendation
                
        except Exception as e:
            print(f"âš ï¸ è§£ææ¨èç»“æœå¤±è´¥: {str(e)}")
        
        # è§£æå¤±è´¥æ—¶ä½¿ç”¨å›é€€ç­–ç•¥
        return self._fallback_selection("", available_models)
    
    def _fallback_selection(
        self,
        question: str,
        available_models: List[ModelConfig]
    ) -> Dict[str, Any]:
        """å›é€€é€‰æ‹©ç­–ç•¥ - åŸºäºæ¨¡å‹èƒ½åŠ›çš„ä¼˜å…ˆçº§æ’åº"""

        # ä¼˜å…ˆçº§ç­–ç•¥ï¼šç»¼åˆèƒ½åŠ› > ä¸“ç²¾èƒ½åŠ› > è½»é‡æ¨¡å‹
        priority_order = [
            # é¡¶çº§ç»¼åˆèƒ½åŠ›æ¨¡å‹
            "claude_sonnet4", "qwen3-max-preview", "gpt-41-0414-global",
            # ä»£ç ä¸“ç²¾æ¨¡å‹
            "qwen3-coder-480b-a35b-instruct",
            # ä¼˜ç§€ç»¼åˆæ¨¡å‹
            "qwen-max", "openmatrix-qwen3-235b-inst-fp8",
            # å¹³è¡¡æ€§èƒ½æ¨¡å‹
            "claude37_sonnet_new", "qwen-plus", "qwen3-coder-plus1",
            # å¤šæ¨¡æ€å’Œåˆ›æ–°æ¨¡å‹
            "glm-4.5",
            # å¿«é€Ÿè½»é‡æ¨¡å‹
            "gpt-5-mini-0807-global", "gpt-41-mini-0414-global",
            "qwen3-coder-plus"
        ]
        
        available_names = [m.name for m in available_models]
        selected = []
        
        # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©
        for model_name in priority_order:
            if model_name in available_names and len(selected) < 3:
                selected.append(model_name)
        
        # å¦‚æœä¸è¶³3ä¸ªï¼Œä»å‰©ä½™æ¨¡å‹ä¸­é€‰æ‹©
        while len(selected) < 3 and len(selected) < len(available_names):
            for model in available_models:
                if model.name not in selected:
                    selected.append(model.name)
                    break
        
        return {
            'problem_analysis': {
                'question_type': 'é€šç”¨é—®é¢˜',
                'complexity_level': 'ä¸­ç­‰',
                'required_capabilities': ['ç»¼åˆåˆ†æ', 'å‡†ç¡®å›ç­”'],
                'key_challenges': ['ä¿¡æ¯æ•´åˆ', 'é€»è¾‘æ¨ç†']
            },
            'recommended_models': [
                {
                    'model_name': model_name,
                    'rank': i + 1,
                    'suitability_score': 8.0 - i * 0.5,
                    'reasons': ['å›é€€ç­–ç•¥é€‰æ‹©'],
                    'expected_contribution': f'æä¾›{["ä¸»è¦", "è¾…åŠ©", "è¡¥å……"][i]}è§‚ç‚¹'
                }
                for i, model_name in enumerate(selected[:3])
            ],
            'selected_models': selected[:3],
            'combination_strategy': 'åŸºäºæ¨¡å‹ä¼˜å…ˆçº§çš„å›é€€é€‰æ‹©',
            'confidence_level': 'ä¸­',
            'analysis_method': 'fallback'
        }


import re
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
# Imports handled in header
import hashlib
import json


@dataclass
class QualityMetrics:
    """è´¨é‡è¯„ä¼°æŒ‡æ ‡"""
    completeness_score: float  # å®Œæ•´æ€§è¯„åˆ† (0-10)
    accuracy_score: float      # å‡†ç¡®æ€§è¯„åˆ† (0-10)
    clarity_score: float       # æ¸…æ™°åº¦è¯„åˆ† (0-10)
    relevance_score: float     # ç›¸å…³æ€§è¯„åˆ† (0-10)
    overall_score: float       # ç»¼åˆè¯„åˆ† (0-10)
    word_count: int           # è¯æ•°
    sentence_count: int       # å¥æ•°
    readability_score: float  # å¯è¯»æ€§è¯„åˆ† (0-10)
    information_density: float # ä¿¡æ¯å¯†åº¦ (0-10)


class AIFusionQualityAnalyzer:
    """AI Fusionè´¨é‡åˆ†æå™¨"""

    def __init__(self, registry=None):
        self.evaluator_model = "claude_sonnet4"  # ç”¨äºè¯„ä¼°çš„æ¨¡å‹
        self.registry = registry  # ModelRegistry å®ä¾‹
        self._current_trace_id: Optional[str] = None
        self._current_parent_observation_id: Optional[str] = None
    
    async def analyze_quality(
        self,
        question: str,
        llm_responses: List[Dict],
        fusion_answer: str,
        trace_id: Optional[str] = None,
        parent_observation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        åˆ†æå›ç­”è´¨é‡
        
        Args:
            question: åŸå§‹é—®é¢˜
            llm_responses: å„æ¨¡å‹çš„å›ç­”æ•°æ®
            fusion_answer: èåˆåçš„å›ç­”
            
        Returns:
            è´¨é‡åˆ†æç»“æœ
        """
        print("ğŸ” å¼€å§‹è´¨é‡åˆ†æ...")

        self._current_trace_id = trace_id
        self._current_parent_observation_id = parent_observation_id

        try:
            # 1. è®¡ç®—åŸºç¡€æŒ‡æ ‡
            basic_metrics = {}
            for response in llm_responses:
                if response['success']:
                    basic_metrics[response['model_name']] = self._calculate_basic_metrics(
                        response['response']
                    )

            # èåˆå›ç­”çš„åŸºç¡€æŒ‡æ ‡
            basic_metrics['fusion_answer'] = self._calculate_basic_metrics(fusion_answer)

            # 2. LLMè¯„ä¼°ï¼ˆå¼‚æ­¥å¹¶å‘ï¼‰
            llm_evaluations = await self._evaluate_with_llm(
                question, llm_responses, fusion_answer
            )

            # 3. å†…å®¹è¯­ä¹‰åˆ†æï¼ˆå¢å¼ºå·®å¼‚åŒ–èƒ½åŠ›ï¼‰
            content_analysis = await self._perform_content_semantic_analysis(
                question, llm_responses, fusion_answer
            )

            # 4. å¯¹æ¯”åˆ†æï¼ˆå¢å¼ºï¼‰
            comparison_analysis = self._perform_enhanced_comparison_analysis(
                basic_metrics, llm_evaluations, content_analysis
            )

            # 5. é€»è¾‘ä¸€è‡´æ€§éªŒè¯
            consistency_check = self._perform_consistency_validation(
                llm_evaluations, comparison_analysis, content_analysis
            )

            # 6. è´¨é‡æ’åï¼ˆç»è¿‡ä¸€è‡´æ€§æ ¡æ­£ï¼‰
            quality_ranking = self._calculate_validated_quality_ranking(
                basic_metrics, llm_evaluations, consistency_check
            )

            # 7. èåˆæ•ˆæœé‡åŒ–åˆ†æï¼ˆæ–°å¢ï¼‰
            fusion_effectiveness = self._analyze_fusion_effectiveness(
                llm_evaluations, comparison_analysis, content_analysis
            )

            # 8. é€Ÿåº¦-è´¨é‡æƒè¡¡åˆ†æï¼ˆæ–°å¢ï¼‰
            speed_quality_tradeoff = self._analyze_speed_quality_tradeoff(
                llm_responses, llm_evaluations
            )

            return {
                'basic_metrics': basic_metrics,
                'llm_evaluations': llm_evaluations,
                'content_analysis': content_analysis,
                'comparison_analysis': comparison_analysis,
                'consistency_check': consistency_check,
                'quality_ranking': quality_ranking,
                'fusion_effectiveness': fusion_effectiveness,  # æ–°å¢èåˆæ•ˆæœåˆ†æ
                'speed_quality_tradeoff': speed_quality_tradeoff  # æ–°å¢é€Ÿåº¦è´¨é‡æƒè¡¡åˆ†æ
            }
        finally:
            # æ¸…ç†ä¸Šä¸‹æ–‡
            self._current_trace_id = None
            self._current_parent_observation_id = None
    
    def _calculate_basic_metrics(self, text: str) -> QualityMetrics:
        """è®¡ç®—åŸºç¡€è´¨é‡æŒ‡æ ‡"""
        if not text:
            return QualityMetrics(0, 0, 0, 0, 0, 0, 0, 0, 0)
        
        # å­—ç¬¦æ•°ç»Ÿè®¡ï¼ˆæ›´å‡†ç¡®çš„ä¸­æ–‡æ–‡æœ¬é•¿åº¦è®¡ç®—ï¼‰
        char_count = len(text)
        # ä¸­æ–‡è¯æ±‡ç»Ÿè®¡ï¼ˆæŒ‰å­—ç¬¦å’Œç©ºæ ¼åˆ†å‰²ï¼‰
        words = re.findall(r'[\w\u4e00-\u9fff]+', text)
        word_count = len(words)
        # å¥å­ç»Ÿè®¡ï¼ˆæ”¹è¿›çš„åˆ†å¥é€»è¾‘ï¼‰
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
        sentence_count = len([s for s in sentences if s.strip()])
        
        # å¯è¯»æ€§è¯„åˆ†ï¼ˆåŸºäºå¹³å‡å¥é•¿ï¼‰
        avg_sentence_length = word_count / max(sentence_count, 1)
        readability_score = max(0, min(10, 10 - (avg_sentence_length - 15) * 0.2))
        
        # ä¿¡æ¯å¯†åº¦ï¼ˆåŸºäºå†…å®¹å¤šæ ·æ€§ï¼‰
        unique_words = len(set(text.lower().split()))
        information_density = min(10, (unique_words / max(word_count, 1)) * 20)
        
        # åŸºç¡€è¯„åˆ†ï¼ˆä¼šåœ¨LLMè¯„ä¼°ä¸­æ›´æ–°ï¼‰
        return QualityMetrics(
            completeness_score=0,  # å¾…LLMè¯„ä¼°
            accuracy_score=0,      # å¾…LLMè¯„ä¼°
            clarity_score=readability_score,
            relevance_score=0,     # å¾…LLMè¯„ä¼°
            overall_score=0,       # å¾…è®¡ç®—
            word_count=char_count,  # ä½¿ç”¨å­—ç¬¦æ•°è€Œéè¯æ•°
            sentence_count=sentence_count,
            readability_score=readability_score,
            information_density=information_density
        )
    
    async def _evaluate_with_llm(
        self,
        question: str,
        llm_responses: List[Dict],
        fusion_answer: str
    ) -> Dict[str, QualityMetrics]:
        """ä½¿ç”¨LLMè¯„ä¼°å›ç­”è´¨é‡ï¼ˆé‡‡ç”¨å¯¹æ¯”è¯„åˆ†æœºåˆ¶ï¼‰"""

        # å‡†å¤‡è¯„ä¼°ä»»åŠ¡
        answer_sources = {}

        # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„å›ç­”
        for response in llm_responses:
            if response['success']:
                answer_sources[response['model_name']] = response['response']

        # æ·»åŠ èåˆå›ç­”
        answer_sources['fusion_answer'] = fusion_answer

        # **ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡å¯¹æ¯”è¯„åˆ†ï¼ˆæ–°å¢ï¼‰**
        # è¿™ä¼šè®©è¯„ä¼°å™¨ä¸€æ¬¡æ€§çœ‹åˆ°æ‰€æœ‰å›ç­”ï¼Œä»è€Œç»™å‡ºç›¸å¯¹è¯„åˆ†
        comparative_scores = await self._comparative_batch_evaluation(
            question, answer_sources
        )

        # **ç¬¬äºŒé˜¶æ®µï¼šå•ç‹¬è¯„ä¼°ï¼ˆä¿ç•™åŸæœ‰è¯¦ç»†è¯„ä¼°ï¼‰**
        evaluation_tasks = []
        source_names = []

        for source_name, answer_text in answer_sources.items():
            # ä¼ å…¥å¯¹æ¯”è¯„åˆ†ä½œä¸ºå‚è€ƒ
            base_score = comparative_scores.get(source_name, 7.0)
            evaluation_tasks.append(
                self._evaluate_single_answer(
                    question,
                    answer_text,
                    source_name,
                    base_reference_score=base_score  # ä¼ å…¥å‚è€ƒåˆ†
                )
            )
            source_names.append(source_name)

        # å¹¶å‘æ‰§è¡Œè¯¦ç»†è¯„ä¼°
        evaluation_results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)

        # æ•´ç†ç»“æœ
        llm_evaluations = {}
        for i, source_name in enumerate(source_names):
            if i < len(evaluation_results) and not isinstance(evaluation_results[i], Exception):
                llm_evaluations[source_name] = evaluation_results[i]
            else:
                # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                print(f"âš ï¸ {source_name} è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                llm_evaluations[source_name] = QualityMetrics(5, 5, 5, 5, 5, 0, 0, 5, 5)

        return llm_evaluations

    async def _comparative_batch_evaluation(
        self,
        question: str,
        answer_sources: Dict[str, str]
    ) -> Dict[str, float]:
        """
        æ‰¹é‡å¯¹æ¯”è¯„åˆ†ï¼šä¸€æ¬¡æ€§è¯„ä¼°æ‰€æœ‰å›ç­”ï¼Œç»™å‡ºç›¸å¯¹è¯„åˆ†
        è¿™èƒ½ç¡®ä¿è¯„åˆ†çš„åŒºåˆ†åº¦å’Œä¸€è‡´æ€§
        """
        print("ğŸ” æ­£åœ¨è¿›è¡Œæ‰¹é‡å¯¹æ¯”è¯„åˆ†...")

        # æ„å»ºå¯¹æ¯”è¯„åˆ†prompt
        comparative_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚ç°åœ¨æœ‰{len(answer_sources)}ä¸ªAIæ¨¡å‹å¯¹åŒä¸€é—®é¢˜ç»™å‡ºäº†å›ç­”ã€‚

**é—®é¢˜ï¼š**
{question}

**å„æ¨¡å‹å›ç­”ï¼š**
"""

        for i, (source_name, answer_text) in enumerate(answer_sources.items(), 1):
            # é™åˆ¶æ¯ä¸ªå›ç­”çš„é•¿åº¦ä»¥é¿å…promptè¿‡é•¿
            truncated_answer = answer_text[:500] + "..." if len(answer_text) > 500 else answer_text
            comparative_prompt += f"""
ã€å›ç­”{i}: {source_name}ã€‘
{truncated_answer}

"""

        comparative_prompt += """
**å¯¹æ¯”è¯„åˆ†ä»»åŠ¡ï¼š**

è¯·å¯¹æ‰€æœ‰å›ç­”è¿›è¡Œç›¸å¯¹è¯„åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼Œè¦æ±‚ï¼š

1. **å¿…é¡»æ‹‰å¼€è¯„åˆ†å·®è·**ï¼šæœ€é«˜åˆ†å’Œæœ€ä½åˆ†çš„å·®è·è‡³å°‘1.5åˆ†
2. **åŸºäºè´¨é‡æ’åº**ï¼šå…ˆåˆ¤æ–­å“ªä¸ªå›ç­”è´¨é‡æœ€å¥½ã€å“ªä¸ªæœ€å·®ã€å“ªä¸ªå±…ä¸­
3. **å‚è€ƒè¯„åˆ†åŒºé—´**ï¼š
   - æœ€ä¼˜ç§€çš„å›ç­”ï¼š7.5-9.0åˆ†
   - ä¸­ç­‰è´¨é‡çš„å›ç­”ï¼š6.0-7.5åˆ†
   - è¾ƒå·®çš„å›ç­”ï¼š4.5-6.0åˆ†

4. **è¯„åˆ†æ ‡å‡†**ï¼šç»¼åˆè€ƒè™‘å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€æ¸…æ™°åº¦ã€ç›¸å…³æ€§

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š**

```
å›ç­”1({æºåç§°}): X.Xåˆ† - [ä¸€å¥è¯è¯„ä»·]
å›ç­”2({æºåç§°}): X.Xåˆ† - [ä¸€å¥è¯è¯„ä»·]
å›ç­”3({æºåç§°}): X.Xåˆ† - [ä¸€å¥è¯è¯„ä»·]
...
```

**æ’åè¯´æ˜ï¼š**
[ç®€è¦è¯´æ˜ä¸ºä»€ä¹ˆè¿™æ ·æ’åºï¼Œå„å›ç­”çš„ä¸»è¦å·®å¼‚åœ¨å“ªé‡Œ]
"""

        try:
            response = await call_llm_async(
                messages=[{"role": "user", "content": comparative_prompt}],
                model=self.evaluator_model,
                max_tokens=1000,
                temperature=0.2,
                registry=self.registry,
                trace_id=self._current_trace_id,
                parent_observation_id=self._current_parent_observation_id,
                langfuse_metadata={
                    "component": "quality_analyzer",
                    "stage": "comparative_scoring"
                },
            )

            # è§£æå¯¹æ¯”è¯„åˆ†ç»“æœ
            comparative_scores = {}
            for line in response.split('\n'):
                # åŒ¹é…æ ¼å¼ï¼šå›ç­”X(æºåç§°): X.Xåˆ†
                for source_name in answer_sources.keys():
                    if source_name in line and ':' in line:
                        # å°è¯•æå–è¯„åˆ†
                        score_match = re.search(r'(\d+\.?\d*)åˆ†', line)
                        if score_match:
                            score = float(score_match.group(1))
                            comparative_scores[source_name] = max(0, min(10, score))
                            break

            # éªŒè¯æ˜¯å¦æ‰€æœ‰å›ç­”éƒ½æœ‰è¯„åˆ†
            for source_name in answer_sources.keys():
                if source_name not in comparative_scores:
                    print(f"âš ï¸ {source_name} æœªè·å¾—å¯¹æ¯”è¯„åˆ†ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    comparative_scores[source_name] = 7.0

            # éªŒè¯è¯„åˆ†åŒºåˆ†åº¦
            scores = list(comparative_scores.values())
            if len(scores) > 1:
                score_range = max(scores) - min(scores)
                if score_range < 1.0:
                    print(f"âš ï¸ å¯¹æ¯”è¯„åˆ†åŒºåˆ†åº¦ä¸è¶³({score_range:.1f}åˆ†)ï¼Œå°†è¿›è¡Œè°ƒæ•´")
                    # å¼ºåˆ¶æ‹‰å¼€å·®è·
                    sorted_items = sorted(comparative_scores.items(), key=lambda x: x[1], reverse=True)
                    for i, (name, _) in enumerate(sorted_items):
                        comparative_scores[name] = 8.5 - i * 0.8  # ä»8.5å¼€å§‹é€’å‡

            print(f"âœ… å¯¹æ¯”è¯„åˆ†å®Œæˆï¼Œè¯„åˆ†åŒºé—´: {min(scores):.1f} - {max(scores):.1f}")
            return comparative_scores

        except Exception as e:
            print(f"âš ï¸ å¯¹æ¯”è¯„åˆ†å¤±è´¥: {str(e)}, ä½¿ç”¨é»˜è®¤è¯„åˆ†")
            # è¿”å›é»˜è®¤è¯„åˆ†
            return {name: 7.0 for name in answer_sources.keys()}
    
    async def _evaluate_single_answer(
        self,
        question: str,
        answer: str,
        source_name: str,
        base_reference_score: float = 7.0
    ) -> QualityMetrics:
        """è¯„ä¼°å•ä¸ªå›ç­”çš„è´¨é‡"""

        evaluation_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„å†…å®¹è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè´Ÿè´£å¯¹AIæ¨¡å‹çš„å›ç­”è¿›è¡Œå®¢è§‚ã€å‡†ç¡®ã€æœ‰åŒºåˆ†åº¦çš„è¯„åˆ†ã€‚

åŸå§‹é—®é¢˜ï¼š
{question}

å¾…è¯„ä¼°å›ç­”ï¼ˆæ¥æº: {source_name}ï¼‰ï¼š
{answer}

**å‚è€ƒè¯„åˆ†ï¼š** é€šè¿‡å¯¹æ¯”è¯„ä¼°ï¼Œè¯¥å›ç­”çš„åˆæ­¥ç»¼åˆè´¨é‡ä¸º {base_reference_score:.1f}/10åˆ†ã€‚
è¯·åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›è¡Œæ›´ç»†è‡´çš„å„ç»´åº¦è¯„åˆ†ã€‚

## è¯„åˆ†è¦æ±‚

è¯·ä»ä»¥ä¸‹5ä¸ªç»´åº¦å¯¹å›ç­”è¿›è¡Œè¯„åˆ†ï¼ˆ0-10åˆ†ï¼Œä¿ç•™ä¸€ä½å°æ•°ï¼‰ï¼š

1. **å®Œæ•´æ€§(Completeness)**: å›ç­”æ˜¯å¦å®Œæ•´åœ°è¦†ç›–äº†é—®é¢˜çš„å„ä¸ªæ–¹é¢
2. **å‡†ç¡®æ€§(Accuracy)**: å›ç­”ä¸­çš„ä¿¡æ¯ã€æ¦‚å¿µã€ç¤ºä¾‹æ˜¯å¦å‡†ç¡®å¯é 
3. **æ¸…æ™°åº¦(Clarity)**: å›ç­”çš„è¡¨è¾¾æ˜¯å¦æ¸…æ™°ã€é€»è¾‘æ˜¯å¦è¿è´¯ã€æ˜“äºç†è§£
4. **ç›¸å…³æ€§(Relevance)**: å›ç­”æ˜¯å¦ç´§æ‰£é—®é¢˜ä¸»é¢˜ï¼Œæ²¡æœ‰ç¦»é¢˜æˆ–å†—ä½™å†…å®¹
5. **ç»¼åˆè´¨é‡(Overall)**: æ•´ä½“å›ç­”è´¨é‡ï¼ˆå¿…é¡»æ˜¯å‰4ä¸ªç»´åº¦çš„å¹³å‡å€¼Â±0.5åˆ†ï¼‰

## è¯„åˆ†æ ‡å‡†ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰

**ä½¿ç”¨ç›¸å¯¹è¯„åˆ†è€Œéç»å¯¹è¯„åˆ†ï¼š**
- 9.0-10.0åˆ†: å“è¶Šè¡¨ç°ï¼Œè¶…è¶Šé¢„æœŸï¼Œå‡ ä¹æ— ç‘•ç–µ
- 7.5-8.9åˆ†: ä¼˜ç§€è¡¨ç°ï¼Œè´¨é‡å¾ˆé«˜ä½†æœ‰å°çš„æ”¹è¿›ç©ºé—´
- 6.0-7.4åˆ†: è‰¯å¥½è¡¨ç°ï¼ŒåŸºæœ¬æ»¡è¶³éœ€æ±‚ä½†æœ‰æ˜æ˜¾ä¸è¶³
- 4.5-5.9åˆ†: åŠæ ¼è¡¨ç°ï¼Œèƒ½å›ç­”é—®é¢˜ä½†è´¨é‡ä¸€èˆ¬
- 3.0-4.4åˆ†: è¾ƒå·®è¡¨ç°ï¼Œå­˜åœ¨æ˜æ˜¾é—®é¢˜å’Œé—æ¼
- 0.0-2.9åˆ†: ä¸åˆæ ¼ï¼Œä¸¥é‡é”™è¯¯æˆ–å®Œå…¨æœªå›ç­”

**ç¦æ­¢è¯„åˆ†è¡Œä¸ºï¼š**
- âŒ ç¦æ­¢ç»™æ‰€æœ‰ç»´åº¦éƒ½æ‰“8-10åˆ†çš„é«˜åˆ†
- âŒ ç¦æ­¢ä¸åŒå›ç­”è·å¾—å®Œå…¨ç›¸åŒçš„è¯„åˆ†
- âŒ ç¦æ­¢ç»™å‡ºæ¨¡ç³Šçš„è¯„åˆ†ç†ç”±

**å¿…é¡»æ‰§è¡Œï¼š**
- âœ… å¿…é¡»å¼•ç”¨å›ç­”ä¸­çš„å…·ä½“å†…å®¹ç‰‡æ®µä½œä¸ºè¯„åˆ†ä¾æ®
- âœ… å¿…é¡»æ˜ç¡®æŒ‡å‡ºè‡³å°‘2ä¸ªä¼˜ç‚¹å’Œ2ä¸ªä¸è¶³
- âœ… å¿…é¡»åŸºäºå®é™…å†…å®¹è´¨é‡ç»™åˆ†ï¼Œä¸å—å›ç­”é•¿åº¦å½±å“
- âœ… ç»¼åˆè¯„åˆ†å¿…é¡»æ¥è¿‘å‰4ä¸ªç»´åº¦çš„å¹³å‡å€¼

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰

å®Œæ•´æ€§è¯„åˆ†: X.X
å‡†ç¡®æ€§è¯„åˆ†: X.X
æ¸…æ™°åº¦è¯„åˆ†: X.X
ç›¸å…³æ€§è¯„åˆ†: X.X
ç»¼åˆè¯„åˆ†: X.X

**è¯„åˆ†ä¾æ®ï¼ˆå¿…å¡«ï¼‰ï¼š**

ã€å®Œæ•´æ€§ã€‘
âœ… ä¼˜ç‚¹: [å¼•ç”¨å…·ä½“å†…å®¹ç‰‡æ®µï¼Œè¯´æ˜å“ªäº›æ–¹é¢åšå¾—å¥½]
âŒ ä¸è¶³: [å¼•ç”¨å…·ä½“å†…å®¹ç‰‡æ®µæˆ–æŒ‡å‡ºç¼ºå¤±çš„å†…å®¹]

ã€å‡†ç¡®æ€§ã€‘
âœ… ä¼˜ç‚¹: [å¼•ç”¨å‡†ç¡®çš„å†…å®¹ç¤ºä¾‹]
âŒ ä¸è¶³: [æŒ‡å‡ºä¸å‡†ç¡®ã€è¯¯å¯¼æˆ–æœ‰äº‰è®®çš„å†…å®¹]

ã€æ¸…æ™°åº¦ã€‘
âœ… ä¼˜ç‚¹: [è¯´æ˜è¡¨è¾¾æ¸…æ™°çš„éƒ¨åˆ†]
âŒ ä¸è¶³: [æŒ‡å‡ºè¡¨è¾¾ä¸æ¸…æˆ–é€»è¾‘æ··ä¹±çš„éƒ¨åˆ†]

ã€ç›¸å…³æ€§ã€‘
âœ… ä¼˜ç‚¹: [è¯´æ˜ç´§æ‰£ä¸»é¢˜çš„éƒ¨åˆ†]
âŒ ä¸è¶³: [æŒ‡å‡ºç¦»é¢˜ã€å†—ä½™æˆ–ä¸å¤Ÿèšç„¦çš„å†…å®¹]

**ç‹¬ç‰¹ç‰¹å¾ï¼š**
[è¯´æ˜è¿™ä¸ªå›ç­”ä¸å…¶ä»–å…¸å‹å›ç­”ç›¸æ¯”çš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œè‡³å°‘50å­—]

**æ ¸å¿ƒå»ºè®®ï¼š**
[ç»™å‡º3ä¸ªå…·ä½“çš„æ”¹è¿›å»ºè®®]
"""
        
        try:
            print(f"ğŸ¤– æ­£åœ¨è¯„ä¼° {source_name} çš„å›ç­”è´¨é‡...")

            response = await call_llm_async(
                messages=[{"role": "user", "content": evaluation_prompt}],
                model=self.evaluator_model,
                max_tokens=2000,  # å¢åŠ tokenæ•°ä»¥å®¹çº³è¯¦ç»†çš„è¯„åˆ†ä¾æ®
                temperature=0.2,   # é™ä½æ¸©åº¦ä»¥æé«˜è¯„åˆ†ä¸€è‡´æ€§
                registry=self.registry,
                trace_id=self._current_trace_id,
                parent_observation_id=self._current_parent_observation_id,
                langfuse_metadata={
                    "component": "quality_analyzer",
                    "stage": "single_answer_evaluation",
                    "source": source_name,
                },
            )
            
            # è§£æè¯„åˆ†ç»“æœ
            scores = self._parse_evaluation_response(response)

            # è®¡ç®—å‡†ç¡®çš„å­—ç¬¦æ•°å’Œå¥å­æ•°
            char_count = len(answer)
            words = re.findall(r'[\w\u4e00-\u9fff]+', answer)
            word_count = len(words)
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', answer)
            sentence_count = len([s for s in sentences if s.strip()])

            # **å…³é”®ä¿®å¤ï¼šä½¿ç”¨å¯¹æ¯”è¯„åˆ†ä½œä¸ºåŸºå‡†ï¼Œåªå…è®¸å°å¹…è°ƒæ•´**
            # è¿™æ ·å¯ä»¥ä¿æŒå¯¹æ¯”è¯„åˆ†çš„åŒºåˆ†åº¦ï¼ŒåŒæ—¶å…è®¸ç»†å¾®çš„ç»´åº¦å·®å¼‚

            completeness = scores.get('completeness', base_reference_score)
            accuracy = scores.get('accuracy', base_reference_score)
            clarity = scores.get('clarity', base_reference_score)
            relevance = scores.get('relevance', base_reference_score)

            # å„ç»´åº¦è¯„åˆ†ä¸èƒ½åç¦»å¯¹æ¯”è¯„åˆ†å¤ªå¤šï¼ˆÂ±1.0åˆ†ä»¥å†…ï¼‰
            completeness = max(base_reference_score - 1.0, min(base_reference_score + 1.0, completeness))
            accuracy = max(base_reference_score - 1.0, min(base_reference_score + 1.0, accuracy))
            clarity = max(base_reference_score - 1.0, min(base_reference_score + 1.0, clarity))
            relevance = max(base_reference_score - 1.0, min(base_reference_score + 1.0, relevance))

            # ç»¼åˆè¯„åˆ†å¿…é¡»æ¥è¿‘å¯¹æ¯”è¯„åˆ†
            dimension_avg = (completeness + accuracy + clarity + relevance) / 4
            overall = (base_reference_score * 0.7 + dimension_avg * 0.3)  # 70%æƒé‡æ¥è‡ªå¯¹æ¯”è¯„åˆ†

            return QualityMetrics(
                completeness_score=round(completeness, 1),
                accuracy_score=round(accuracy, 1),
                clarity_score=round(clarity, 1),
                relevance_score=round(relevance, 1),
                overall_score=round(overall, 1),
                word_count=char_count,  # ä½¿ç”¨å­—ç¬¦æ•°
                sentence_count=sentence_count,
                readability_score=scores.get('clarity', 5.0),
                information_density=min(10, len(set([w.lower() for w in words])) / max(word_count, 1) * 20)
            )
            
        except Exception as e:
            print(f"âš ï¸ è¯„ä¼° {source_name} æ—¶å‡ºé”™: {str(e)}")
            return QualityMetrics(5, 5, 5, 5, 5, 0, 0, 5, 5)
    
    def _parse_evaluation_response(self, response: str) -> Dict[str, float]:
        """è§£æLLMè¯„ä¼°å“åº”"""
        scores = {}
        
        patterns = {
            'completeness': r'å®Œæ•´æ€§è¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)',
            'accuracy': r'å‡†ç¡®æ€§è¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)',
            'clarity': r'æ¸…æ™°åº¦è¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)',
            'relevance': r'ç›¸å…³æ€§è¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)',
            'overall': r'ç»¼åˆè¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)'
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, response)
            if match:
                try:
                    score = float(match.group(1))
                    scores[key] = max(0, min(10, score))  # é™åˆ¶åœ¨0-10èŒƒå›´å†…
                except ValueError:
                    scores[key] = 5.0
            else:
                scores[key] = 5.0
        
        return scores
    
    async def _perform_content_semantic_analysis(
        self,
        question: str,
        llm_responses: List[Dict],
        fusion_answer: str
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå†…å®¹è¯­ä¹‰åˆ†æï¼Œè¯†åˆ«çœŸå®å·®å¼‚"""
        print("ğŸ” æ­£åœ¨è¿›è¡Œå†…å®¹è¯­ä¹‰åˆ†æ...")

        content_analysis = {
            'content_uniqueness': {},
            'approach_differences': {},
            'perspective_analysis': {},
            'semantic_similarity': {},
            'content_themes': {},
            'structure_patterns': {},
            'individualized_profiles': {}  # æ–°å¢ï¼šä¸ªæ€§åŒ–æ¡£æ¡ˆ
        }

        try:
            # 1. åˆ†æå„æ¨¡å‹çš„è§’åº¦å’Œæ–¹æ³•å·®å¼‚
            approach_analysis = await self._analyze_response_approaches(
                question, [r for r in llm_responses if r['success']]
            )
            content_analysis['approach_differences'] = approach_analysis

            # 2. è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
            similarity_matrix = self._calculate_content_similarity(
                [r['response'] for r in llm_responses if r['success']], fusion_answer
            )
            content_analysis['semantic_similarity'] = similarity_matrix

            # 3. æå–å…³é”®ä¸»é¢˜å’Œè§‚ç‚¹
            themes_analysis = await self._extract_content_themes(
                question, [r for r in llm_responses if r['success']], fusion_answer
            )
            content_analysis['content_themes'] = themes_analysis

            # 4. åˆ†æç»“æ„æ¨¡å¼
            structure_analysis = self._analyze_structure_patterns(
                [r for r in llm_responses if r['success']], fusion_answer
            )
            content_analysis['structure_patterns'] = structure_analysis

            # 5. è¯„ä¼°å†…å®¹ç‹¬ç‰¹æ€§
            uniqueness_scores = self._calculate_content_uniqueness(
                [r for r in llm_responses if r['success']], fusion_answer
            )
            content_analysis['content_uniqueness'] = uniqueness_scores

            # 6. **æ–°å¢ï¼šæ·±åº¦ä¸ªæ€§åŒ–åˆ†æ**
            individualized_profiles = await self._deep_individualized_analysis(
                question, [r for r in llm_responses if r['success']]
            )
            content_analysis['individualized_profiles'] = individualized_profiles

        except Exception as e:
            print(f"âš ï¸ å†…å®¹è¯­ä¹‰åˆ†æå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤ç»“æ„
            pass

        return content_analysis
    
    def _perform_enhanced_comparison_analysis(
        self, 
        basic_metrics: Dict[str, QualityMetrics],
        llm_evaluations: Dict[str, QualityMetrics],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºçš„å¯¹æ¯”åˆ†æï¼ˆèå…¥å†…å®¹è¯­ä¹‰åˆ†æï¼‰"""
        
        analysis = {
            'fusion_advantages': [],
            'model_strengths': {},
            'improvement_areas': [],
            'statistical_summary': {},
            'individual_analysis': {}
        }
        
        # è·å–èåˆå›ç­”çš„è¯„åˆ†
        fusion_eval = llm_evaluations.get('fusion_answer')
        if not fusion_eval:
            return analysis
        
        # åˆ†æèåˆå›ç­”çš„ä¼˜åŠ¿
        model_evaluations = {k: v for k, v in llm_evaluations.items() if k != 'fusion_answer'}
        
        if model_evaluations:
            # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
            avg_scores = {
                'completeness': sum(eval.completeness_score for eval in model_evaluations.values()) / len(model_evaluations),
                'accuracy': sum(eval.accuracy_score for eval in model_evaluations.values()) / len(model_evaluations),
                'clarity': sum(eval.clarity_score for eval in model_evaluations.values()) / len(model_evaluations),
                'relevance': sum(eval.relevance_score for eval in model_evaluations.values()) / len(model_evaluations),
                'overall': sum(eval.overall_score for eval in model_evaluations.values()) / len(model_evaluations)
            }
            
            # èåˆå›ç­”çš„ä¼˜åŠ¿åˆ†æ
            if fusion_eval.completeness_score > avg_scores['completeness']:
                analysis['fusion_advantages'].append(f"å®Œæ•´æ€§æå‡ {fusion_eval.completeness_score - avg_scores['completeness']:.1f}åˆ†")
            
            if fusion_eval.accuracy_score > avg_scores['accuracy']:
                analysis['fusion_advantages'].append(f"å‡†ç¡®æ€§æå‡ {fusion_eval.accuracy_score - avg_scores['accuracy']:.1f}åˆ†")
            
            if fusion_eval.clarity_score > avg_scores['clarity']:
                analysis['fusion_advantages'].append(f"æ¸…æ™°åº¦æå‡ {fusion_eval.clarity_score - avg_scores['clarity']:.1f}åˆ†")
            
            if fusion_eval.overall_score > avg_scores['overall']:
                analysis['fusion_advantages'].append(f"ç»¼åˆè´¨é‡æå‡ {fusion_eval.overall_score - avg_scores['overall']:.1f}åˆ†")
            
            # ä¸ªæ€§åŒ–æ¨¡å‹åˆ†æ - ç»“åˆå†…å®¹è¯­ä¹‰åˆ†æ
            analysis['model_strengths'] = self._analyze_enhanced_model_strengths(
                model_evaluations, avg_scores, content_analysis
            )
            
            # è¯¦ç»†çš„ä¸ªä½“åˆ†æ
            analysis['individual_analysis'] = self._analyze_individual_performance(
                model_evaluations, basic_metrics
            )
            
            # ç»Ÿè®¡æ‘˜è¦
            analysis['statistical_summary'] = {
                'fusion_overall_score': fusion_eval.overall_score,
                'models_avg_score': avg_scores['overall'],
                'improvement': fusion_eval.overall_score - avg_scores['overall'],
                'best_individual_score': max(eval.overall_score for eval in model_evaluations.values()),
                'fusion_vs_best': fusion_eval.overall_score - max(eval.overall_score for eval in model_evaluations.values())
            }
        
        return analysis
    
    def _calculate_quality_ranking(
        self, 
        basic_metrics: Dict[str, QualityMetrics],
        llm_evaluations: Dict[str, QualityMetrics]
    ) -> List[Dict[str, Any]]:
        """è®¡ç®—è´¨é‡æ’å"""
        
        ranking = []
        
        for source_name, evaluation in llm_evaluations.items():
            basic = basic_metrics.get(source_name)
            
            # ç¡®ä¿è¯„åˆ†çš„ä¸€è‡´æ€§æ£€æŸ¥
            overall_score = evaluation.overall_score
            dimension_avg = (evaluation.completeness_score + evaluation.accuracy_score + 
                           evaluation.clarity_score + evaluation.relevance_score) / 4
            
            # å¦‚æœç»¼åˆè¯„åˆ†ä¸å„ç»´åº¦å¹³å‡åˆ†å·®å¼‚è¿‡å¤§ï¼Œè¿›è¡Œè°ƒæ•´
            if abs(overall_score - dimension_avg) > 2.0:
                overall_score = dimension_avg
                print(f"âš ï¸ {source_name} çš„è¯„åˆ†å­˜åœ¨ä¸ä¸€è‡´ï¼Œå·²è°ƒæ•´ç»¼åˆè¯„åˆ†ä¸º {overall_score:.1f}")
            
            ranking.append({
                'source': source_name,
                'overall_score': overall_score,
                'completeness': evaluation.completeness_score,
                'accuracy': evaluation.accuracy_score,
                'clarity': evaluation.clarity_score,
                'relevance': evaluation.relevance_score,
                'word_count': basic.word_count if basic else 0,
                'is_fusion': source_name == 'fusion_answer'
            })
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åºï¼Œåˆ†æ•°ç›¸åŒæ—¶æŒ‰å‡†ç¡®æ€§æ’åº
        ranking.sort(key=lambda x: (x['overall_score'], x['accuracy']), reverse=True)
        
        # æ·»åŠ æ’åï¼Œå¤„ç†å¹¶åˆ—æƒ…å†µ
        current_rank = 1
        for i, item in enumerate(ranking):
            if i > 0 and ranking[i-1]['overall_score'] != item['overall_score']:
                current_rank = i + 1
            item['rank'] = current_rank
        
        return ranking
    
    def _analyze_individual_model_strengths(
        self, 
        model_evaluations: Dict[str, QualityMetrics], 
        avg_scores: Dict[str, float]
    ) -> Dict[str, List[str]]:
        """åˆ†ææ¯ä¸ªæ¨¡å‹çš„ä¸ªæ€§åŒ–ä¼˜åŠ¿"""
        strengths = {}
        
        for model_name, eval_result in model_evaluations.items():
            model_strengths = []
            
            # ç›¸å¯¹ä¼˜åŠ¿åˆ†æï¼ˆç›¸æ¯”å¹³å‡æ°´å¹³ï¼‰
            if eval_result.completeness_score > avg_scores['completeness'] + 0.5:
                diff = eval_result.completeness_score - avg_scores['completeness']
                model_strengths.append(f"å®Œæ•´æ€§è¶…ç¾¤(+{diff:.1f})")
            elif eval_result.completeness_score >= 8.0:
                model_strengths.append("å®Œæ•´æ€§ä¼˜ç§€")
            
            if eval_result.accuracy_score > avg_scores['accuracy'] + 0.5:
                diff = eval_result.accuracy_score - avg_scores['accuracy']
                model_strengths.append(f"å‡†ç¡®æ€§å‡ºä¼—(+{diff:.1f})")
            elif eval_result.accuracy_score >= 8.0:
                model_strengths.append("å‡†ç¡®æ€§å¯é ")
            
            if eval_result.clarity_score > avg_scores['clarity'] + 0.5:
                diff = eval_result.clarity_score - avg_scores['clarity']
                model_strengths.append(f"è¡¨è¾¾æœ€æ¸…æ™°(+{diff:.1f})")
            elif eval_result.clarity_score >= 8.0:
                model_strengths.append("è¡¨è¾¾æ¸…æ™°")
            
            if eval_result.relevance_score > avg_scores['relevance'] + 0.5:
                diff = eval_result.relevance_score - avg_scores['relevance']
                model_strengths.append(f"é«˜åº¦ç›¸å…³(+{diff:.1f})")
            elif eval_result.relevance_score >= 8.0:
                model_strengths.append("åˆ‡é¢˜ç²¾å‡†")
            
            # ç»¼åˆä¼˜åŠ¿
            if eval_result.overall_score > avg_scores['overall'] + 1.0:
                model_strengths.append("ç»¼åˆè¡¨ç°æœ€ä½³")
            elif eval_result.overall_score > avg_scores['overall'] + 0.5:
                model_strengths.append("ç»¼åˆè¡¨ç°ä¼˜ç§€")
            
            # ç‰¹è‰²åˆ†æ
            if eval_result.word_count > 0:
                # å†…å®¹ä¸°å¯Œåº¦åˆ†æ
                if eval_result.information_density >= 8.0:
                    model_strengths.append("ä¿¡æ¯å¯†åº¦é«˜")
                
                # å›ç­”é£æ ¼åˆ†æ
                if eval_result.word_count > 300:
                    model_strengths.append("å›ç­”è¯¦å°½")
                elif eval_result.word_count < 100:
                    model_strengths.append("å›ç­”ç®€æ´")
            
            if model_strengths:
                strengths[model_name] = model_strengths
            else:
                # å³ä½¿æ²¡æœ‰çªå‡ºä¼˜åŠ¿ï¼Œä¹Ÿè¦ç»™å‡ºåŸºæœ¬è¯„ä»·
                best_dimension = max([
                    ('å®Œæ•´æ€§', eval_result.completeness_score),
                    ('å‡†ç¡®æ€§', eval_result.accuracy_score), 
                    ('æ¸…æ™°åº¦', eval_result.clarity_score),
                    ('ç›¸å…³æ€§', eval_result.relevance_score)
                ], key=lambda x: x[1])
                strengths[model_name] = [f"{best_dimension[0]}ç›¸å¯¹è¾ƒå¥½({best_dimension[1]:.1f}/10)"]
        
        return strengths
    
    def _analyze_enhanced_model_strengths(
        self,
        model_evaluations: Dict[str, QualityMetrics],
        avg_scores: Dict[str, float],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, List[str]]:
        """å¢å¼ºçš„æ¨¡å‹ä¼˜åŠ¿åˆ†æï¼ˆä¼˜å…ˆä½¿ç”¨æ·±åº¦ä¸ªæ€§åŒ–åˆ†æç»“æœï¼‰"""

        enhanced_strengths = {}

        # **ä¼˜å…ˆä½¿ç”¨æ·±åº¦ä¸ªæ€§åŒ–åˆ†æç»“æœ**
        individualized_profiles = content_analysis.get('individualized_profiles', {})
        if individualized_profiles and isinstance(individualized_profiles, dict):
            profiles_data = individualized_profiles.get('individualized_profiles', {})

            if profiles_data:
                print("âœ… ä½¿ç”¨æ·±åº¦ä¸ªæ€§åŒ–åˆ†æç»“æœç”Ÿæˆæ¨¡å‹ä¼˜åŠ¿æè¿°")
                # ä½¿ç”¨æ·±åº¦åˆ†æçš„ç»“æœ
                for model_name in model_evaluations.keys():
                    if model_name in profiles_data:
                        profile = profiles_data[model_name]
                        strength_list = []

                        # 1. æ ‡å¿—æ€§ç‰¹å¾ï¼ˆæœ€é‡è¦ï¼‰
                        signature = profile.get('signature_characteristics', '')
                        if signature:
                            strength_list.append(signature)

                        # 2. ç‹¬ç‰¹è´¡çŒ®ç‚¹ï¼ˆæœ€å¤š2ä¸ªï¼‰
                        contributions = profile.get('unique_contributions', [])
                        if contributions:
                            for contrib in contributions[:2]:
                                # æˆªæ–­è¿‡é•¿çš„æè¿°
                                truncated = contrib[:80] + "..." if len(contrib) > 80 else contrib
                                strength_list.append(truncated)

                        # 3. æ¯”è¾ƒä¼˜åŠ¿
                        advantage = profile.get('comparative_advantage', '')
                        if advantage:
                            truncated = advantage[:100] + "..." if len(advantage) > 100 else advantage
                            strength_list.append(truncated)

                        # 4. å†…å®¹é£æ ¼ï¼ˆå¯é€‰ï¼‰
                        style = profile.get('content_style', '')
                        if style and len(strength_list) < 4:
                            truncated = style[:80] + "..." if len(style) > 80 else style
                            strength_list.append(truncated)

                        enhanced_strengths[model_name] = strength_list[:5]  # æœ€å¤š5æ¡
                    else:
                        # å¦‚æœæ²¡æœ‰æ·±åº¦åˆ†æç»“æœï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
                        enhanced_strengths[model_name] = self._fallback_strength_analysis(
                            model_name, model_evaluations[model_name], avg_scores, content_analysis
                        )
            else:
                # æ·±åº¦åˆ†æä¸ºç©ºï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
                print("âš ï¸ æ·±åº¦ä¸ªæ€§åŒ–åˆ†æç»“æœä¸ºç©ºï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                enhanced_strengths = self._fallback_enhanced_analysis(
                    model_evaluations, avg_scores, content_analysis
                )
        else:
            # æ²¡æœ‰æ·±åº¦åˆ†æï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
            print("âš ï¸ æœªæ‰¾åˆ°æ·±åº¦ä¸ªæ€§åŒ–åˆ†æç»“æœï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
            enhanced_strengths = self._fallback_enhanced_analysis(
                model_evaluations, avg_scores, content_analysis
            )

        return enhanced_strengths

    def _fallback_strength_analysis(
        self,
        model_name: str,
        evaluation: QualityMetrics,
        avg_scores: Dict[str, float],
        content_analysis: Dict[str, Any]
    ) -> List[str]:
        """å•ä¸ªæ¨¡å‹çš„å›é€€ä¼˜åŠ¿åˆ†æ"""
        strength_list = []

        # è·å–å†…å®¹å·®å¼‚åŒ–ä¿¡æ¯
        uniqueness_scores = content_analysis.get('content_uniqueness', {})
        structure_patterns = content_analysis.get('structure_patterns', {})
        content_themes = content_analysis.get('content_themes', {})

        # 1. ç›¸å¯¹ä¼˜åŠ¿åˆ†æ
        if evaluation.completeness_score > avg_scores.get('completeness', 7.0) + 0.5:
            diff = evaluation.completeness_score - avg_scores.get('completeness', 7.0)
            strength_list.append(f"å®Œæ•´æ€§è¶…ç¾¤(+{diff:.1f})")
        elif evaluation.completeness_score >= 8.0:
            strength_list.append("å®Œæ•´æ€§ä¼˜ç§€")

        if evaluation.accuracy_score > avg_scores.get('accuracy', 7.0) + 0.5:
            diff = evaluation.accuracy_score - avg_scores.get('accuracy', 7.0)
            strength_list.append(f"å‡†ç¡®æ€§å‡ºä¼—(+{diff:.1f})")
        elif evaluation.accuracy_score >= 8.0:
            strength_list.append("å‡†ç¡®æ€§å¯é ")

        # 2. ç‹¬ç‰¹æ€§åˆ†æ
        uniqueness = uniqueness_scores.get(model_name, 0)
        if uniqueness > 0.3:
            strength_list.append(f"å†…å®¹ç‹¬ç‰¹æ€§é«˜({uniqueness:.1%})")
        elif uniqueness > 0.15:
            strength_list.append(f"æœ‰ä¸€å®šç‹¬ç‰¹æ€§({uniqueness:.1%})")

        # 3. ç»“æ„ç‰¹å¾
        if model_name in structure_patterns:
            structure = structure_patterns[model_name]
            if structure.get('list_items', 0) > 2:
                strength_list.append("ç»“æ„æ¸…æ™°ï¼Œå–„ç”¨åˆ—è¡¨")
            if structure.get('code_blocks', 0) > 0:
                strength_list.append("æä¾›ä»£ç ç¤ºä¾‹")

        # 4. ä¸»é¢˜è¦†ç›–
        model_unique_points = content_themes.get('model_unique_points', {})
        if model_name in model_unique_points:
            unique_points = model_unique_points[model_name]
            if unique_points:
                strength_list.append(f"ç‹¬ç‰¹è§‚ç‚¹: {', '.join(unique_points[:1])}")

        # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œè‡³å°‘ç»™å‡ºä¸€ä¸ªåŸºæœ¬è¯„ä»·
        if not strength_list:
            best_dim = max([
                ('å®Œæ•´æ€§', evaluation.completeness_score),
                ('å‡†ç¡®æ€§', evaluation.accuracy_score),
                ('æ¸…æ™°åº¦', evaluation.clarity_score),
                ('ç›¸å…³æ€§', evaluation.relevance_score)
            ], key=lambda x: x[1])
            strength_list.append(f"{best_dim[0]}ç›¸å¯¹è¾ƒå¥½({best_dim[1]:.1f}/10)")

        return strength_list[:5]

    def _fallback_enhanced_analysis(
        self,
        model_evaluations: Dict[str, QualityMetrics],
        avg_scores: Dict[str, float],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, List[str]]:
        """å›é€€çš„å¢å¼ºåˆ†æï¼ˆå½“æ·±åº¦åˆ†æå¤±è´¥æ—¶ï¼‰"""
        enhanced_strengths = {}

        for model_name, evaluation in model_evaluations.items():
            enhanced_strengths[model_name] = self._fallback_strength_analysis(
                model_name, evaluation, avg_scores, content_analysis
            )

        return enhanced_strengths
    
    def _analyze_individual_performance(
        self, 
        model_evaluations: Dict[str, QualityMetrics],
        basic_metrics: Dict[str, QualityMetrics]
    ) -> Dict[str, Dict[str, Any]]:
        """è¯¦ç»†çš„ä¸ªä½“æ€§èƒ½åˆ†æ"""
        individual_analysis = {}
        
        # è®¡ç®—å„ç»´åº¦çš„æœ€é«˜åˆ†ï¼Œç”¨äºç›¸å¯¹æ¯”è¾ƒ
        max_scores = {
            'completeness': max(eval.completeness_score for eval in model_evaluations.values()),
            'accuracy': max(eval.accuracy_score for eval in model_evaluations.values()),
            'clarity': max(eval.clarity_score for eval in model_evaluations.values()),
            'relevance': max(eval.relevance_score for eval in model_evaluations.values()),
            'overall': max(eval.overall_score for eval in model_evaluations.values())
        }
        
        for model_name, eval_result in model_evaluations.items():
            basic_metric = basic_metrics.get(model_name)
            
            analysis = {
                'performance_highlights': [],
                'relative_ranking': {},
                'style_characteristics': [],
                'improvement_potential': []
            }
            
            # æ€§èƒ½äº®ç‚¹ - åªæœ‰å”¯ä¸€æœ€é«˜åˆ†æ‰ç®—"æœ€ä½³"
            # ç»Ÿè®¡æ¯ä¸ªç»´åº¦æœ‰å¤šå°‘ä¸ªæ¨¡å‹è¾¾åˆ°æœ€é«˜åˆ†
            dimension_counts = {
                'completeness': sum(1 for eval in model_evaluations.values() if eval.completeness_score == max_scores['completeness']),
                'accuracy': sum(1 for eval in model_evaluations.values() if eval.accuracy_score == max_scores['accuracy']),
                'clarity': sum(1 for eval in model_evaluations.values() if eval.clarity_score == max_scores['clarity']),
                'relevance': sum(1 for eval in model_evaluations.values() if eval.relevance_score == max_scores['relevance'])
            }

            # åªæœ‰å”¯ä¸€æœ€é«˜åˆ†æ‰æ ‡è®°ä¸º"æœ€ä½³"ï¼Œå¦åˆ™æ ‡è®°ä¸º"å¹¶åˆ—æœ€ä½³"æˆ–"ä¼˜ç§€"
            if eval_result.completeness_score == max_scores['completeness']:
                if dimension_counts['completeness'] == 1:
                    analysis['performance_highlights'].append("å®Œæ•´æ€§æœ€ä½³")
                elif eval_result.completeness_score >= 8.0:
                    analysis['performance_highlights'].append(f"å®Œæ•´æ€§ä¼˜ç§€(å¹¶åˆ—ç¬¬1)")

            if eval_result.accuracy_score == max_scores['accuracy']:
                if dimension_counts['accuracy'] == 1:
                    analysis['performance_highlights'].append("å‡†ç¡®æ€§æœ€é«˜")
                elif eval_result.accuracy_score >= 8.0:
                    analysis['performance_highlights'].append(f"å‡†ç¡®æ€§ä¼˜ç§€(å¹¶åˆ—ç¬¬1)")

            if eval_result.clarity_score == max_scores['clarity']:
                if dimension_counts['clarity'] == 1:
                    analysis['performance_highlights'].append("è¡¨è¾¾æœ€æ¸…æ™°")
                elif eval_result.clarity_score >= 8.0:
                    analysis['performance_highlights'].append(f"æ¸…æ™°åº¦ä¼˜ç§€(å¹¶åˆ—ç¬¬1)")

            if eval_result.relevance_score == max_scores['relevance']:
                if dimension_counts['relevance'] == 1:
                    analysis['performance_highlights'].append("ç›¸å…³æ€§æœ€å¼º")
                elif eval_result.relevance_score >= 8.0:
                    analysis['performance_highlights'].append(f"ç›¸å…³æ€§ä¼˜ç§€(å¹¶åˆ—ç¬¬1)")
            
            # ç›¸å¯¹æ’åï¼ˆåœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼‰
            models_list = list(model_evaluations.items())
            for dimension in ['completeness', 'accuracy', 'clarity', 'relevance', 'overall']:
                sorted_models = sorted(models_list, 
                                     key=lambda x: getattr(x[1], f'{dimension}_score'), 
                                     reverse=True)
                rank = next(i for i, (name, _) in enumerate(sorted_models, 1) if name == model_name)
                analysis['relative_ranking'][dimension] = f"{rank}/{len(models_list)}"
            
            # é£æ ¼ç‰¹å¾åˆ†æï¼ˆä¸ªæ€§åŒ–ï¼‰
            if basic_metric and basic_metric.word_count > 0:
                char_count = basic_metric.word_count  # å­—ç¬¦æ•°
                
                # æ›´ç²¾ç»†çš„å†…å®¹ç‰¹å¾åˆ†æ
                if char_count > 500:
                    analysis['style_characteristics'].append("è¯¦ç»†å…¨é¢å‹ï¼Œæ·±åº¦é˜é‡Š")
                elif char_count > 300:
                    analysis['style_characteristics'].append("ä¸­ç­‰è¯¦ç»†å‹ï¼Œç»“æ„å®Œæ•´")
                elif char_count < 150:
                    analysis['style_characteristics'].append("ç®€æ´ç²¾ç‚¼å‹ï¼Œç›´å‡»è¦ç‚¹") 
                else:
                    analysis['style_characteristics'].append("é€‚ä¸­å¹³è¡¡å‹ï¼Œç»“æ„æ¸…æ™°")
                
                # ä¿¡æ¯å¯†åº¦çš„ä¸ªæ€§åŒ–åˆ†æ
                if basic_metric.information_density > 8.0:
                    analysis['style_characteristics'].append("é«˜ä¿¡æ¯å¯†åº¦ï¼Œå†…å®¹ç²¾ç»†")
                elif basic_metric.information_density > 6.0:
                    analysis['style_characteristics'].append("ä¿¡æ¯ä¸°å¯Œï¼Œå†…å®¹å……å®")
                elif basic_metric.information_density < 4.0:
                    analysis['style_characteristics'].append("è¡¨è¾¾èˆ’ç¼“ï¼Œæ˜“äºç†è§£")
                
                # æ·»åŠ å¯è¯»æ€§ç‰¹å¾
                if basic_metric.readability_score > 8.0:
                    analysis['style_characteristics'].append("å¯è¯»æ€§ä¼˜ç§€ï¼Œè¡Œæ–‡æµç•…")
                elif basic_metric.readability_score < 5.0:
                    analysis['style_characteristics'].append("è¡¨è¾¾å¤æ‚ï¼Œéœ€ä»”ç»†ç†è§£")
            
            # ä¸ªæ€§åŒ–çš„æ”¹è¿›æ½œåŠ›åˆ†æ
            dimensions = [
                ('å®Œæ•´æ€§', eval_result.completeness_score),
                ('å‡†ç¡®æ€§', eval_result.accuracy_score),
                ('æ¸…æ™°åº¦', eval_result.clarity_score), 
                ('ç›¸å…³æ€§', eval_result.relevance_score)
            ]
            
            # æ‰¾å‡ºæœ€å¼±å’Œæœ€å¼ºçš„ç»´åº¦
            sorted_dims = sorted(dimensions, key=lambda x: x[1])
            weakest = sorted_dims[0]
            strongest = sorted_dims[-1]
            
            # ä¸ªæ€§åŒ–çš„æ”¹è¿›å»ºè®®
            if weakest[1] < 6.0:
                improvement_suggestions = {
                    'å®Œæ•´æ€§': 'å¯å¢åŠ å›ç­”çš„å…¨é¢æ€§å’Œæ·±åº¦',
                    'å‡†ç¡®æ€§': 'éœ€è¦æé«˜ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§',
                    'æ¸…æ™°åº¦': 'è¡¨è¾¾å¯ä»¥æ›´åŠ æ¸…æ™°æ˜äº†',
                    'ç›¸å…³æ€§': 'éœ€è¦æ›´å¥½åœ°ç†è§£å’Œå¯¹é’ˆé—®é¢˜æ ¸å¿ƒ'
                }
                analysis['improvement_potential'].append(improvement_suggestions.get(weakest[0], f"{weakest[0]}æœ‰æå‡ç©ºé—´"))
            elif weakest[1] < 7.5:
                analysis['improvement_potential'].append(f"{weakest[0]}è¡¨ç°å°šå¯ï¼Œæœ‰è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´")
            
            # ä¼˜åŠ¿ç»´åº¦åˆ†æ
            if strongest[1] >= 8.5:
                excellence_descriptions = {
                    'å®Œæ•´æ€§': 'åœ¨å†…å®¹å®Œæ•´æ€§æ–¹é¢è¡¨ç°çªå‡ºï¼Œå¯ä½œä¸ºæ ¸å¿ƒç«äº‰åŠ›',
                    'å‡†ç¡®æ€§': 'ä¿¡æ¯å‡†ç¡®æ€§æ˜¯è¯¥æ¨¡å‹çš„æœ€å¤§äº®ç‚¹',
                    'æ¸…æ™°åº¦': 'è¡¨è¾¾æ¸…æ™°åº¦ä¼˜ç§€ï¼Œé€‚åˆå¤æ‚é—®é¢˜è§£é‡Š',
                    'ç›¸å…³æ€§': 'å¯¹é—®é¢˜çš„ç†è§£å’Œå¯¹é’ˆæ€§æ˜¯æ˜¾è‘—ä¼˜åŠ¿'
                }
                analysis['improvement_potential'].append(excellence_descriptions.get(strongest[0], f"åœ¨{strongest[0]}æ–¹é¢è¡¨ç°çªå‡º"))
            elif strongest[1] >= 8.0:
                analysis['improvement_potential'].append(f"{strongest[0]}è¡¨ç°ä¼˜ç§€ï¼Œå¯ä½œä¸ºç›¸å¯¹ä¼˜åŠ¿")
            
            # æ·»åŠ æ¨¡å‹ç‰¹å¼‚æ€§åˆ†æ
            model_specificity = self._analyze_model_specificity(model_name, eval_result, basic_metric)
            if model_specificity:
                analysis['improvement_potential'].extend(model_specificity)
            
            individual_analysis[model_name] = analysis
        
        return individual_analysis
    
    def _analyze_model_specificity(self, model_name: str, eval_result: QualityMetrics, basic_metric: QualityMetrics) -> List[str]:
        """åˆ†ææ¨¡å‹ç‰¹å¼‚æ€§ï¼Œé¿å…åŒè´¨åŒ–åˆ†æ"""
        specificity = []
        
        # åŸºäºæ¨¡å‹åç§°çš„ç‰¹å¼‚æ€§åˆ†æ
        model_traits = {
            'claude_sonnet4': [
                f"é€»è¾‘æ¨ç†èƒ½åŠ›åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­è¯„åˆ†ä¸º{eval_result.clarity_score:.1f}",
                f"é€‚åˆå¤æ‚é—®é¢˜åˆ†æå’Œæ·±åº¦æ€è€ƒ"
            ],
            'gpt-41-0414-global': [
                f"æ•°å­¦å’Œé€»è¾‘åˆ†æèƒ½åŠ›åœ¨æœ¬æ¬¡è¡¨ç°ä¸º{eval_result.accuracy_score:.1f}åˆ†",
                f"æŠ€æœ¯é—®é¢˜å¤„ç†èƒ½åŠ›è¾ƒå¼º"
            ],
            'qwen-max': [
                f"ä¸­æ–‡ç†è§£å’Œå¤„ç†èƒ½åŠ›åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­è¡¨ç°ä¸º{eval_result.relevance_score:.1f}åˆ†",
                f"å¯¹ä¸­æ–‡è¯­å¢ƒå’Œæ–‡åŒ–èƒŒæ™¯ç†è§£æ·±å…¥"
            ],
            'claude37_sonnet_new': [
                f"å¹³è¡¡æ€§è¡¨ç°ï¼Œæœ¬æ¬¡ç»¼åˆè¯„åˆ†{eval_result.overall_score:.1f}",
                f"é€‚åˆæ—¥å¸¸å¯¹è¯å’Œé€šç”¨ä»»åŠ¡"
            ],
            'qwen-plus': [
                f"æ€§ä»·æ¯”è¾ƒé«˜ï¼Œæœ¬æ¬¡ä»»åŠ¡ä¸­æ•ˆç‡è¡¨ç°è‰¯å¥½",
                f"å®ç”¨æ€§å¼ºï¼Œé€‚åˆå¤šç§åœºæ™¯"
            ],
            'gpt-41-mini-0414-global': [
                f"å¿«é€Ÿå“åº”ç‰¹æ€§ï¼Œé€‚åˆè½»é‡çº§ä»»åŠ¡",
                f"æ•ˆç‡å¯¼å‘ï¼Œèµ„æºèŠ‚çº¦å‹æ¨¡å‹"
            ]
        }
        
        if model_name in model_traits:
            specificity.extend(model_traits[model_name][:1])  # åªå–ç¬¬ä¸€ä¸ªç‰¹å¼‚æ€§æè¿°
        
        return specificity
    
    async def _deep_individualized_analysis(
        self,
        question: str,
        responses: List[Dict]
    ) -> Dict[str, Any]:
        """
        æ·±åº¦ä¸ªæ€§åŒ–åˆ†æ - å¼ºåˆ¶è¯†åˆ«æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹ç‰¹å¾
        å³ä½¿è¯„åˆ†ç›¸è¿‘ï¼Œä¹Ÿè¦æŒ–æ˜å‡ºé£æ ¼ã€è§’åº¦ã€æ·±åº¦çš„å·®å¼‚
        """
        if not responses or len(responses) < 2:
            return {}

        print("ğŸ” æ­£åœ¨è¿›è¡Œæ·±åº¦ä¸ªæ€§åŒ–åˆ†æ...")

        try:
            # æ„å»ºå¼ºåˆ¶å·®å¼‚åŒ–åˆ†ææç¤º
            individualization_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ†æä¸“å®¶ã€‚ç°æœ‰{len(responses)}ä¸ªAIæ¨¡å‹å¯¹åŒä¸€é—®é¢˜ç»™å‡ºäº†å›ç­”ï¼Œå³ä½¿å®ƒä»¬è´¨é‡ç›¸è¿‘ï¼Œä¹Ÿå¿…ç„¶å­˜åœ¨é£æ ¼ã€è§’åº¦ã€æ·±åº¦çš„å·®å¼‚ã€‚

**é—®é¢˜ï¼š**
{question}

**å„æ¨¡å‹å›ç­”ï¼š**
"""
            for i, response in enumerate(responses, 1):
                model_name = response['model_name']
                answer_text = response['response']
                # æä¾›å®Œæ•´å†…å®¹ä»¥ä¾¿æ·±åº¦åˆ†æ
                truncated = answer_text[:800] + "..." if len(answer_text) > 800 else answer_text
                individualization_prompt += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æ¨¡å‹{i}: {model_name}ã€‘
å­—ç¬¦æ•°: {len(answer_text)}
å†…å®¹: {truncated}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""

            individualization_prompt += f"""
**åˆ†æä»»åŠ¡ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ï¼š**

è¯·ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆ**å®Œå…¨ä¸åŒ**çš„ä¸ªæ€§åŒ–æ¡£æ¡ˆã€‚ç¦æ­¢ä½¿ç”¨ç›¸åŒæˆ–ç›¸ä¼¼çš„æè¿°ã€‚

å¯¹æ¯ä¸ªæ¨¡å‹ï¼Œå¿…é¡»å®Œæˆä»¥ä¸‹åˆ†æï¼š

1. **å†…å®¹é£æ ¼ç‰¹å¾**ï¼ˆå¿…é¡»å…·ä½“ä¸”ä¸åŒï¼‰
   - å¼•ç”¨è¯¥æ¨¡å‹å›ç­”ä¸­çš„å…·ä½“å†…å®¹ç‰‡æ®µ
   - æè¿°ç‹¬ç‰¹çš„è¡¨è¾¾æ–¹å¼ã€å¥å¼ç‰¹ç‚¹
   - è¯´æ˜ä¸å…¶ä»–æ¨¡å‹çš„æ˜æ˜¾åŒºåˆ«

2. **è§£ç­”è§’åº¦ä¸æ·±åº¦**ï¼ˆå¿…é¡»å·®å¼‚åŒ–ï¼‰
   - è¯¥æ¨¡å‹ä»ä»€ä¹ˆç‹¬ç‰¹è§’åº¦åˆ‡å…¥é—®é¢˜
   - ä¾§é‡äºç†è®º/å®è·µ/ç¤ºä¾‹/æ­¥éª¤ä¸­çš„å“ªäº›æ–¹é¢
   - å†…å®¹æ·±åº¦å’Œå¹¿åº¦çš„å…·ä½“ç‰¹å¾

3. **ç‹¬ç‰¹è´¡çŒ®ç‚¹**ï¼ˆè‡³å°‘åˆ—å‡º2-3ä¸ªå…·ä½“å†…å®¹ï¼‰
   - å¼•ç”¨è¯¥æ¨¡å‹æä¾›çš„ç‹¬æœ‰ä¿¡æ¯ã€è§‚ç‚¹æˆ–æ–¹æ³•
   - è¯´æ˜è¿™äº›å†…å®¹åœ¨å…¶ä»–æ¨¡å‹å›ç­”ä¸­ç¼ºå¤±æˆ–å¼±åŒ–
   - å…·ä½“åˆ°å¯ä»¥å®šä½çš„å†…å®¹ç‰‡æ®µ

4. **ä¼˜åŠ¿åŠ£åŠ¿å¯¹æ¯”**ï¼ˆå¿…é¡»åŸºäºå®é™…å†…å®¹ï¼‰
   - ç›¸æ¯”å…¶ä»–{len(responses)-1}ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æœ€çªå‡ºçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ
   - ç›¸æ¯”å…¶ä»–{len(responses)-1}ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æœ€æ˜æ˜¾çš„ä¸è¶³æ˜¯ä»€ä¹ˆ
   - å…·ä½“å¼•ç”¨å†…å®¹æ”¯æ’‘åˆ¤æ–­

5. **é€‚ç”¨åœºæ™¯æ¨è**ï¼ˆå¿…é¡»ä¸ªæ€§åŒ–ï¼‰
   - åŸºäºè¯¥æ¨¡å‹çš„é£æ ¼å’Œå†…å®¹ç‰¹ç‚¹
   - æ¨è1-2ä¸ªæœ€é€‚åˆä½¿ç”¨è¯¥æ¨¡å‹çš„å…·ä½“åœºæ™¯
   - è§£é‡Šä¸ºä»€ä¹ˆè¯¥æ¨¡å‹åœ¨è¿™äº›åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½

**è¾“å‡ºè¦æ±‚ï¼š**

å¿…é¡»ä»¥JSONæ ¼å¼è¿”å›ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å‹çš„æè¿°å®Œå…¨ä¸åŒï¼š

```json
{{
    "individualized_profiles": {{
        "æ¨¡å‹1åç§°": {{
            "content_style": "å…·ä½“çš„é£æ ¼æè¿°ï¼Œå¼•ç”¨å®é™…å†…å®¹ç‰‡æ®µ",
            "approach_depth": "ç‹¬ç‰¹çš„è§’åº¦å’Œæ·±åº¦åˆ†æ",
            "unique_contributions": [
                "å…·ä½“è´¡çŒ®ç‚¹1ï¼ˆå¼•ç”¨å†…å®¹ï¼‰",
                "å…·ä½“è´¡çŒ®ç‚¹2ï¼ˆå¼•ç”¨å†…å®¹ï¼‰",
                "å…·ä½“è´¡çŒ®ç‚¹3ï¼ˆå¼•ç”¨å†…å®¹ï¼‰"
            ],
            "comparative_advantage": "ç›¸æ¯”å…¶ä»–æ¨¡å‹çš„æœ€å¤§ä¼˜åŠ¿ï¼ˆå…·ä½“è¯´æ˜ï¼‰",
            "comparative_weakness": "ç›¸æ¯”å…¶ä»–æ¨¡å‹çš„ä¸»è¦ä¸è¶³ï¼ˆå…·ä½“è¯´æ˜ï¼‰",
            "best_use_scenarios": [
                "æœ€é€‚åœºæ™¯1åŠç†ç”±",
                "æœ€é€‚åœºæ™¯2åŠç†ç”±"
            ],
            "signature_characteristics": "è¯¥æ¨¡å‹çš„æ ‡å¿—æ€§ç‰¹å¾æ€»ç»“ï¼ˆ50å­—ä»¥å†…ï¼‰"
        }},
        "æ¨¡å‹2åç§°": {{
            ...ï¼ˆå®Œå…¨ä¸åŒçš„åˆ†æï¼‰
        }},
        ...
    }},
    "differentiation_summary": "æ€»ç»“å„æ¨¡å‹ä¹‹é—´çš„æ ¸å¿ƒå·®å¼‚ï¼ˆ100å­—ä»¥å†…ï¼‰"
}}
```

**ä¸¥æ ¼ç¦æ­¢ï¼š**
- âŒ ç¦æ­¢å¯¹ä¸åŒæ¨¡å‹ä½¿ç”¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„æè¿°
- âŒ ç¦æ­¢ä½¿ç”¨æ¨¡ç³Šã€é€šç”¨çš„è¯„ä»·ï¼ˆå¦‚"å›ç­”è¯¦å°½"ã€"è¡¨è¾¾æ¸…æ™°"ç­‰ï¼‰
- âŒ ç¦æ­¢ä¸å¼•ç”¨å…·ä½“å†…å®¹å°±åšå‡ºåˆ¤æ–­
- âŒ ç¦æ­¢è®©æ‰€æœ‰æ¨¡å‹éƒ½"ä¼˜ç§€"æˆ–éƒ½"ä¸è¶³"

**å¿…é¡»æ‰§è¡Œï¼š**
- âœ… å¿…é¡»å¼•ç”¨æ¯ä¸ªæ¨¡å‹å›ç­”ä¸­çš„å…·ä½“å†…å®¹ç‰‡æ®µ
- âœ… å¿…é¡»æ˜ç¡®æŒ‡å‡ºå„æ¨¡å‹ä¹‹é—´çš„å…·ä½“å·®å¼‚
- âœ… å¿…é¡»ä¸ºæ¯ä¸ªæ¨¡å‹æ‰¾å‡ºè‡³å°‘2ä¸ªç‹¬ç‰¹è´¡çŒ®ç‚¹
- âœ… å¿…é¡»ç»™å‡ºä¸ªæ€§åŒ–çš„åœºæ™¯æ¨èï¼Œä¸èƒ½é€šç”¨åŒ–
"""

            response = await call_llm_async(
                messages=[{"role": "user", "content": individualization_prompt}],
                model=self.evaluator_model,
                max_tokens=2500,  # å¢åŠ tokenæ•°ä»¥æ”¯æŒè¯¦ç»†åˆ†æ
                temperature=0.4,   # é€‚å½“æé«˜æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§
                registry=self.registry,
                trace_id=self._current_trace_id,
                parent_observation_id=self._current_parent_observation_id,
                langfuse_metadata={
                    "component": "quality_analyzer",
                    "stage": "individualized_profiles"
                },
            )

            # è§£æJSONç»“æœ
            try:
                json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    result = json.loads(json_str)
                    print(f"âœ… æ·±åº¦ä¸ªæ€§åŒ–åˆ†æå®Œæˆï¼Œå·²è¯†åˆ«{len(result.get('individualized_profiles', {}))}ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹ç‰¹å¾")
                    return result
                else:
                    # å°è¯•ç›´æ¥è§£æ
                    result = json.loads(response)
                    return result
            except Exception as parse_error:
                print(f"âš ï¸ JSONè§£æå¤±è´¥: {str(parse_error)}, ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                # è¿”å›åŸºç¡€ç»“æ„
                return {
                    "individualized_profiles": {},
                    "differentiation_summary": "æ·±åº¦åˆ†æè§£æå¤±è´¥"
                }

        except Exception as e:
            print(f"âš ï¸ æ·±åº¦ä¸ªæ€§åŒ–åˆ†æå¤±è´¥: {str(e)}")
            return {}

    async def _analyze_response_approaches(
        self,
        question: str,
        responses: List[Dict]
    ) -> Dict[str, Any]:
        """åˆ†æå„æ¨¡å‹å›ç­”çš„æ–¹æ³•å’Œè§’åº¦å·®å¼‚"""
        if not responses:
            return {}

        try:
            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹å„ä¸ªAIæ¨¡å‹å¯¹åŒä¸€é—®é¢˜çš„å›ç­”æ–¹æ³•å’Œè§’åº¦å·®å¼‚ã€‚

é—®é¢˜: {question}

å„æ¨¡å‹å›ç­”:
"""
            for i, response in enumerate(responses, 1):
                analysis_prompt += f"\nã€æ¨¡å‹{i}: {response['model_name']}ã€‘\n{response['response'][:300]}...\n"

            analysis_prompt += """
è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æå„æ¨¡å‹çš„å·®å¼‚:
1. è§£ç­”æ–¹æ³• (ç†è®ºåˆ†æã€å®ä¾‹ä¸¾è¯ã€æ­¥éª¤æŒ‡å¯¼ç­‰)
2. ä¾§é‡ç‚¹ (æŠ€æœ¯ç»†èŠ‚ã€å®ç”¨å»ºè®®ã€ç†è®ºåŸç†ç­‰)
3. è¡¨è¾¾é£æ ¼ (ç®€æ´ç›´æ¥ã€è¯¦ç»†é˜è¿°ã€ç»“æ„åŒ–ç­‰)
4. ç‹¬ç‰¹è§‚ç‚¹ (å„æ¨¡å‹ç‰¹æœ‰çš„è§è§£æˆ–æ–¹æ³•)

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœ:
{{
    "model_approaches": {{
        "æ¨¡å‹å": {{"method": "æ–¹æ³•", "focus": "ä¾§é‡ç‚¹", "style": "é£æ ¼", "unique_insights": ["ç‹¬ç‰¹è§‚ç‚¹1", "ç‹¬ç‰¹è§‚ç‚¹2"]}}
    }},
    "differentiation_summary": "å·®å¼‚åŒ–æ€»ç»“"
}}
"""

            response = await call_llm_async(
                messages=[{"role": "user", "content": analysis_prompt}],
                model=self.evaluator_model,
                max_tokens=1000,
                temperature=0.3,
                registry=self.registry,
                trace_id=self._current_trace_id,
                parent_observation_id=self._current_parent_observation_id,
                langfuse_metadata={
                    "component": "quality_analyzer",
                    "stage": "approach_analysis"
                },
            )

            # å°è¯•è§£æJSON
            import json
            try:
                result = json.loads(response)
                return result
            except:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–ç‰ˆæœ¬
                return {"model_approaches": {}, "differentiation_summary": "è§£æå¤±è´¥"}

        except Exception as e:
            print(f"âš ï¸ è§’åº¦åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def _calculate_content_similarity(
        self, 
        responses: List[str], 
        fusion_answer: str
    ) -> Dict[str, Any]:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦çŸ©é˜µ"""
        similarity_matrix = {}
        
        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºå…³é”®è¯é‡å ï¼‰
        def calculate_keyword_similarity(text1: str, text2: str) -> float:
            # æå–å…³é”®è¯
            words1 = set(re.findall(r'[\w\u4e00-\u9fff]+', text1.lower()))
            words2 = set(re.findall(r'[\w\u4e00-\u9fff]+', text2.lower()))
            
            if not words1 or not words2:
                return 0.0
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            return intersection / union if union > 0 else 0.0
        
        # è®¡ç®—å„å›ç­”é—´çš„ç›¸ä¼¼åº¦
        for i, response1 in enumerate(responses):
            for j, response2 in enumerate(responses):
                if i < j:
                    similarity = calculate_keyword_similarity(response1, response2)
                    similarity_matrix[f"model_{i+1}_vs_model_{j+1}"] = similarity
        
        # è®¡ç®—ä¸èåˆå›ç­”çš„ç›¸ä¼¼åº¦
        for i, response in enumerate(responses):
            similarity = calculate_keyword_similarity(response, fusion_answer)
            similarity_matrix[f"model_{i+1}_vs_fusion"] = similarity
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        if similarity_matrix:
            avg_similarity = sum(similarity_matrix.values()) / len(similarity_matrix)
            similarity_matrix['average_similarity'] = avg_similarity
        
        return similarity_matrix
    
    async def _extract_content_themes(
        self, 
        question: str,
        responses: List[Dict], 
        fusion_answer: str
    ) -> Dict[str, Any]:
        """æå–å†…å®¹ä¸»é¢˜å’Œå…³é”®è§‚ç‚¹"""
        if not responses:
            return {}
        
        try:
            themes_prompt = f"""
è¯·æå–ä»¥ä¸‹å›ç­”ä¸­çš„ä¸»è¦ä¸»é¢˜å’Œå…³é”®è§‚ç‚¹ã€‚

é—®é¢˜: {question}

å„æ¨¡å‹å›ç­”:
"""
            for i, response in enumerate(responses, 1):
                themes_prompt += f"\nã€æ¨¡å‹{i}ã€‘\n{response['response'][:200]}...\n"
            
            themes_prompt += f"""
ã€èåˆå›ç­”ã€‘
{fusion_answer[:200]}...

è¯·åˆ†æ:
1. å„å›ç­”æ¶µç›–çš„ä¸»è¦ä¸»é¢˜
2. æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹è§‚ç‚¹
3. å…±åŒè§‚ç‚¹
4. èåˆå›ç­”æ–°å¢çš„å†…å®¹

è¿”å›JSONæ ¼å¼:
{{
    "main_themes": ["ä¸»é¢˜1", "ä¸»é¢˜2"],
    "model_unique_points": {{"æ¨¡å‹å": ["è§‚ç‚¹1", "è§‚ç‚¹2"]}},
    "common_points": ["å…±åŒè§‚ç‚¹1", "å…±åŒè§‚ç‚¹2"],
    "fusion_additions": ["èåˆæ–°å¢å†…å®¹1", "èåˆæ–°å¢å†…å®¹2"]
}}
"""
            
            response = await call_llm_async(
                messages=[{"role": "user", "content": themes_prompt}],
                model=self.evaluator_model,
                max_tokens=800,
                temperature=0.3,
                registry=self.registry,
                trace_id=self._current_trace_id,
                parent_observation_id=self._current_parent_observation_id,
                langfuse_metadata={
                    "component": "quality_analyzer",
                    "stage": "theme_extraction"
                },
            )
            
            try:
                import json
                result = json.loads(response)
                return result
            except:
                return {"main_themes": [], "model_unique_points": {}, "common_points": [], "fusion_additions": []}
                
        except Exception as e:
            print(f"âš ï¸ ä¸»é¢˜æå–å¤±è´¥: {str(e)}")
            return {}
    
    def _analyze_structure_patterns(
        self, 
        responses: List[Dict], 
        fusion_answer: str
    ) -> Dict[str, Any]:
        """åˆ†æå›ç­”çš„ç»“æ„æ¨¡å¼"""
        structure_analysis = {}
        
        def analyze_text_structure(text: str) -> Dict[str, Any]:
            # åˆ†ææ–‡æœ¬ç»“æ„ç‰¹å¾
            lines = text.split('\n')
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            
            # æ£€æµ‹åˆ—è¡¨é¡¹
            list_items = len(re.findall(r'^[â€¢\-\*\d+\.\)]\s+', text, re.MULTILINE))
            
            # æ£€æµ‹æ ‡é¢˜æˆ–é‡ç‚¹
            headers = len(re.findall(r'^[#]+\s+|^\*\*.*\*\*|^ã€.*ã€‘', text, re.MULTILINE))
            
            # æ£€æµ‹ä»£ç å—
            code_blocks = len(re.findall(r'```|`[^`]+`', text))
            
            return {
                'paragraph_count': len(paragraphs),
                'list_items': list_items,
                'headers': headers,
                'code_blocks': code_blocks,
                'avg_paragraph_length': sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0,
                'has_structured_format': list_items > 0 or headers > 0
            }
        
        # åˆ†æå„æ¨¡å‹çš„ç»“æ„æ¨¡å¼
        for response in responses:
            model_name = response['model_name']
            structure = analyze_text_structure(response['response'])
            structure_analysis[model_name] = structure
        
        # åˆ†æèåˆå›ç­”çš„ç»“æ„
        structure_analysis['fusion_answer'] = analyze_text_structure(fusion_answer)
        
        return structure_analysis
    
    def _calculate_content_uniqueness(
        self, 
        responses: List[Dict], 
        fusion_answer: str
    ) -> Dict[str, float]:
        """è®¡ç®—å†…å®¹ç‹¬ç‰¹æ€§è¯„åˆ†"""
        uniqueness_scores = {}
        
        # æå–æ‰€æœ‰æ–‡æœ¬çš„å…³é”®è¯
        all_texts = [r['response'] for r in responses] + [fusion_answer]
        all_keywords = []
        
        for text in all_texts:
            keywords = set(re.findall(r'[\w\u4e00-\u9fff]+', text.lower()))
            all_keywords.append(keywords)
        
        # è®¡ç®—æ¯ä¸ªå›ç­”çš„ç‹¬ç‰¹æ€§
        for i, response in enumerate(responses):
            model_keywords = all_keywords[i]
            other_keywords = set()
            
            # æ”¶é›†å…¶ä»–å›ç­”çš„å…³é”®è¯
            for j, other_keywords_set in enumerate(all_keywords):
                if j != i:
                    other_keywords.update(other_keywords_set)
            
            # è®¡ç®—ç‹¬ç‰¹å…³é”®è¯æ¯”ä¾‹
            if model_keywords:
                unique_keywords = model_keywords - other_keywords
                uniqueness_score = len(unique_keywords) / len(model_keywords)
                uniqueness_scores[response['model_name']] = uniqueness_score
            else:
                uniqueness_scores[response['model_name']] = 0.0
        
        # è®¡ç®—èåˆå›ç­”çš„ç‹¬ç‰¹æ€§
        fusion_keywords = all_keywords[-1]
        model_keywords_union = set()
        for keywords in all_keywords[:-1]:
            model_keywords_union.update(keywords)
        
        if fusion_keywords:
            fusion_unique = fusion_keywords - model_keywords_union
            uniqueness_scores['fusion_answer'] = len(fusion_unique) / len(fusion_keywords)
        else:
            uniqueness_scores['fusion_answer'] = 0.0
        
        return uniqueness_scores
    
    def _perform_consistency_validation(
        self,
        llm_evaluations: Dict[str, QualityMetrics],
        comparison_analysis: Dict[str, Any],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ‰§è¡Œé€»è¾‘ä¸€è‡´æ€§éªŒè¯"""
        print("ğŸ” æ­£åœ¨è¿›è¡Œé€»è¾‘ä¸€è‡´æ€§éªŒè¯...")
        
        consistency_issues = []
        corrections = {}
        validation_summary = {}
        
        # 1. è¯„åˆ†å†…éƒ¨ä¸€è‡´æ€§æ£€æŸ¥
        score_consistency = self._check_score_consistency(llm_evaluations)
        consistency_issues.extend(score_consistency['issues'])
        corrections.update(score_consistency['corrections'])
        
        # 2. æ’åé€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
        ranking_consistency = self._check_ranking_consistency(llm_evaluations, comparison_analysis)
        consistency_issues.extend(ranking_consistency['issues'])
        corrections.update(ranking_consistency['corrections'])
        
        # 3. ä¼˜åŠ¿æè¿°ä¸€è‡´æ€§æ£€æŸ¥
        strength_consistency = self._check_strength_consistency(
            llm_evaluations, comparison_analysis, content_analysis
        )
        consistency_issues.extend(strength_consistency['issues'])
        corrections.update(strength_consistency['corrections'])
        
        # 4. å†…å®¹åˆ†æä¸è¯„åˆ†ä¸€è‡´æ€§æ£€æŸ¥
        content_score_consistency = self._check_content_score_consistency(
            llm_evaluations, content_analysis
        )
        consistency_issues.extend(content_score_consistency['issues'])
        corrections.update(content_score_consistency['corrections'])
        
        # ç”ŸæˆéªŒè¯æ‘˜è¦
        validation_summary = {
            'total_issues': len(consistency_issues),
            'critical_issues': len([i for i in consistency_issues if i.get('severity', 'low') == 'critical']),
            'corrected_items': len(corrections),
            'consistency_score': max(0, 100 - len(consistency_issues) * 10)  # 100åˆ†åˆ¶
        }
        
        return {
            'issues': consistency_issues,
            'corrections': corrections,
            'validation_summary': validation_summary
        }
    
    def _check_score_consistency(self, llm_evaluations: Dict[str, QualityMetrics]) -> Dict[str, Any]:
        """æ£€æŸ¥è¯„åˆ†å†…éƒ¨ä¸€è‡´æ€§"""
        issues = []
        corrections = {}
        
        for source, metrics in llm_evaluations.items():
            # æ£€æŸ¥ç»¼åˆè¯„åˆ†ä¸å„ç»´åº¦å¹³å‡åˆ†çš„ä¸€è‡´æ€§
            dimension_scores = [
                metrics.completeness_score,
                metrics.accuracy_score,
                metrics.clarity_score,
                metrics.relevance_score
            ]
            avg_dimension_score = sum(dimension_scores) / len(dimension_scores)
            overall_score = metrics.overall_score
            
            # å¦‚æœå·®å¼‚è¶…è¿‡1.5åˆ†ï¼Œæ ‡è®°ä¸ºä¸ä¸€è‡´
            if abs(overall_score - avg_dimension_score) > 1.5:
                issues.append({
                    'type': 'score_inconsistency',
                    'source': source,
                    'severity': 'critical',
                    'description': f"{source}ç»¼åˆè¯„åˆ†({overall_score:.1f})ä¸ç»´åº¦å¹³å‡åˆ†({avg_dimension_score:.1f})å·®å¼‚è¿‡å¤§",
                    'suggestion': f"å»ºè®®è°ƒæ•´ç»¼åˆè¯„åˆ†ä¸º{avg_dimension_score:.1f}"
                })
                corrections[f"{source}_overall_score"] = avg_dimension_score
            
            # æ£€æŸ¥å¼‚å¸¸é«˜åˆ†æˆ–ä½åˆ†
            for dim_name, score in zip(['completeness', 'accuracy', 'clarity', 'relevance'], dimension_scores):
                if score > 9.5:
                    issues.append({
                        'type': 'extreme_score',
                        'source': source,
                        'severity': 'warning',
                        'description': f"{source}çš„{dim_name}è¯„åˆ†({score:.1f})è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨åå·®"
                    })
                elif score < 1.0:
                    issues.append({
                        'type': 'extreme_score', 
                        'source': source,
                        'severity': 'warning',
                        'description': f"{source}çš„{dim_name}è¯„åˆ†({score:.1f})è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨åå·®"
                    })
        
        return {'issues': issues, 'corrections': corrections}
    
    def _check_ranking_consistency(
        self, 
        llm_evaluations: Dict[str, QualityMetrics],
        comparison_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æ’åé€»è¾‘ä¸€è‡´æ€§"""
        issues = []
        corrections = {}
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        sorted_by_overall = sorted(
            llm_evaluations.items(),
            key=lambda x: x[1].overall_score,
            reverse=True
        )
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å‹éƒ½æœ‰ç›¸åŒçš„è¯„åˆ†ï¼ˆåŒè´¨åŒ–é—®é¢˜ï¼‰
        scores = [metrics.overall_score for _, metrics in sorted_by_overall]
        if len(set(scores)) <= 2 and len(scores) > 2:
            issues.append({
                'type': 'homogeneous_scores',
                'severity': 'critical',
                'description': "æ£€æµ‹åˆ°è¯„åˆ†åŒè´¨åŒ–é—®é¢˜ï¼Œå¤§å¤šæ•°æ¨¡å‹è¯„åˆ†ç›¸è¿‘",
                'suggestion': "éœ€è¦é‡æ–°è¯„ä¼°æ¨¡å‹å·®å¼‚æ€§"
            })
        
        # æ£€æŸ¥èåˆå›ç­”æ˜¯å¦åˆç†æ’å
        fusion_eval = llm_evaluations.get('fusion_answer')
        if fusion_eval:
            model_scores = [metrics.overall_score for name, metrics in llm_evaluations.items() if name != 'fusion_answer']
            if model_scores:
                max_model_score = max(model_scores)
                avg_model_score = sum(model_scores) / len(model_scores)
                
                # å¦‚æœèåˆå›ç­”è¯„åˆ†æ¯”æ‰€æœ‰å•æ¨¡å‹éƒ½ä½å¾ˆå¤š
                if fusion_eval.overall_score < avg_model_score - 1.0:
                    issues.append({
                        'type': 'fusion_underperformance',
                        'severity': 'critical',
                        'description': f"èåˆå›ç­”è¯„åˆ†({fusion_eval.overall_score:.1f})æ˜æ˜¾ä½äºæ¨¡å‹å¹³å‡åˆ†({avg_model_score:.1f})",
                        'suggestion': "æ£€æŸ¥èåˆç®—æ³•æ˜¯å¦æœ‰æ•ˆ"
                    })
                
                # å¦‚æœèåˆå›ç­”è¯„åˆ†å¼‚å¸¸é«˜
                elif fusion_eval.overall_score > max_model_score + 1.0:
                    issues.append({
                        'type': 'fusion_overperformance',
                        'severity': 'warning',
                        'description': f"èåˆå›ç­”è¯„åˆ†({fusion_eval.overall_score:.1f})å¼‚å¸¸é«˜äºæœ€ä½³æ¨¡å‹({max_model_score:.1f})",
                        'suggestion': "éªŒè¯èåˆæ•ˆæœæ˜¯å¦çœŸå®"
                    })
        
        return {'issues': issues, 'corrections': corrections}
    
    def _check_strength_consistency(
        self,
        llm_evaluations: Dict[str, QualityMetrics],
        comparison_analysis: Dict[str, Any],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥ä¼˜åŠ¿æè¿°ä¸€è‡´æ€§"""
        issues = []
        corrections = {}
        
        model_strengths = comparison_analysis.get('model_strengths', {})
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å‹éƒ½æœ‰ç›¸åŒçš„ä¼˜åŠ¿æè¿°
        if model_strengths:
            all_strengths = list(model_strengths.values())
            
            # è®¡ç®—æè¿°ç›¸ä¼¼åº¦
            similar_count = 0
            total_comparisons = 0
            
            for i in range(len(all_strengths)):
                for j in range(i + 1, len(all_strengths)):
                    total_comparisons += 1
                    # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå…±åŒè¯æ±‡ï¼‰
                    str1 = ' '.join(all_strengths[i])
                    str2 = ' '.join(all_strengths[j])
                    common_words = len(set(str1.split()) & set(str2.split()))
                    if common_words > 3:  # å¦‚æœæœ‰è¶…è¿‡3ä¸ªå…±åŒè¯æ±‡
                        similar_count += 1
            
            if total_comparisons > 0 and similar_count / total_comparisons > 0.7:
                issues.append({
                    'type': 'homogeneous_strengths',
                    'severity': 'critical',
                    'description': "æ£€æµ‹åˆ°æ¨¡å‹ä¼˜åŠ¿æè¿°é«˜åº¦ç›¸ä¼¼ï¼Œç¼ºä¹å·®å¼‚åŒ–",
                    'suggestion': "éœ€è¦åŸºäºå®é™…å†…å®¹é‡æ–°åˆ†æå„æ¨¡å‹ç‰¹è‰²"
                })
        
        # æ£€æŸ¥ä¼˜åŠ¿æè¿°ä¸è¯„åˆ†çš„ä¸€è‡´æ€§
        for model_name, strengths in model_strengths.items():
            if model_name in llm_evaluations:
                metrics = llm_evaluations[model_name]
                
                # å¦‚æœæè¿°è¯´æŸä¸ªç»´åº¦æ˜¯ä¼˜åŠ¿ï¼Œä½†è¯„åˆ†ä¸é«˜
                if any("å®Œæ•´æ€§" in s for s in strengths) and metrics.completeness_score < 7.0:
                    issues.append({
                        'type': 'strength_score_mismatch',
                        'source': model_name,
                        'severity': 'warning',
                        'description': f"{model_name}è¢«æè¿°ä¸ºå®Œæ•´æ€§ä¼˜åŠ¿ï¼Œä½†è¯„åˆ†ä»…{metrics.completeness_score:.1f}",
                        'suggestion': "è°ƒæ•´ä¼˜åŠ¿æè¿°æˆ–é‡æ–°è¯„åˆ†"
                    })
        
        return {'issues': issues, 'corrections': corrections}
    
    def _check_content_score_consistency(
        self,
        llm_evaluations: Dict[str, QualityMetrics],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥å†…å®¹åˆ†æä¸è¯„åˆ†ä¸€è‡´æ€§"""
        issues = []
        corrections = {}
        
        uniqueness_scores = content_analysis.get('content_uniqueness', {})
        
        # æ£€æŸ¥ç‹¬ç‰¹æ€§ä¸è¯„åˆ†çš„å…³ç³»
        for model_name, uniqueness in uniqueness_scores.items():
            if model_name in llm_evaluations and model_name != 'fusion_answer':
                metrics = llm_evaluations[model_name]
                
                # å¦‚æœå†…å®¹ç‹¬ç‰¹æ€§å¾ˆé«˜ä½†ç»¼åˆè¯„åˆ†å¾ˆä½
                if uniqueness > 0.4 and metrics.overall_score < 6.0:
                    issues.append({
                        'type': 'uniqueness_score_mismatch',
                        'source': model_name,
                        'severity': 'warning',
                        'description': f"{model_name}å†…å®¹ç‹¬ç‰¹æ€§é«˜({uniqueness:.1%})ä½†ç»¼åˆè¯„åˆ†ä½({metrics.overall_score:.1f})",
                        'suggestion': "æ£€æŸ¥è¯„åˆ†æ ‡å‡†æ˜¯å¦è¿‡äºä¸¥æ ¼æˆ–å­˜åœ¨åå·®"
                    })
                
                # å¦‚æœå†…å®¹ç‹¬ç‰¹æ€§å¾ˆä½ä½†è¯„åˆ†å¾ˆé«˜
                elif uniqueness < 0.1 and metrics.overall_score > 8.5:
                    issues.append({
                        'type': 'uniqueness_score_mismatch',
                        'source': model_name,
                        'severity': 'warning', 
                        'description': f"{model_name}å†…å®¹ç‹¬ç‰¹æ€§ä½({uniqueness:.1%})ä½†ç»¼åˆè¯„åˆ†é«˜({metrics.overall_score:.1f})",
                        'suggestion': "éªŒè¯é«˜åˆ†æ˜¯å¦åˆç†æˆ–æ˜¯å¦å­˜åœ¨è¯„åˆ†åå·®"
                    })
        
        return {'issues': issues, 'corrections': corrections}
    
    def _calculate_validated_quality_ranking(
        self,
        basic_metrics: Dict[str, QualityMetrics],
        llm_evaluations: Dict[str, QualityMetrics],
        consistency_check: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """è®¡ç®—ç»è¿‡ä¸€è‡´æ€§æ ¡æ­£çš„è´¨é‡æ’å"""
        
        # åº”ç”¨ä¸€è‡´æ€§æ ¡æ­£
        corrected_evaluations = dict(llm_evaluations)
        corrections = consistency_check.get('corrections', {})
        
        for correction_key, correction_value in corrections.items():
            if '_overall_score' in correction_key:
                source_name = correction_key.replace('_overall_score', '')
                if source_name in corrected_evaluations:
                    # åˆ›å»ºæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œåº”ç”¨æ ¡æ­£
                    original = corrected_evaluations[source_name]
                    corrected_evaluations[source_name] = QualityMetrics(
                        completeness_score=original.completeness_score,
                        accuracy_score=original.accuracy_score,
                        clarity_score=original.clarity_score,
                        relevance_score=original.relevance_score,
                        overall_score=correction_value,  # åº”ç”¨æ ¡æ­£å€¼
                        word_count=original.word_count,
                        sentence_count=original.sentence_count,
                        readability_score=original.readability_score,
                        information_density=original.information_density
                    )
        
        # ä½¿ç”¨æ ¡æ­£åçš„è¯„ä¼°ç”Ÿæˆæ’å
        return self._calculate_quality_ranking(basic_metrics, corrected_evaluations)

    def _analyze_fusion_effectiveness(
        self,
        llm_evaluations: Dict[str, QualityMetrics],
        comparison_analysis: Dict[str, Any],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        èåˆæ•ˆæœé‡åŒ–åˆ†æ - å¯¹æ¯”èåˆå‰åçš„è´¨é‡æå‡

        è¿”å›è¯¦ç»†çš„èåˆæ•ˆæœåˆ†æï¼ŒåŒ…æ‹¬ï¼š
        1. å„ç»´åº¦çš„æå‡é‡åŒ–
        2. èåˆä»·å€¼è¯„ä¼°
        3. å†…å®¹æ•´åˆæ•ˆæœ
        4. æœ€ä½³å®è·µå»ºè®®
        """
        print("ğŸ” æ­£åœ¨è¿›è¡Œèåˆæ•ˆæœé‡åŒ–åˆ†æ...")

        fusion_eval = llm_evaluations.get('fusion_answer')
        if not fusion_eval:
            return {
                'overall_effectiveness': 'N/A',
                'dimension_improvements': {},
                'fusion_value_score': 0,
                'recommendation': 'æ— èåˆå›ç­”æ•°æ®'
            }

        # è·å–æ‰€æœ‰å•æ¨¡å‹çš„è¯„åˆ†
        model_evaluations = {k: v for k, v in llm_evaluations.items() if k != 'fusion_answer'}

        if not model_evaluations:
            return {
                'overall_effectiveness': 'N/A',
                'dimension_improvements': {},
                'fusion_value_score': 0,
                'recommendation': 'æ— æ¨¡å‹å›ç­”æ•°æ®'
            }

        # 1. è®¡ç®—å„ç»´åº¦çš„ç»Ÿè®¡æ•°æ®
        dimension_stats = self._calculate_dimension_statistics(model_evaluations, fusion_eval)

        # 2. é‡åŒ–å„ç»´åº¦çš„æå‡
        dimension_improvements = self._quantify_dimension_improvements(
            model_evaluations, fusion_eval, dimension_stats
        )

        # 3. è¯„ä¼°èåˆä»·å€¼
        fusion_value = self._evaluate_fusion_value(
            dimension_improvements, dimension_stats, content_analysis
        )

        # 4. å†…å®¹æ•´åˆæ•ˆæœåˆ†æ
        integration_effectiveness = self._analyze_integration_effectiveness(
            model_evaluations, fusion_eval, content_analysis
        )

        # 5. ç”Ÿæˆèåˆæ•ˆæœç­‰çº§å’Œå»ºè®®
        effectiveness_level, recommendations = self._generate_effectiveness_assessment(
            dimension_improvements, fusion_value, integration_effectiveness
        )

        return {
            'overall_effectiveness': effectiveness_level,
            'dimension_stats': dimension_stats,
            'dimension_improvements': dimension_improvements,
            'fusion_value_score': fusion_value['total_score'],
            'fusion_value_breakdown': fusion_value,
            'integration_effectiveness': integration_effectiveness,
            'recommendations': recommendations,
            'summary': self._generate_fusion_summary(
                dimension_improvements, fusion_value, effectiveness_level
            )
        }

    def _calculate_dimension_statistics(
        self,
        model_evaluations: Dict[str, QualityMetrics],
        fusion_eval: QualityMetrics
    ) -> Dict[str, Any]:
        """è®¡ç®—å„ç»´åº¦çš„ç»Ÿè®¡æ•°æ®"""
        dimensions = ['completeness', 'accuracy', 'clarity', 'relevance', 'overall']
        stats = {}

        for dim in dimensions:
            scores = [getattr(eval, f'{dim}_score') for eval in model_evaluations.values()]
            fusion_score = getattr(fusion_eval, f'{dim}_score')

            stats[dim] = {
                'model_avg': sum(scores) / len(scores),
                'model_max': max(scores),
                'model_min': min(scores),
                'model_std': self._calculate_std(scores),
                'fusion_score': fusion_score,
                'models_count': len(scores)
            }

        return stats

    def _calculate_std(self, scores: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(scores) < 2:
            return 0.0
        mean = sum(scores) / len(scores)
        variance = sum((s - mean) ** 2 for s in scores) / len(scores)
        return variance ** 0.5

    def _quantify_dimension_improvements(
        self,
        model_evaluations: Dict[str, QualityMetrics],
        fusion_eval: QualityMetrics,
        dimension_stats: Dict[str, Any]
    ) -> Dict[str, Any]:
        """é‡åŒ–å„ç»´åº¦çš„æå‡"""
        improvements = {}
        dimensions = ['completeness', 'accuracy', 'clarity', 'relevance', 'overall']

        for dim in dimensions:
            stats = dimension_stats[dim]
            fusion_score = stats['fusion_score']
            model_avg = stats['model_avg']
            model_max = stats['model_max']

            # è®¡ç®—æå‡é‡
            improvement_vs_avg = fusion_score - model_avg
            improvement_vs_max = fusion_score - model_max

            # è®¡ç®—æå‡ç™¾åˆ†æ¯”
            improvement_pct_vs_avg = (improvement_vs_avg / model_avg * 100) if model_avg > 0 else 0
            improvement_pct_vs_max = (improvement_vs_max / model_max * 100) if model_max > 0 else 0

            # åˆ¤æ–­æå‡æ˜¾è‘—æ€§
            if improvement_vs_avg > 1.0:
                significance = "æ˜¾è‘—æå‡"
            elif improvement_vs_avg > 0.5:
                significance = "æ˜æ˜¾æå‡"
            elif improvement_vs_avg > 0.2:
                significance = "è½»å¾®æå‡"
            elif improvement_vs_avg > -0.2:
                significance = "åŸºæœ¬æŒå¹³"
            elif improvement_vs_avg > -0.5:
                significance = "è½»å¾®ä¸‹é™"
            else:
                significance = "æ˜æ˜¾ä¸‹é™"

            improvements[dim] = {
                'absolute_improvement_vs_avg': round(improvement_vs_avg, 2),
                'absolute_improvement_vs_max': round(improvement_vs_max, 2),
                'percentage_improvement_vs_avg': round(improvement_pct_vs_avg, 1),
                'percentage_improvement_vs_max': round(improvement_pct_vs_max, 1),
                'significance': significance,
                'fusion_score': fusion_score,
                'models_avg': model_avg,
                'models_max': model_max
            }

        return improvements

    def _evaluate_fusion_value(
        self,
        dimension_improvements: Dict[str, Any],
        dimension_stats: Dict[str, Any],
        content_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è¯„ä¼°èåˆä»·å€¼"""

        # 1. è´¨é‡æå‡ä»·å€¼ï¼ˆ40åˆ†ï¼‰
        quality_value = 0
        overall_imp = dimension_improvements['overall']['absolute_improvement_vs_avg']
        if overall_imp > 1.0:
            quality_value = 40
        elif overall_imp > 0.5:
            quality_value = 30
        elif overall_imp > 0.2:
            quality_value = 20
        elif overall_imp > 0:
            quality_value = 10

        # 2. å†…å®¹æ•´åˆä»·å€¼ï¼ˆ30åˆ†ï¼‰
        integration_value = 0
        content_themes = content_analysis.get('content_themes', {})
        fusion_additions = content_themes.get('fusion_additions', [])
        if len(fusion_additions) >= 3:
            integration_value = 30
        elif len(fusion_additions) >= 2:
            integration_value = 20
        elif len(fusion_additions) >= 1:
            integration_value = 10

        # 3. ä¸€è‡´æ€§ä»·å€¼ï¼ˆ15åˆ†ï¼‰
        consistency_value = 15  # é»˜è®¤æ»¡åˆ†
        # å¦‚æœèåˆå›ç­”æ¯”æœ€ä½³å•æ¨¡å‹è¿˜ä½ï¼Œæ‰£åˆ†
        if dimension_improvements['overall']['absolute_improvement_vs_max'] < -0.5:
            consistency_value = 0
        elif dimension_improvements['overall']['absolute_improvement_vs_max'] < 0:
            consistency_value = 10

        # 4. å…¨é¢æ€§ä»·å€¼ï¼ˆ15åˆ†ï¼‰
        comprehensiveness_value = 0
        positive_dims = sum(1 for dim in ['completeness', 'accuracy', 'clarity', 'relevance']
                           if dimension_improvements[dim]['absolute_improvement_vs_avg'] > 0)
        comprehensiveness_value = positive_dims * 3.75  # æ¯ä¸ªç»´åº¦3.75åˆ†

        total_score = quality_value + integration_value + consistency_value + comprehensiveness_value

        return {
            'total_score': round(total_score, 1),
            'quality_value': quality_value,
            'integration_value': integration_value,
            'consistency_value': consistency_value,
            'comprehensiveness_value': comprehensiveness_value,
            'max_score': 100,
            'level': self._get_value_level(total_score)
        }

    def _get_value_level(self, score: float) -> str:
        """è·å–ä»·å€¼ç­‰çº§"""
        if score >= 80:
            return "ä¼˜ç§€"
        elif score >= 60:
            return "è‰¯å¥½"
        elif score >= 40:
            return "ä¸€èˆ¬"
        elif score >= 20:
            return "è¾ƒå·®"
        else:
            return "å¾ˆå·®"

    def _analyze_integration_effectiveness(
        self,
        model_evaluations: Dict[str, QualityMetrics],
        fusion_eval: QualityMetrics,
        content_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†æå†…å®¹æ•´åˆæ•ˆæœ"""

        # 1. å†…å®¹è¦†ç›–åº¦
        content_themes = content_analysis.get('content_themes', {})
        common_points = content_themes.get('common_points', [])
        fusion_additions = content_themes.get('fusion_additions', [])
        model_unique_points = content_themes.get('model_unique_points', {})

        # è®¡ç®—æ€»çš„ç‹¬ç‰¹è§‚ç‚¹æ•°
        total_unique_points = sum(len(points) for points in model_unique_points.values())

        coverage_score = min(100, (len(common_points) + total_unique_points) * 10)

        # 2. æ–°å¢å†…å®¹ä»·å€¼
        addition_value = len(fusion_additions) * 20  # æ¯ä¸ªæ–°å¢ç‚¹20åˆ†
        addition_value = min(100, addition_value)

        # 3. å†…å®¹ç‹¬ç‰¹æ€§
        uniqueness_scores = content_analysis.get('content_uniqueness', {})
        fusion_uniqueness = uniqueness_scores.get('fusion_answer', 0)

        # 4. ç»“æ„ä¼˜åŒ–
        structure_patterns = content_analysis.get('structure_patterns', {})
        fusion_structure = structure_patterns.get('fusion_answer', {})
        model_structures = {k: v for k, v in structure_patterns.items() if k != 'fusion_answer'}

        # è®¡ç®—ç»“æ„æ”¹è¿›
        if model_structures:
            avg_structure_score = sum(
                s.get('has_structured_format', False) for s in model_structures.values()
            ) / len(model_structures)
            fusion_has_structure = fusion_structure.get('has_structured_format', False)
            structure_improvement = fusion_has_structure and avg_structure_score < 0.5
        else:
            structure_improvement = False

        return {
            'coverage_score': round(coverage_score, 1),
            'addition_value': round(addition_value, 1),
            'fusion_uniqueness': round(fusion_uniqueness * 100, 1),
            'structure_improved': structure_improvement,
            'common_points_covered': len(common_points),
            'new_content_added': len(fusion_additions),
            'total_unique_perspectives': total_unique_points
        }

    def _generate_effectiveness_assessment(
        self,
        dimension_improvements: Dict[str, Any],
        fusion_value: Dict[str, Any],
        integration_effectiveness: Dict[str, Any]
    ) -> tuple:
        """ç”Ÿæˆèåˆæ•ˆæœç­‰çº§å’Œå»ºè®®"""

        # ç»¼åˆè¯„ä¼°ç­‰çº§
        total_score = fusion_value['total_score']
        overall_imp = dimension_improvements['overall']['absolute_improvement_vs_avg']

        if total_score >= 80 and overall_imp > 0.5:
            level = "å“è¶Š"
            emoji = "ğŸŒŸ"
        elif total_score >= 60 and overall_imp > 0.2:
            level = "ä¼˜ç§€"
            emoji = "â­"
        elif total_score >= 40 and overall_imp >= 0:
            level = "è‰¯å¥½"
            emoji = "âœ…"
        elif total_score >= 20:
            level = "ä¸€èˆ¬"
            emoji = "âš ï¸"
        else:
            level = "éœ€æ”¹è¿›"
            emoji = "âŒ"

        effectiveness_level = f"{emoji} {level} ({total_score:.1f}/100)"

        # ç”Ÿæˆå»ºè®®
        recommendations = []

        # åŸºäºå„ç»´åº¦æå‡æƒ…å†µç»™å‡ºå»ºè®®
        for dim, imp_data in dimension_improvements.items():
            if dim == 'overall':
                continue
            if imp_data['absolute_improvement_vs_avg'] < 0:
                dim_name_map = {
                    'completeness': 'å®Œæ•´æ€§',
                    'accuracy': 'å‡†ç¡®æ€§',
                    'clarity': 'æ¸…æ™°åº¦',
                    'relevance': 'ç›¸å…³æ€§'
                }
                recommendations.append(
                    f"èåˆè¿‡ç¨‹ä¸­{dim_name_map[dim]}æœ‰æ‰€ä¸‹é™ï¼Œå»ºè®®åŠ å¼ºè¯¥ç»´åº¦çš„å†…å®¹æ•´åˆ"
                )

        # åŸºäºå†…å®¹æ•´åˆæ•ˆæœç»™å‡ºå»ºè®®
        if integration_effectiveness['new_content_added'] == 0:
            recommendations.append("èåˆå›ç­”æœªæ·»åŠ æ–°å†…å®¹ï¼Œå»ºè®®åœ¨èåˆæ—¶å¢åŠ ç»¼åˆæ€§è§è§£")

        if integration_effectiveness['fusion_uniqueness'] < 10:
            recommendations.append("èåˆå›ç­”ç‹¬ç‰¹æ€§è¾ƒä½ï¼Œå»ºè®®å¢åŠ ç‹¬ç‰¹çš„ç»¼åˆæ€§åˆ†æ")

        if not integration_effectiveness['structure_improved']:
            recommendations.append("å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å›ç­”çš„ç»“æ„ç»„ç»‡ï¼Œæé«˜å¯è¯»æ€§")

        # åŸºäºèåˆä»·å€¼ç»™å‡ºå»ºè®®
        if fusion_value['quality_value'] < 20:
            recommendations.append("æ•´ä½“è´¨é‡æå‡æœ‰é™ï¼Œå»ºè®®ä¼˜åŒ–èåˆç®—æ³•æˆ–æ¨¡å‹é€‰æ‹©")

        if fusion_value['integration_value'] < 15:
            recommendations.append("å†…å®¹æ•´åˆä»·å€¼è¾ƒä½ï¼Œå»ºè®®æ›´å¥½åœ°èåˆå„æ¨¡å‹çš„ä¼˜åŠ¿è§‚ç‚¹")

        # å¦‚æœæ²¡æœ‰éœ€è¦æ”¹è¿›çš„åœ°æ–¹ï¼Œç»™å‡ºæ­£é¢åé¦ˆ
        if not recommendations:
            if total_score >= 80:
                recommendations.append("èåˆæ•ˆæœä¼˜ç§€ï¼ŒæˆåŠŸæ•´åˆäº†å¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿")
            elif total_score >= 60:
                recommendations.append("èåˆæ•ˆæœè‰¯å¥½ï¼Œåœ¨å¤šä¸ªç»´åº¦ä¸Šå®ç°äº†æå‡")
            else:
                recommendations.append("èåˆåŸºæœ¬è¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œå¯ç»§ç»­ä¿æŒ")

        return effectiveness_level, recommendations

    def _generate_fusion_summary(
        self,
        dimension_improvements: Dict[str, Any],
        fusion_value: Dict[str, Any],
        effectiveness_level: str
    ) -> str:
        """ç”Ÿæˆèåˆæ•ˆæœæ‘˜è¦"""

        overall_imp = dimension_improvements['overall']['absolute_improvement_vs_avg']
        overall_pct = dimension_improvements['overall']['percentage_improvement_vs_avg']

        # æ‰¾å‡ºæå‡æœ€å¤§çš„ç»´åº¦
        best_improved_dim = None
        best_improvement = -999
        for dim, imp_data in dimension_improvements.items():
            if dim != 'overall' and imp_data['absolute_improvement_vs_avg'] > best_improvement:
                best_improvement = imp_data['absolute_improvement_vs_avg']
                best_improved_dim = dim

        dim_name_map = {
            'completeness': 'å®Œæ•´æ€§',
            'accuracy': 'å‡†ç¡®æ€§',
            'clarity': 'æ¸…æ™°åº¦',
            'relevance': 'ç›¸å…³æ€§'
        }

        summary_parts = []

        # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•´ä½“æ•ˆæœ
        if overall_imp > 0.5:
            summary_parts.append(
                f"èåˆæ•ˆæœ{effectiveness_level}ï¼Œç»¼åˆè´¨é‡ç›¸æ¯”æ¨¡å‹å¹³å‡æå‡{overall_imp:.1f}åˆ†({overall_pct:+.1f}%)"
            )
        elif overall_imp > 0:
            summary_parts.append(
                f"èåˆæ•ˆæœ{effectiveness_level}ï¼Œç»¼åˆè´¨é‡ç›¸æ¯”æ¨¡å‹å¹³å‡æå‡{overall_imp:.1f}åˆ†"
            )
        else:
            summary_parts.append(
                f"èåˆæ•ˆæœ{effectiveness_level}ï¼Œç»¼åˆè´¨é‡ä¸æ¨¡å‹å¹³å‡åŸºæœ¬æŒå¹³"
            )

        # ç¬¬äºŒéƒ¨åˆ†ï¼šæœ€ä½³æå‡ç»´åº¦
        if best_improved_dim and best_improvement > 0.2:
            summary_parts.append(
                f"{dim_name_map[best_improved_dim]}æå‡æœ€ä¸ºæ˜¾è‘—({best_improvement:+.1f}åˆ†)"
            )

        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»·å€¼è¯„ä¼°
        value_score = fusion_value['total_score']
        value_level = fusion_value['level']
        summary_parts.append(
            f"èåˆä»·å€¼{value_level}({value_score:.1f}/100åˆ†)"
        )

        return "ï¼›".join(summary_parts) + "ã€‚"

    def _analyze_speed_quality_tradeoff(
        self,
        llm_responses: List[Dict],
        llm_evaluations: Dict[str, QualityMetrics]
    ) -> Dict[str, Any]:
        """
        é€Ÿåº¦-è´¨é‡æƒè¡¡åˆ†æ

        åˆ†æå„æ¨¡å‹çš„å“åº”æ—¶é—´ä¸è´¨é‡ä¹‹é—´çš„å…³ç³»ï¼Œè¯†åˆ«ï¼š
        1. é€Ÿåº¦æœ€å¿«çš„æ¨¡å‹
        2. è´¨é‡æœ€é«˜çš„æ¨¡å‹
        3. æ€§ä»·æ¯”æœ€ä¼˜çš„æ¨¡å‹ï¼ˆç»¼åˆé€Ÿåº¦å’Œè´¨é‡ï¼‰
        4. åœºæ™¯åŒ–æ¨è

        Args:
            llm_responses: å„æ¨¡å‹çš„å“åº”æ•°æ®ï¼ˆåŒ…å«response_timeï¼‰
            llm_evaluations: å„æ¨¡å‹çš„è´¨é‡è¯„ä¼°ç»“æœ

        Returns:
            é€Ÿåº¦-è´¨é‡æƒè¡¡åˆ†æç»“æœ
        """

        # 1. æå–æˆåŠŸå“åº”çš„æ¨¡å‹æ•°æ®
        model_data = []
        for response in llm_responses:
            if response.get('success') and response['model_name'] in llm_evaluations:
                model_name = response['model_name']
                response_time = response.get('response_time', 0)
                quality_metrics = llm_evaluations[model_name]

                model_data.append({
                    'model_name': model_name,
                    'response_time': response_time,
                    'quality_score': quality_metrics.overall_score,
                    'completeness': quality_metrics.completeness_score,
                    'accuracy': quality_metrics.accuracy_score,
                    'clarity': quality_metrics.clarity_score,
                    'relevance': quality_metrics.relevance_score
                })

        if not model_data:
            return {
                'available': False,
                'message': 'æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œé€Ÿåº¦-è´¨é‡æƒè¡¡åˆ†æ'
            }

        # 2. è®¡ç®—æ•ˆç‡æŒ‡æ ‡ï¼ˆè´¨é‡/æ—¶é—´ï¼‰
        for model in model_data:
            if model['response_time'] > 0:
                # æ•ˆç‡å¾—åˆ† = è´¨é‡åˆ†æ•° / å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
                model['efficiency_score'] = model['quality_score'] / model['response_time']
            else:
                model['efficiency_score'] = 0

        # 3. è¯†åˆ«å„ç±»æœ€ä½³æ¨¡å‹
        fastest_model = min(model_data, key=lambda x: x['response_time'])
        highest_quality_model = max(model_data, key=lambda x: x['quality_score'])
        most_efficient_model = max(model_data, key=lambda x: x['efficiency_score'])

        # 4. è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_response_time = sum(m['response_time'] for m in model_data) / len(model_data)
        avg_quality_score = sum(m['quality_score'] for m in model_data) / len(model_data)
        avg_efficiency = sum(m['efficiency_score'] for m in model_data) / len(model_data)

        # 5. æ¨¡å‹åˆ†ç±»ï¼ˆå¿«é€Ÿå‹ã€è´¨é‡å‹ã€å¹³è¡¡å‹ï¼‰
        model_categories = self._categorize_models(
            model_data, avg_response_time, avg_quality_score, avg_efficiency
        )

        # 6. ç›¸å…³æ€§åˆ†æï¼ˆé€Ÿåº¦ä¸è´¨é‡çš„å…³ç³»ï¼‰
        correlation_analysis = self._analyze_speed_quality_correlation(model_data)

        # 7. åœºæ™¯åŒ–æ¨è
        scenario_recommendations = self._generate_scenario_recommendations(
            fastest_model, highest_quality_model, most_efficient_model, model_categories
        )

        # 8. æƒè¡¡è¯„ä¼°
        tradeoff_assessment = self._assess_tradeoffs(
            fastest_model, highest_quality_model, most_efficient_model,
            avg_response_time, avg_quality_score
        )

        return {
            'available': True,
            'fastest_model': {
                'name': fastest_model['model_name'],
                'response_time': fastest_model['response_time'],
                'quality_score': fastest_model['quality_score'],
                'efficiency_score': fastest_model['efficiency_score']
            },
            'highest_quality_model': {
                'name': highest_quality_model['model_name'],
                'response_time': highest_quality_model['response_time'],
                'quality_score': highest_quality_model['quality_score'],
                'efficiency_score': highest_quality_model['efficiency_score']
            },
            'most_efficient_model': {
                'name': most_efficient_model['model_name'],
                'response_time': most_efficient_model['response_time'],
                'quality_score': most_efficient_model['quality_score'],
                'efficiency_score': most_efficient_model['efficiency_score']
            },
            'statistics': {
                'avg_response_time': avg_response_time,
                'avg_quality_score': avg_quality_score,
                'avg_efficiency': avg_efficiency,
                'speed_range': {
                    'min': fastest_model['response_time'],
                    'max': max(m['response_time'] for m in model_data)
                },
                'quality_range': {
                    'min': min(m['quality_score'] for m in model_data),
                    'max': highest_quality_model['quality_score']
                }
            },
            'model_categories': model_categories,
            'correlation_analysis': correlation_analysis,
            'scenario_recommendations': scenario_recommendations,
            'tradeoff_assessment': tradeoff_assessment,
            'all_models_data': model_data  # å®Œæ•´æ•°æ®ä¾›è¿›ä¸€æ­¥åˆ†æ
        }

    def _categorize_models(
        self,
        model_data: List[Dict],
        avg_time: float,
        avg_quality: float,
        avg_efficiency: float
    ) -> Dict[str, List[str]]:
        """å°†æ¨¡å‹åˆ†ç±»ä¸ºå¿«é€Ÿå‹ã€è´¨é‡å‹ã€å¹³è¡¡å‹"""

        fast_models = []
        quality_models = []
        balanced_models = []

        for model in model_data:
            time = model['response_time']
            quality = model['quality_score']
            efficiency = model['efficiency_score']

            # å¿«é€Ÿå‹ï¼šå“åº”æ—¶é—´æ˜¾è‘—ä½äºå¹³å‡å€¼
            is_fast = time < avg_time * 0.8
            # è´¨é‡å‹ï¼šè´¨é‡æ˜¾è‘—é«˜äºå¹³å‡å€¼
            is_high_quality = quality > avg_quality * 1.1
            # å¹³è¡¡å‹ï¼šæ•ˆç‡é«˜äºå¹³å‡å€¼
            is_balanced = efficiency > avg_efficiency

            if is_fast and not is_high_quality:
                fast_models.append(model['model_name'])
            elif is_high_quality and not is_fast:
                quality_models.append(model['model_name'])
            elif is_balanced or (is_fast and is_high_quality):
                balanced_models.append(model['model_name'])
            else:
                # é»˜è®¤å½’ç±»åˆ°å¹³è¡¡å‹
                balanced_models.append(model['model_name'])

        return {
            'fast_models': fast_models,
            'quality_models': quality_models,
            'balanced_models': balanced_models
        }

    def _analyze_speed_quality_correlation(self, model_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æé€Ÿåº¦ä¸è´¨é‡çš„ç›¸å…³æ€§"""

        if len(model_data) < 2:
            return {
                'correlation_type': 'insufficient_data',
                'description': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æç›¸å…³æ€§'
            }

        # ç®€å•çš„ç›¸å…³æ€§åˆ†æ
        times = [m['response_time'] for m in model_data]
        qualities = [m['quality_score'] for m in model_data]

        # è®¡ç®—æ’åºä¸€è‡´æ€§ï¼ˆå¿«çš„æ˜¯å¦è´¨é‡ä¹Ÿé«˜ï¼‰
        time_ranks = self._get_ranks(times, reverse=True)  # æ—¶é—´è¶ŠçŸ­æ’åè¶Šé«˜
        quality_ranks = self._get_ranks(qualities, reverse=False)  # è´¨é‡è¶Šé«˜æ’åè¶Šé«˜

        # è®¡ç®—rankç›¸å…³æ€§ï¼ˆç®€åŒ–ç‰ˆSpearmanç›¸å…³ï¼‰
        rank_diffs = sum(abs(time_ranks[i] - quality_ranks[i]) for i in range(len(model_data)))
        max_diff = len(model_data) * (len(model_data) - 1)
        similarity = 1 - (rank_diffs / max_diff) if max_diff > 0 else 0

        if similarity > 0.6:
            correlation_type = 'positive'
            description = 'é€Ÿåº¦å¿«çš„æ¨¡å‹å¾€å¾€è´¨é‡ä¹Ÿè¾ƒé«˜ï¼ˆæ­£ç›¸å…³ï¼‰'
        elif similarity < 0.4:
            correlation_type = 'negative'
            description = 'é€Ÿåº¦å¿«çš„æ¨¡å‹è´¨é‡ç›¸å¯¹è¾ƒä½ï¼ˆè´Ÿç›¸å…³/æƒè¡¡å…³ç³»ï¼‰'
        else:
            correlation_type = 'neutral'
            description = 'é€Ÿåº¦ä¸è´¨é‡ä¹‹é—´æ— æ˜æ˜¾ç›¸å…³æ€§'

        return {
            'correlation_type': correlation_type,
            'similarity_score': similarity,
            'description': description
        }

    def _get_ranks(self, values: List[float], reverse: bool = False) -> List[int]:
        """è·å–æ•°å€¼çš„æ’å"""
        sorted_indices = sorted(range(len(values)), key=lambda i: values[i], reverse=reverse)
        ranks = [0] * len(values)
        for rank, idx in enumerate(sorted_indices):
            ranks[idx] = rank
        return ranks

    def _generate_scenario_recommendations(
        self,
        fastest: Dict,
        highest_quality: Dict,
        most_efficient: Dict,
        categories: Dict
    ) -> Dict[str, str]:
        """ç”Ÿæˆåœºæ™¯åŒ–æ¨è"""

        recommendations = {}

        # æ—¶é—´æ•æ„Ÿåœºæ™¯
        recommendations['time_critical'] = (
            f"æ¨èä½¿ç”¨ {fastest['model_name']} "
            f"(å“åº”æ—¶é—´: {fastest['response_time']:.2f}ç§’, "
            f"è´¨é‡: {fastest['quality_score']:.1f}/10)"
        )

        # è´¨é‡ä¼˜å…ˆåœºæ™¯
        recommendations['quality_critical'] = (
            f"æ¨èä½¿ç”¨ {highest_quality['model_name']} "
            f"(è´¨é‡: {highest_quality['quality_score']:.1f}/10, "
            f"å“åº”æ—¶é—´: {highest_quality['response_time']:.2f}ç§’)"
        )

        # ç»¼åˆåœºæ™¯ï¼ˆæ€§ä»·æ¯”ï¼‰
        recommendations['balanced'] = (
            f"æ¨èä½¿ç”¨ {most_efficient['model_name']} "
            f"(æ•ˆç‡å¾—åˆ†: {most_efficient['efficiency_score']:.2f}, "
            f"è´¨é‡: {most_efficient['quality_score']:.1f}/10, "
            f"å“åº”æ—¶é—´: {most_efficient['response_time']:.2f}ç§’)"
        )

        # ç”Ÿäº§ç¯å¢ƒæ¨è
        if categories['balanced_models']:
            recommendations['production'] = (
                f"ç”Ÿäº§ç¯å¢ƒæ¨èå¹³è¡¡å‹æ¨¡å‹: {', '.join(categories['balanced_models'])}"
            )
        else:
            recommendations['production'] = (
                f"ç”Ÿäº§ç¯å¢ƒå¯è€ƒè™‘æ•ˆç‡æœ€ä¼˜çš„ {most_efficient['model_name']}"
            )

        return recommendations

    def _assess_tradeoffs(
        self,
        fastest: Dict,
        highest_quality: Dict,
        most_efficient: Dict,
        avg_time: float,
        avg_quality: float
    ) -> Dict[str, Any]:
        """è¯„ä¼°æƒè¡¡å…³ç³»"""

        assessments = []

        # 1. æœ€å¿«æ¨¡å‹çš„è´¨é‡æŸå¤±
        if fastest['model_name'] != highest_quality['model_name']:
            quality_loss = highest_quality['quality_score'] - fastest['quality_score']
            time_gain = highest_quality['response_time'] - fastest['response_time']

            if quality_loss > 1.0:
                assessments.append({
                    'type': 'speed_over_quality',
                    'message': f"é€‰æ‹©æœ€å¿«æ¨¡å‹ {fastest['model_name']} å¯èŠ‚çœ {time_gain:.2f}ç§’ï¼Œä½†è´¨é‡é™ä½ {quality_loss:.1f}åˆ†",
                    'severity': 'high' if quality_loss > 2.0 else 'medium'
                })
            else:
                assessments.append({
                    'type': 'speed_benefit',
                    'message': f"é€‰æ‹©æœ€å¿«æ¨¡å‹ {fastest['model_name']} å¯èŠ‚çœ {time_gain:.2f}ç§’ï¼Œè´¨é‡æŸå¤±è¾ƒå°({quality_loss:.1f}åˆ†)",
                    'severity': 'low'
                })

        # 2. æœ€é«˜è´¨é‡æ¨¡å‹çš„æ—¶é—´ä»£ä»·
        if highest_quality['model_name'] != fastest['model_name']:
            time_cost = highest_quality['response_time'] - fastest['response_time']
            quality_gain = highest_quality['quality_score'] - fastest['quality_score']

            if time_cost > 3.0:
                assessments.append({
                    'type': 'quality_over_speed',
                    'message': f"é€‰æ‹©æœ€é«˜è´¨é‡æ¨¡å‹ {highest_quality['model_name']} éœ€é¢å¤–ç­‰å¾… {time_cost:.2f}ç§’ï¼Œè´¨é‡æå‡ {quality_gain:.1f}åˆ†",
                    'severity': 'high' if time_cost > 5.0 else 'medium'
                })
            else:
                assessments.append({
                    'type': 'quality_benefit',
                    'message': f"é€‰æ‹©æœ€é«˜è´¨é‡æ¨¡å‹ {highest_quality['model_name']} ä»…éœ€é¢å¤– {time_cost:.2f}ç§’ï¼Œè´¨é‡æå‡ {quality_gain:.1f}åˆ†",
                    'severity': 'low'
                })

        # 3. æœ€ä¼˜æ•ˆç‡æ¨¡å‹æ¨è
        if most_efficient['model_name'] not in [fastest['model_name'], highest_quality['model_name']]:
            assessments.append({
                'type': 'efficiency_recommendation',
                'message': f"{most_efficient['model_name']} æä¾›æœ€ä½³çš„é€Ÿåº¦-è´¨é‡å¹³è¡¡ï¼ˆæ•ˆç‡: {most_efficient['efficiency_score']:.2f}ï¼‰",
                'severity': 'info'
            })

        # 4. æ•´ä½“æƒè¡¡è¯„ä¼°
        if avg_time < 2.0:
            overall_assessment = "æ‰€æœ‰æ¨¡å‹å“åº”éƒ½å¾ˆå¿«ï¼Œå¯ä¼˜å…ˆè€ƒè™‘è´¨é‡"
        elif avg_quality < 7.0:
            overall_assessment = "æ•´ä½“è´¨é‡æœ‰å¾…æå‡ï¼Œå»ºè®®ä¼˜å…ˆé€‰æ‹©é«˜è´¨é‡æ¨¡å‹"
        else:
            overall_assessment = "é€Ÿåº¦å’Œè´¨é‡éƒ½è¡¨ç°è‰¯å¥½ï¼Œå¯æ ¹æ®å…·ä½“åœºæ™¯çµæ´»é€‰æ‹©"

        return {
            'individual_tradeoffs': assessments,
            'overall_assessment': overall_assessment
        }
