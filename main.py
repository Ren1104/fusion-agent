#!/usr/bin/env python3
"""AI Fusion å‘½ä»¤è¡Œå…¥å£"""

import asyncio
import os
from dotenv import load_dotenv

load_dotenv()

from flow import create_ai_fusion_flow
from providers import ModelRegistry, ModelInfo
from analyzer import ModelConfig
from langfuse_tracer import create_trace, finish_observation, flush


def check_env():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡"""
    if not os.getenv("OPENAI_API_KEY") and not os.getenv("ANTHROPIC_API_KEY"):
        print("âŒ è¯·è‡³å°‘é…ç½®ä¸€ä¸ª API Key:")
        print("   OPENAI_API_KEY æˆ– ANTHROPIC_API_KEY")
        return False
    return True


async def chat():
    """äº¤äº’å¼èŠå¤©"""
    print("ğŸŒŸ æ¬¢è¿ä½¿ç”¨ AI Fusion!")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")

    # åˆå§‹åŒ– ModelRegistry (æ–°æ¶æ„)
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹æ³¨å†Œä¸­å¿ƒ...")
    registry = ModelRegistry()
    available_models_info = await registry.discover_all_models()

    if not available_models_info:
        print("âŒ æœªå‘ç°ä»»ä½•å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        return

    print(f"âœ… æˆåŠŸåŠ è½½ {len(available_models_info)} ä¸ªæ¨¡å‹\n")

    # è½¬æ¢ ModelInfo ä¸º ModelConfigï¼ˆå‘åå…¼å®¹ï¼‰
    available_models = [
        ModelConfig(
            name=model.model_id,
            provider=model.provider,
            api_key="",  # ä¸å†éœ€è¦ï¼Œregistry ä¼šå¤„ç†
            base_url=None
        )
        for model in available_models_info
    ]

    flow = create_ai_fusion_flow()

    while True:
        try:
            question = input("ğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()

            if question.lower() in ["quit", "exit", "é€€å‡º"]:
                print("ğŸ‘‹ å†è§!")
                break

            if not question:
                continue

            # åˆ›å»º Langfuse trace
            trace = create_trace(
                name=question[:100],  # ä½¿ç”¨é—®é¢˜ä½œä¸º trace åç§°ï¼ˆæˆªæ–­åˆ°100å­—ç¬¦ï¼‰
                metadata={"source": "main_cli"},
                input_data={"question": question}
            )
            trace_id = trace.trace_id if trace else None
            trace_observation_id = trace.id if trace else None

            # é€šè¿‡ shared ä¼ é€’ registryã€æ¨¡å‹åˆ—è¡¨å’Œ trace_id
            shared = {
                "user_question": question,
                "registry": registry,
                "available_models": available_models,
                "trace_id": trace_id,  # ä¼ é€’ trace_id ç»™æ‰€æœ‰èŠ‚ç‚¹
                "langfuse_trace_observation_id": trace_observation_id,
                "langfuse_trace": trace,
            }

            flow_error: Exception | None = None
            try:
                await flow.run_async(shared)
            except Exception as exc:  # è®©å¤–å±‚æ•è·åæç¤ºç”¨æˆ·
                flow_error = exc
                raise
            finally:
                if trace:
                    finish_observation(
                        trace,
                        output_data={
                            "final_answer": shared.get("final_answer"),
                            "quality_analysis": shared.get("quality_analysis"),
                            "report_path": shared.get("report_path"),
                        },
                        metadata={
                            "selected_models": [m.name for m in shared.get("selected_models", [])],
                            "analysis_method": shared.get("analysis_method"),
                        },
                        level="ERROR"
                        if flow_error or not shared.get("final_answer")
                        else None,
                        status_message=str(flow_error) if flow_error else None,
                    )

                flush()

            print(f"\nğŸ¯ å›ç­”:\n{shared.get('final_answer', 'å¤„ç†å¤±è´¥')}\n")

            if trace_id:
                print(f"ğŸ“Š Langfuse Trace ID: {trace_id}\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}\n")


if __name__ == "__main__":
    if check_env():
        asyncio.run(chat())
