"""
AI Fusion èŠ‚ç‚¹å®ç°
åŒ…å«æ¨¡å‹é€‰æ‹©å™¨ã€å¹¶å‘LLMè°ƒç”¨å’Œå›ç­”èåˆç­‰æ ¸å¿ƒèŠ‚ç‚¹
"""

import asyncio
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from pocketflow import AsyncNode
from analyzer import call_llm_async, ModelConfig, AIFusionSmartSelector, AIFusionQualityAnalyzer
from reporter import AIFusionReporter
from langfuse_tracer import create_span, finish_observation


class ModelSelectorNode(AsyncNode):
    """
    æ¨¡å‹é€‰æ‹©å™¨èŠ‚ç‚¹
    æ ¹æ®ç”¨æˆ·é—®é¢˜çš„ç‰¹å¾è‡ªåŠ¨é€‰æ‹©ä¸‰ä¸ªæœ€åˆé€‚çš„LLMæ¨¡å‹
    """

    def __init__(self):
        super().__init__(max_retries=2, wait=1)
        self.smart_selector = None  # å°†åœ¨ prep_async ä¸­åˆå§‹åŒ–
        # ä¿ç•™ä¼ ç»Ÿé€‰æ‹©ç­–ç•¥ä½œä¸ºå›é€€
        self.fallback_criteria = {
            "æŠ€æœ¯/ç¼–ç¨‹": ["gpt-41-0414-global", "claude_sonnet4", "qwen-max"],
            "åˆ›æ„å†™ä½œ": ["claude_sonnet4", "qwen-max", "claude37_sonnet_new"],
            "æ•°å­¦/é€»è¾‘": ["gpt-41-0414-global", "claude_sonnet4", "qwen-max"],
            "æ—¥å¸¸å¯¹è¯": ["gpt-41-mini-0414-global", "qwen-plus", "claude37_sonnet_new"],
            "ä¸“ä¸šçŸ¥è¯†": ["claude_sonnet4", "qwen-max", "gpt-41-0414-global"],
            "ç¿»è¯‘": ["qwen-max", "claude_sonnet4", "gpt-41-0414-global"],
            "åˆ†ææ€»ç»“": ["claude_sonnet4", "qwen-max", "gpt-41-0414-global"],
            "é»˜è®¤": ["claude_sonnet4", "gpt-41-0414-global", "qwen-max"]
        }

    async def prep_async(self, shared):
        """å‡†å¤‡é˜¶æ®µï¼šè·å–ç”¨æˆ·é—®é¢˜ã€å¯ç”¨æ¨¡å‹ã€registry å’Œ trace_id"""
        question = shared.get("user_question", "")
        available_models = shared.get("available_models", [])
        registry = shared.get("registry")
        trace_id = shared.get("trace_id")
        trace_observation_id = shared.get("langfuse_trace_observation_id")

        if not question:
            raise ValueError("ç”¨æˆ·é—®é¢˜ä¸èƒ½ä¸ºç©º")

        if not available_models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")

        # åˆå§‹åŒ– smart_selectorï¼ˆä½¿ç”¨ registryï¼‰
        if self.smart_selector is None:
            self.smart_selector = AIFusionSmartSelector(registry=registry)

        return {
            "question": question,
            "available_models": available_models,
            "trace_id": trace_id,
            "trace_observation_id": trace_observation_id
        }

    async def exec_async(self, inputs):
        """æ‰§è¡Œé˜¶æ®µï¼šæ™ºèƒ½åˆ†æé—®é¢˜å¹¶é€‰æ‹©åˆé€‚çš„æ¨¡å‹"""
        question = inputs["question"]
        available_models = inputs["available_models"]
        trace_id = inputs.get("trace_id")
        parent_observation_id = inputs.get("trace_observation_id")

        selector_span = create_span(
            trace_id,
            name="ModelSelector",
            parent_observation_id=parent_observation_id,
            input_data={
                "question": question,
                "available_models": [model.name for model in available_models],
            },
            metadata={"node": "ModelSelector"},
        )
        current_parent_id = selector_span.id if selector_span else parent_observation_id

        print("ğŸ§  æ­£åœ¨è¿›è¡Œæ™ºèƒ½æ¨¡å‹é€‰æ‹©åˆ†æ...")

        try:
            # ä½¿ç”¨æ™ºèƒ½é€‰æ‹©å™¨
            recommendation = await self.smart_selector.intelligent_model_selection(
                question,
                available_models,
                trace_id=trace_id,
                parent_observation_id=current_parent_id,
            )

            selected_models = recommendation.get('selected_models', [])
            analysis_method = recommendation.get('analysis_method', 'intelligent_llm')

            # æ˜¾ç¤ºåˆ†æç»“æœ
            self._display_selection_analysis(recommendation)

            result = {
                "selected_model_names": selected_models,
                "question_type": recommendation.get('problem_analysis', {}).get('question_type', 'æ™ºèƒ½åˆ†æ'),
                "selection_analysis": recommendation,
                "analysis_method": analysis_method
            }

            finish_observation(
                selector_span,
                output_data={
                    "selected_model_names": selected_models,
                    "analysis_method": analysis_method,
                    "question_type": result["question_type"],
                },
                metadata={"node": "ModelSelector"},
            )

            return result

        except Exception as e:
            print(f"âš ï¸ æ™ºèƒ½æ¨¡å‹é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {str(e)}")
            result = await self._fallback_selection(
                question,
                available_models,
                trace_id=trace_id,
                parent_observation_id=current_parent_id,
            )
            finish_observation(
                selector_span,
                output_data={
                    "selected_model_names": result["selected_model_names"],
                    "analysis_method": result.get("analysis_method"),
                    "question_type": result.get("question_type"),
                },
                metadata={"node": "ModelSelector", "fallback": True},
                level="WARNING",
                status_message=str(e),
            )
            return result
        finally:
            if selector_span:
                # finish_observation åœ¨ try/except æ§åˆ¶åˆ†æ”¯é‡Œå·²ç»è°ƒç”¨ï¼Œè¿™é‡Œç¡®ä¿å¾…å¤„ç†å¯¹è±¡å·²ç»“æŸ
                pass

    def _display_selection_analysis(self, recommendation: Dict[str, Any]):
        """æ˜¾ç¤ºé€‰æ‹©åˆ†æç»“æœ"""
        problem_analysis = recommendation.get('problem_analysis', {})
        recommended_models = recommendation.get('recommended_models', [])

        print(f"ğŸ“‹ é—®é¢˜åˆ†æ:")
        print(f"   ç±»å‹: {problem_analysis.get('question_type', 'æœªçŸ¥')}")
        print(f"   å¤æ‚åº¦: {problem_analysis.get('complexity_level', 'æœªçŸ¥')}")
        print(f"   æ‰€éœ€èƒ½åŠ›: {', '.join(problem_analysis.get('required_capabilities', []))}")

        print(f"ğŸ¯ æ¨èæ¨¡å‹ç»„åˆ:")
        for model in recommended_models:
            score = model.get('suitability_score', 0)
            reasons = ', '.join(model.get('reasons', []))
            print(f"   {model.get('rank', 0)}. {model.get('model_name', '')} (é€‚åˆåº¦: {score}/10)")
            print(f"      ç†ç”±: {reasons}")
            print(f"      è´¡çŒ®: {model.get('expected_contribution', '')}")

        strategy = recommendation.get('combination_strategy', '')
        confidence = recommendation.get('confidence_level', '')
        if strategy:
            print(f"ğŸ”— ç»„åˆç­–ç•¥: {strategy}")
        if confidence:
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {confidence}")

    async def _fallback_selection(
        self,
        question: str,
        available_models: List[ModelConfig],
        trace_id: Optional[str] = None,
        parent_observation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """ä¼ ç»Ÿé€‰æ‹©æ–¹æ³•ä½œä¸ºå›é€€"""
        print("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿé—®é¢˜ç±»å‹åˆ†æ...")

        # ç®€åŒ–çš„é—®é¢˜ç±»å‹åˆ†æ
        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹é—®é¢˜çš„ç±»å‹ï¼Œä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€é€‚åˆçš„ä¸€ä¸ªï¼š
- æŠ€æœ¯/ç¼–ç¨‹
- åˆ›æ„å†™ä½œ
- æ•°å­¦/é€»è¾‘
- æ—¥å¸¸å¯¹è¯
- ä¸“ä¸šçŸ¥è¯†
- ç¿»è¯‘
- åˆ†ææ€»ç»“

é—®é¢˜: {question}

è¯·åªè¿”å›ç±»åˆ«åç§°ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚
"""

        try:
            question_type = await call_llm_async(
                messages=[{"role": "user", "content": analysis_prompt}],
                model="gpt-41-0414-global",
                trace_id=trace_id,
                parent_observation_id=parent_observation_id,
                langfuse_metadata={"node": "ModelSelector", "stage": "fallback_type_detection"}
            )
            question_type = question_type.strip()
            print(f"ğŸ“Š é—®é¢˜ç±»å‹: {question_type}")

        except Exception as e:
            print(f"âš ï¸ ç±»å‹åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤: {str(e)}")
            question_type = "é»˜è®¤"

        # ä½¿ç”¨ä¼ ç»Ÿé€‰æ‹©ç­–ç•¥
        preferred_models = self.fallback_criteria.get(question_type, self.fallback_criteria["é»˜è®¤"])
        available_model_names = [model.name for model in available_models]

        selected_models = []
        for preferred in preferred_models:
            if preferred in available_model_names and len(selected_models) < 3:
                selected_models.append(preferred)

        # è¡¥å……ä¸è¶³çš„æ¨¡å‹
        while len(selected_models) < 3 and len(selected_models) < len(available_model_names):
            for model_name in available_model_names:
                if model_name not in selected_models:
                    selected_models.append(model_name)
                    break

        print(f"âœ… é€‰æ‹©ç»“æœ: {selected_models}")

        return {
            "selected_model_names": selected_models,
            "question_type": question_type,
            "analysis_method": "fallback"
        }

    async def post_async(self, shared, prep_res, exec_res):
        """åå¤„ç†é˜¶æ®µï¼šä¿å­˜é€‰æ‹©çš„æ¨¡å‹åˆ°å…±äº«çŠ¶æ€"""
        if exec_res:
            selected_models = []
            available_models = shared["available_models"]

            # æ„å»ºé€‰æ‹©çš„æ¨¡å‹é…ç½®åˆ—è¡¨
            for model_name in exec_res["selected_model_names"]:
                for model_config in available_models:
                    if model_config.name == model_name:
                        selected_models.append(model_config)
                        break

            shared["selected_models"] = selected_models
            shared["question_type"] = exec_res["question_type"]
            shared["selection_analysis"] = exec_res.get("selection_analysis", {})
            shared["analysis_method"] = exec_res.get("analysis_method", "unknown")

            print(f"âœ… å·²é€‰æ‹© {len(selected_models)} ä¸ªæ¨¡å‹: {[m.name for m in selected_models]}")
            return "continue"

        return None


class ParallelLLMNode(AsyncNode):
    """
    å¹¶å‘LLMè°ƒç”¨èŠ‚ç‚¹
    åŒæ—¶è°ƒç”¨ä¸‰ä¸ªé€‰å®šçš„LLMæ¨¡å‹è·å–å›ç­”
    """

    def __init__(self):
        super().__init__(max_retries=2, wait=1)

    async def prep_async(self, shared):
        """å‡†å¤‡é˜¶æ®µï¼šè·å–é€‰å®šçš„æ¨¡å‹ã€ç”¨æˆ·é—®é¢˜ã€registry å’Œ trace_id"""
        question = shared.get("user_question", "")
        selected_models = shared.get("selected_models", [])
        registry = shared.get("registry")
        trace_id = shared.get("trace_id")
        trace_observation_id = shared.get("langfuse_trace_observation_id")

        if not question:
            raise ValueError("ç”¨æˆ·é—®é¢˜ä¸èƒ½ä¸ºç©º")

        if not selected_models:
            raise ValueError("æ²¡æœ‰é€‰å®šçš„æ¨¡å‹")

        return {
            "question": question,
            "models": selected_models,
            "registry": registry,
            "trace_id": trace_id,
            "trace_observation_id": trace_observation_id
        }

    async def call_single_llm(
        self,
        model_config: ModelConfig,
        question: str,
        model_index: int,
        registry=None,
        trace_id=None,
        parent_observation_id: Optional[str] = None,
    ):
        """è°ƒç”¨å•ä¸ªLLMæ¨¡å‹"""
        start_time = time.time()
        try:
            print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨æ¨¡å‹ {model_index + 1}: {model_config.name}")

            messages = [
                {"role": "user", "content": question}
            ]

            # ä½¿ç”¨ registry è°ƒç”¨æ¨¡å‹ï¼Œä¼ é€’ trace_id
            response = await call_llm_async(
                messages=messages,
                model=model_config.name,
                registry=registry,
                trace_id=trace_id,
                return_response_obj=True,
                parent_observation_id=parent_observation_id,
                langfuse_metadata={
                    "component": "parallel_llm",
                    "model_index": model_index + 1,
                    "model_name": model_config.name,
                },
            )
            response_text = response.text
            usage_details = response.usage

            end_time = time.time()
            response_time = end_time - start_time

            print(f"âœ… æ¨¡å‹ {model_index + 1} ({model_config.name}) å›ç­”å®Œæˆï¼Œè€—æ—¶: {response_time:.2f}ç§’")
            print(f"ğŸ“ æ¨¡å‹ {model_index + 1} å“åº”å†…å®¹:")
            print(f"{'=' * 50}")
            print(response_text[:200] + "..." if len(response_text) > 200 else response_text)
            print(f"{'=' * 50}\n")

            return {
                "model_name": model_config.name,
                "response": response_text,
                "token_usage": usage_details,
                "response_time": response_time,
                "success": True,
                "error": None,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            end_time = time.time()
            response_time = end_time - start_time
            print(f"âŒ æ¨¡å‹ {model_index + 1} ({model_config.name}) è°ƒç”¨å¤±è´¥: {str(e)}ï¼Œè€—æ—¶: {response_time:.2f}ç§’")
            return {
                "model_name": model_config.name,
                "response": "",
                "response_time": response_time,
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def exec_async(self, inputs):
        """æ‰§è¡Œé˜¶æ®µï¼šå¹¶å‘è°ƒç”¨æ‰€æœ‰é€‰å®šçš„LLM"""
        question = inputs["question"]
        models = inputs["models"]
        registry = inputs["registry"]
        trace_id = inputs["trace_id"]
        trace_observation_id = inputs.get("trace_observation_id")

        print(f"ğŸš€ å¼€å§‹å¹¶å‘è°ƒç”¨ {len(models)} ä¸ªLLMæ¨¡å‹...")

        parallel_span = create_span(
            trace_id,
            name="ParallelLLM",
            parent_observation_id=trace_observation_id,
            input_data={
                "question": question,
                "selected_models": [model.name for model in models],
            },
            metadata={"node": "ParallelLLM"},
        )
        generation_parent_id = parallel_span.id if parallel_span else trace_observation_id

        # å¹¶å‘è°ƒç”¨æ‰€æœ‰æ¨¡å‹ï¼Œä¼ é€’ trace_id
        tasks = [
            self.call_single_llm(
                model,
                question,
                i,
                registry=registry,
                trace_id=trace_id,
                parent_observation_id=generation_parent_id,
            )
            for i, model in enumerate(models)
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        successful_responses = []
        failed_responses = []

        for result in results:
            if isinstance(result, Exception):
                failed_responses.append({
                    "model_name": "unknown",
                    "error": str(result),
                    "success": False
                })
            elif result["success"]:
                successful_responses.append(result)
            else:
                failed_responses.append(result)

        print(f"ğŸ“Š è°ƒç”¨ç»“æœ: {len(successful_responses)} æˆåŠŸ, {len(failed_responses)} å¤±è´¥")

        finish_observation(
            parallel_span,
            output_data={
                "successful": [
                    {
                        "model_name": r["model_name"],
                        "response_time": r["response_time"],
                        "token_usage": r.get("token_usage"),
                    }
                    for r in successful_responses
                ],
                "failed": [
                    {"model_name": r.get("model_name"), "error": r.get("error")}
                    for r in failed_responses
                ],
            },
            metadata={"node": "ParallelLLM"},
            level="ERROR" if successful_responses == [] else None,
        )

        return {
            "successful_responses": successful_responses,
            "failed_responses": failed_responses
        }

    async def post_async(self, shared, prep_res, exec_res):
        """åå¤„ç†é˜¶æ®µï¼šä¿å­˜LLMå›ç­”åˆ°å…±äº«çŠ¶æ€"""
        if exec_res:
            shared["llm_responses"] = exec_res["successful_responses"]
            shared["failed_responses"] = exec_res["failed_responses"]

            if exec_res["successful_responses"]:
                print("âœ… LLMå¹¶å‘è°ƒç”¨å®Œæˆï¼Œå¼€å§‹èåˆå›ç­”...")
                return "continue"
            else:
                print("âŒ æ‰€æœ‰LLMè°ƒç”¨éƒ½å¤±è´¥äº†")
                return None

        return None


class FusionAgentNode(AsyncNode):
    """
    å›ç­”èåˆä»£ç†èŠ‚ç‚¹
    åˆ†æå’Œèåˆå¤šä¸ªLLMçš„å›ç­”ï¼Œç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆå›ç­”
    """

    def __init__(self):
        super().__init__(max_retries=3, wait=2)

    async def prep_async(self, shared):
        """å‡†å¤‡é˜¶æ®µï¼šè·å–æ‰€æœ‰LLMçš„å›ç­”ã€registry å’Œ trace_id"""
        question = shared.get("user_question", "")
        responses = shared.get("llm_responses", [])
        question_type = shared.get("question_type", "æœªçŸ¥")
        registry = shared.get("registry")
        trace_id = shared.get("trace_id")
        trace_observation_id = shared.get("langfuse_trace_observation_id")

        if not question:
            raise ValueError("ç”¨æˆ·é—®é¢˜ä¸èƒ½ä¸ºç©º")

        if not responses:
            raise ValueError("æ²¡æœ‰LLMå›ç­”å¯ä¾›èåˆ")

        return {
            "question": question,
            "responses": responses,
            "question_type": question_type,
            "registry": registry,
            "trace_id": trace_id,
            "trace_observation_id": trace_observation_id
        }

    async def exec_async(self, inputs):
        """æ‰§è¡Œé˜¶æ®µï¼šèåˆå¤šä¸ªLLMçš„å›ç­”"""
        question = inputs["question"]
        responses = inputs["responses"]
        question_type = inputs["question_type"]
        registry = inputs["registry"]
        trace_id = inputs["trace_id"]
        trace_observation_id = inputs.get("trace_observation_id")

        print("ğŸ§  æ­£åœ¨ä½¿ç”¨AIä»£ç†èåˆå¤šä¸ªå›ç­”...")

        # æ„å»ºèåˆæç¤º
        fusion_prompt = self._build_fusion_prompt(question, responses, question_type)

        fusion_span = create_span(
            trace_id,
            name="FusionAgent",
            parent_observation_id=trace_observation_id,
            input_data={
                "question": question,
                "question_type": question_type,
                "model_count": len(responses),
            },
            metadata={"node": "FusionAgent"},
        )
        generation_parent_id = fusion_span.id if fusion_span else trace_observation_id

        try:
            # ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹è¿›è¡Œèåˆ
            import os
            fusion_model = os.getenv("AI_FUSION_MODEL", "claude_sonnet4")

            response = await call_llm_async(
                messages=[{"role": "user", "content": fusion_prompt}],
                model=fusion_model,
                registry=registry,
                trace_id=trace_id,
                return_response_obj=True,
                parent_observation_id=generation_parent_id,
                langfuse_metadata={
                    "component": "fusion_agent",
                    "question_type": question_type,
                },
            )
            fused_answer = response.text

            finish_observation(
                fusion_span,
                output_data={
                    "fused_answer": fused_answer,
                    "token_usage": response.usage,
                },
                metadata={"node": "FusionAgent"},
            )

            return fused_answer

        except Exception as e:
            print(f"âŒ å›ç­”èåˆå¤±è´¥: {str(e)}")
            finish_observation(
                fusion_span,
                output_data={"error": str(e)},
                metadata={"node": "FusionAgent"},
                level="ERROR",
                status_message=str(e),
            )
            # å¦‚æœèåˆå¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„å›ç­”
            if responses:
                return f"èåˆå¤±è´¥ï¼Œä»¥ä¸‹æ˜¯ç¬¬ä¸€ä¸ªæ¨¡å‹çš„å›ç­”ï¼š\n\n{responses[0]['response']}"
            else:
                return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚"

    def _build_fusion_prompt(self, question: str, responses: List[Dict], question_type: str) -> str:
        """æ„å»ºèåˆæç¤º"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå›ç­”èåˆä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªå…³äº"{question_type}"çš„é—®é¢˜ï¼Œæˆ‘å·²ç»ä»å¤šä¸ªAIæ¨¡å‹è·å¾—äº†ä¸åŒçš„å›ç­”ã€‚

è¯·åˆ†æè¿™äº›å›ç­”ï¼Œæå–å„è‡ªçš„ä¼˜ç‚¹ï¼Œç„¶åèåˆæˆä¸€ä¸ªå…¨é¢ã€å‡†ç¡®ã€æœ‰ç”¨çš„æœ€ç»ˆå›ç­”ã€‚

ç”¨æˆ·é—®é¢˜: {question}

å„æ¨¡å‹çš„å›ç­”:
"""

        for i, response in enumerate(responses, 1):
            prompt += f"\nã€æ¨¡å‹ {i}: {response['model_name']}ã€‘\n{response['response']}\n"

        prompt += """
è¯·æ ¹æ®ä»¥ä¸Šå›ç­”ï¼Œç”Ÿæˆä¸€ä¸ªèåˆåçš„æœ€ç»ˆå›ç­”ã€‚è¦æ±‚ï¼š

1. ç»¼åˆå„ä¸ªå›ç­”çš„ä¼˜ç‚¹å’Œæœ‰ç”¨ä¿¡æ¯
2. ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€å®Œæ•´ä¸”æ˜“äºç†è§£
3. å¦‚æœå„å›ç­”æœ‰å†²çªï¼Œè¯·æŒ‡å‡ºå¹¶ç»™å‡ºæœ€å¯é çš„ä¿¡æ¯
4. ä¿æŒå›ç­”çš„é€»è¾‘æ€§å’Œæ¡ç†æ€§
5. é€‚å½“æ·»åŠ è¡¥å……ä¿¡æ¯ä½¿å›ç­”æ›´å…¨é¢

æœ€ç»ˆèåˆå›ç­”:"""

        return prompt

    async def post_async(self, shared, prep_res, exec_res):
        """åå¤„ç†é˜¶æ®µï¼šä¿å­˜èåˆåçš„æœ€ç»ˆå›ç­”"""
        if exec_res:
            shared["final_answer"] = exec_res
            print("âœ… å›ç­”èåˆå®Œæˆï¼")
            return "analyze"  # ç»§ç»­åˆ°è´¨é‡åˆ†æèŠ‚ç‚¹

        return None


class QualityAnalyzerNode(AsyncNode):
    """
    è´¨é‡åˆ†æèŠ‚ç‚¹
    å¯¹èåˆå›ç­”å’Œå„æ¨¡å‹å›ç­”è¿›è¡Œè´¨é‡åˆ†æ
    """

    def __init__(self):
        super().__init__(max_retries=2, wait=1)
        self.analyzer = None  # å°†åœ¨ prep_async ä¸­åˆå§‹åŒ–

    async def prep_async(self, shared):
        """å‡†å¤‡é˜¶æ®µï¼šè·å–é—®é¢˜ã€å›ç­”å’Œèåˆç»“æœä»¥åŠregistry"""
        question = shared.get("user_question", "")
        llm_responses = shared.get("llm_responses", [])
        final_answer = shared.get("final_answer", "")
        registry = shared.get("registry")
        trace_id = shared.get("trace_id")
        trace_observation_id = shared.get("langfuse_trace_observation_id")

        if not question:
            raise ValueError("ç”¨æˆ·é—®é¢˜ä¸èƒ½ä¸ºç©º")

        if not llm_responses:
            print("âš ï¸ æ²¡æœ‰LLMå›ç­”ï¼Œè·³è¿‡è´¨é‡åˆ†æ")
            return None

        # åˆå§‹åŒ– analyzerï¼ˆä½¿ç”¨ registryï¼‰
        if self.analyzer is None:
            self.analyzer = AIFusionQualityAnalyzer(registry=registry)

        return {
            "question": question,
            "llm_responses": llm_responses,
            "final_answer": final_answer,
            "trace_id": trace_id,
            "trace_observation_id": trace_observation_id
        }

    async def exec_async(self, inputs):
        """æ‰§è¡Œé˜¶æ®µï¼šè¿›è¡Œè´¨é‡åˆ†æ"""
        if inputs is None:
            return None

        trace_id = inputs.get("trace_id")
        trace_observation_id = inputs.get("trace_observation_id")

        analysis_span = create_span(
            trace_id,
            name="QualityAnalyzer",
            parent_observation_id=trace_observation_id,
            input_data={
                "question": inputs["question"],
                "final_answer": inputs["final_answer"][:200] if inputs["final_answer"] else "",
                "response_count": len(inputs["llm_responses"]),
            },
            metadata={"node": "QualityAnalyzer"},
        )
        parent_observation_id = analysis_span.id if analysis_span else trace_observation_id

        print("\nğŸ” æ­£åœ¨è¿›è¡Œè´¨é‡åˆ†æ...")

        try:
            quality_analysis = await self.analyzer.analyze_quality(
                question=inputs["question"],
                llm_responses=inputs["llm_responses"],
                fusion_answer=inputs["final_answer"],
                trace_id=trace_id,
                parent_observation_id=parent_observation_id,
            )

            finish_observation(
                analysis_span,
                output_data={"quality_analysis": quality_analysis},
                metadata={"node": "QualityAnalyzer"},
            )

            return quality_analysis
        except Exception as exc:
            finish_observation(
                analysis_span,
                output_data={"error": str(exc)},
                metadata={"node": "QualityAnalyzer"},
                level="ERROR",
                status_message=str(exc),
            )
            raise

    async def post_async(self, shared, prep_res, exec_res):
        """åå¤„ç†é˜¶æ®µï¼šä¿å­˜è´¨é‡åˆ†æç»“æœ"""
        if exec_res:
            shared["quality_analysis"] = exec_res
            print("âœ… è´¨é‡åˆ†æå®Œæˆï¼")
            return "report"  # ç»§ç»­åˆ°æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
        else:
            # å¦‚æœæ²¡æœ‰åˆ†æç»“æœï¼Œç›´æ¥è·³åˆ°æŠ¥å‘ŠèŠ‚ç‚¹
            return "report"


class ReportGeneratorNode(AsyncNode):
    """
    æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
    ç”Ÿæˆè¯¦ç»†çš„Markdownåˆ†ææŠ¥å‘Š
    """

    def __init__(self):
        super().__init__(max_retries=1, wait=1)
        self.reporter = AIFusionReporter()

    async def prep_async(self, shared):
        """å‡†å¤‡é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰å¿…è¦çš„æ•°æ®"""
        question = shared.get("user_question", "")
        question_type = shared.get("question_type", "æœªçŸ¥")
        llm_responses = shared.get("llm_responses", [])
        final_answer = shared.get("final_answer", "")
        quality_analysis = shared.get("quality_analysis")
        selection_analysis = shared.get("selection_analysis", {})
        selected_models = shared.get("selected_models", [])

        return {
            "question": question,
            "question_type": question_type,
            "llm_responses": llm_responses,
            "final_answer": final_answer,
            "selected_models": [m.name for m in selected_models],
            "quality_analysis": quality_analysis,
            "selection_analysis": selection_analysis
        }

    async def exec_async(self, inputs):
        """æ‰§è¡Œé˜¶æ®µï¼šç”ŸæˆæŠ¥å‘Š"""
        # æ‰“å°ç®€è¦æ‘˜è¦
        if inputs["quality_analysis"]:
            self.reporter.print_summary(
                inputs["llm_responses"],
                inputs["final_answer"],
                inputs["quality_analysis"]
            )

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_path = self.reporter.generate_report(
            question=inputs["question"],
            question_type=inputs["question_type"],
            llm_responses=inputs["llm_responses"],
            final_answer=inputs["final_answer"],
            selected_models=inputs["selected_models"],
            quality_analysis=inputs["quality_analysis"],
            selection_analysis=inputs["selection_analysis"]
        )

        return report_path

    async def post_async(self, shared, prep_res, exec_res):
        """åå¤„ç†é˜¶æ®µï¼šä¿å­˜æŠ¥å‘Šè·¯å¾„"""
        if exec_res:
            shared["report_path"] = exec_res
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {exec_res}")
            return "complete"  # æµç¨‹ç»“æŸ

        return None
