"""
AI Fusionå·¥å…·å‡½æ•°å’Œé…ç½®ç®¡ç†
åŒ…å«LLMè°ƒç”¨ã€æ¨¡å‹é…ç½®ã€ç¯å¢ƒéªŒè¯ç­‰å®ç”¨åŠŸèƒ½

å…¼å®¹æ€§è¯´æ˜ï¼š
- ä¿ç•™åŸæœ‰çš„ ModelConfig å’Œå‡½æ•°æ¥å£
- æ–°å¢åŸºäº ModelRegistry çš„æ‰©å±•åŠŸèƒ½
- å¯ä»¥æ— ç¼åˆ‡æ¢æ–°æ—§ä¸¤ç§ä½¿ç”¨æ–¹å¼
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from openai import AsyncOpenAI
from dotenv import load_dotenv

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

@dataclass
class ModelConfig:
    """LLMæ¨¡å‹é…ç½®ç±»ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰"""
    name: str
    provider: str  # openai, anthropic, google, etc.
    api_key: str
    base_url: Optional[str] = None
    max_tokens: int = 2000
    temperature: float = 0.7


def get_available_models() -> List[ModelConfig]:
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„LLMæ¨¡å‹é…ç½®
    æ ¹æ®ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥ç¡®å®šå¯ç”¨æ¨¡å‹

    å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰å®ç°
    """
    models = []

    # OpenAIæ¨¡å‹ï¼ˆæ”¯æŒè‡ªå®šä¹‰base_urlï¼‰
    openai_key = os.environ.get("OPENAI_API_KEY")
    openai_base_url = os.environ.get("OPENAI_BASE_URL")

    if openai_key:
        # ä»ç¯å¢ƒå˜é‡æˆ–ä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨
        if os.getenv("AVAILABLE_MODELS"):
            model_names = [m.strip() for m in os.getenv("AVAILABLE_MODELS").split(",")]
        else:
            # é»˜è®¤æ¨¡å‹åˆ—è¡¨ï¼ˆåŸºäºå½“å‰é¡¹ç›®ï¼‰
            model_names = [
                "qwen-max", "qwen-plus", "claude_sonnet4", "gpt-41-0414-global",
                "claude37_sonnet_new", "gpt-41-mini-0414-global", "glm-4.5",
                "openmatrix-qwen3-235b-inst-fp8", "qwen3-max-preview",
                "gpt-5-mini-0807-global", "qwen3-coder-480b-a35b-instruct",
                "qwen3-coder-plus1", "qwen3-coder-plus"
            ]

        for model_name in model_names:
            models.append(ModelConfig(
                name=model_name,
                provider="openai",
                api_key=openai_key,
                base_url=openai_base_url,
                max_tokens=2000
            ))

    return models


async def call_llm_async(
    messages: List[Dict[str, str]],
    model: str,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    max_tokens: int = 2000,
    temperature: float = 0.7
) -> str:
    """
    å¼‚æ­¥è°ƒç”¨LLMçš„é€šç”¨å‡½æ•°
    æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ç›¸åº”çš„æä¾›å•†è°ƒç”¨æ¨¡å‹

    å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰å‚æ•°æ ¼å¼
    """

    # å°è¯•ä½¿ç”¨æ–°çš„ ModelRegistry æ–¹å¼
    try:
        registry = get_model_registry()
        if registry:
            return await registry.call_model(
                model_id=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
    except Exception as e:
        print(f"âš ï¸ ModelRegistry è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼: {str(e)}")

    # å›é€€åˆ°åŸæœ‰çš„ OpenAI å…¼å®¹æ–¹å¼
    api_key = api_key or os.getenv("OPENAI_API_KEY")
    base_url = base_url or os.getenv("OPENAI_BASE_URL")

    # ç»Ÿä¸€ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨æ‰€æœ‰æ¨¡å‹
    client = AsyncOpenAI(
        api_key=api_key,
        base_url=base_url
    )

    try:
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )

        return response.choices[0].message.content

    except Exception as e:
        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        raise Exception(f"è°ƒç”¨æ¨¡å‹ {model} å¤±è´¥: {str(e)}")


def validate_environment() -> bool:
    """
    éªŒè¯ç¯å¢ƒé…ç½®
    æ£€æŸ¥æ˜¯å¦é…ç½®äº†OpenAI APIå¯†é’¥

    å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰å®ç°
    """
    openai_key = os.environ.get("OPENAI_API_KEY")

    if not openai_key:
        print("âŒ ç¯å¢ƒé…ç½®æ£€æŸ¥å¤±è´¥ï¼")
        print("è¯·é…ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š")
        print("- OPENAI_API_KEY: OpenAI APIå¯†é’¥")
        print("- OPENAI_BASE_URL: è‡ªå®šä¹‰OpenAIå…¼å®¹æœåŠ¡åœ°å€ï¼ˆå¯é€‰ï¼‰")
        return False

    available_models = get_available_models()
    print(f"âœ… ç¯å¢ƒéªŒè¯é€šè¿‡ï¼å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹:")
    for model in available_models:
        print(f"  - {model.name} ({model.provider})")

    return len(available_models) >= 1


def setup_example_env():
    """
    è®¾ç½®ç¤ºä¾‹ç¯å¢ƒå˜é‡ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
    å®é™…ä½¿ç”¨æ—¶è¯·è®¾ç½®çœŸå®çš„APIå¯†é’¥

    å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰å®ç°
    """
    example_env = {
        "OPENAI_API_KEY": "your-openai-api-key-here",
        "OPENAI_BASE_URL": "https://api.openai.com/v1",  # å¯é€‰ï¼Œè‡ªå®šä¹‰OpenAIå…¼å®¹æœåŠ¡åœ°å€
    }

    print("ğŸ“ ç¤ºä¾‹ç¯å¢ƒå˜é‡é…ç½®:")
    for key, value in example_env.items():
        print(f"export {key}='{value}'")
    print("\nè¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µè®¾ç½®ç›¸åº”çš„APIå¯†é’¥")


async def test_model_connection(model_config: ModelConfig) -> bool:
    """
    æµ‹è¯•å•ä¸ªæ¨¡å‹çš„è¿æ¥

    å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰å®ç°
    """
    try:
        test_messages = [{"role": "user", "content": "Hello, please respond with 'OK'"}]

        response = await call_llm_async(
            messages=test_messages,
            model=model_config.name,
            api_key=model_config.api_key,
            base_url=model_config.base_url,
            max_tokens=10,
            temperature=0.1
        )

        print(f"âœ… {model_config.name} è¿æ¥æµ‹è¯•æˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ {model_config.name} è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


async def test_all_models():
    """
    æµ‹è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„è¿æ¥

    å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰å®ç°
    """
    models = get_available_models()
    if not models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹é…ç½®")
        return

    print(f"ğŸ§ª å¼€å§‹æµ‹è¯• {len(models)} ä¸ªæ¨¡å‹çš„è¿æ¥...")

    tasks = [test_model_connection(model) for model in models]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful = sum(1 for result in results if result is True)
    print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ: {successful}/{len(models)} ä¸ªæ¨¡å‹å¯ç”¨")


# ============================================
# æ–°å¢ï¼šæ‰©å±•åŠŸèƒ½ï¼ˆåŸºäº ModelRegistryï¼‰
# ============================================

def get_model_registry():
    """
    è·å–æ¨¡å‹æ³¨å†Œä¸­å¿ƒå®ä¾‹ï¼ˆæ–°åŠŸèƒ½ï¼‰

    ä½¿ç”¨ç¤ºä¾‹ï¼š
        from ai_fusion.utils import get_model_registry

        registry = get_model_registry()
        models = await registry.discover_all_models()
    """
    try:
        from ai_fusion.registry import ModelRegistry
        from ai_fusion.providers.universal_provider import UniversalProvider
        from ai_fusion.registry.model_registry import ProviderConfig

        # åˆ›å»ºè‡ªå®šä¹‰é…ç½®ï¼Œä½¿ç”¨ UniversalProvider
        custom_providers = {
            "universal": ProviderConfig(
                provider_class=UniversalProvider,
                api_key_env="OPENAI_API_KEY",
                base_url_env="OPENAI_BASE_URL"
            )
        }

        return ModelRegistry(custom_providers=custom_providers)
    except ImportError:
        print("âš ï¸ ModelRegistry åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–")
        return None


async def get_available_models_v2():
    """
    è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆæ–°ç‰ˆæœ¬ï¼ŒåŸºäº ModelRegistryï¼‰

    è¿”å› List[ModelInfo] è€Œä¸æ˜¯ List[ModelConfig]
    æä¾›æ›´ä¸°å¯Œçš„æ¨¡å‹ä¿¡æ¯
    """
    registry = get_model_registry()
    if registry:
        return await registry.discover_all_models()
    else:
        # é™çº§åˆ°æ—§ç‰ˆæœ¬
        print("âš ï¸ ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹å‘ç°æ–¹å¼")
        return get_available_models()
