"""
AI Fusion æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆè¯¦ç»†çš„markdownåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«æ¨¡å‹æ€§èƒ½å¯¹æ¯”å’Œå›ç­”è´¨é‡åˆ†æ
"""

import os
from typing import List, Dict, Any, Optional
from datetime import datetime


class AIFusionReporter:
    """AI FusionæŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–æŠ¥å‘Šç›®å½•ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼"reports"
        self.report_dir = os.getenv("AI_FUSION_REPORT_DIR", "reports")
        self._ensure_report_dir()

    def _ensure_report_dir(self):
        """ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.report_dir):
            os.makedirs(self.report_dir)

    def generate_report(
        self,
        question: str,
        question_type: str,
        llm_responses: List[Dict],
        final_answer: str,
        selected_models: List[str],
        quality_analysis: Optional[Dict[str, Any]] = None,
        selection_analysis: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š

        Args:
            question: ç”¨æˆ·é—®é¢˜
            question_type: é—®é¢˜ç±»å‹
            llm_responses: å„æ¨¡å‹çš„å“åº”æ•°æ®
            final_answer: èåˆåçš„æœ€ç»ˆå›ç­”
            selected_models: é€‰æ‹©çš„æ¨¡å‹åˆ—è¡¨

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ai_fusion_report_{timestamp}.md"
        filepath = os.path.join(self.report_dir, filename)

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = self._build_report_content(
            question, question_type, llm_responses, final_answer, selected_models, quality_analysis, selection_analysis
        )

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
        return filepath

    def _build_report_content(
        self,
        question: str,
        question_type: str,
        llm_responses: List[Dict],
        final_answer: str,
        selected_models: List[str],
        quality_analysis: Optional[Dict[str, Any]] = None,
        selection_analysis: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ„å»ºæŠ¥å‘Šå†…å®¹ - ä¼˜åŒ–ç‰ˆï¼Œä¸“æ³¨äºèåˆå›ç­”å’Œæœ€ä½³æ¨¡å‹è¡¨ç°"""

        # ç»Ÿè®¡ä¿¡æ¯
        total_models = len(llm_responses)
        fusion_length = len(final_answer)

        report = f"""# AI Fusion åˆ†ææŠ¥å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **é—®é¢˜ç±»å‹**: {question_type}
- **å‚ä¸æ¨¡å‹æ•°é‡**: {total_models}

## â“ ç”¨æˆ·é—®é¢˜
```
{question}
```

## ğŸ¯ AI Fusion èåˆå›ç­”
**å­—ç¬¦æ•°**: {fusion_length}

```
{final_answer}
```

## ğŸ† æœ€ä½³æ¨¡å‹è¡¨ç°

{self._generate_best_models_section(llm_responses, quality_analysis)}

---
*æœ¬æŠ¥å‘Šç”± AI Fusion ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        return report

    def _generate_best_models_section(self, llm_responses: List[Dict], quality_analysis: Optional[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæœ€ä½³æ¨¡å‹è¡¨ç°éƒ¨åˆ† - æ˜¾ç¤ºæœ€å¿«å’Œæœ€é«˜è´¨é‡çš„æ¨¡å‹ï¼ŒåŒ…å«è¯¦ç»†çš„è¯„åˆ†è®¡ç®—è¯´æ˜"""
        section = ""

        # æ‰¾å‡ºå“åº”æ—¶é—´æœ€å¿«çš„æ¨¡å‹
        successful_responses = [r for r in llm_responses if r.get('success')]
        if successful_responses:
            fastest_model = min(successful_responses, key=lambda x: x.get('response_time', float('inf')))
            section += f"### ğŸš€ å“åº”é€Ÿåº¦æœ€å¿«\n\n"
            section += f"**{fastest_model['model_name']}**\n"
            section += f"- å“åº”æ—¶é—´: {fastest_model.get('response_time', 0):.2f}ç§’\n"
            section += f"- å›ç­”é•¿åº¦: {len(fastest_model['response'])}å­—ç¬¦\n\n"

        # æ‰¾å‡ºè´¨é‡æœ€é«˜çš„æ¨¡å‹å¹¶è¯¦ç»†è¯´æ˜è¯„åˆ†è®¡ç®—
        if quality_analysis and 'quality_ranking' in quality_analysis:
            ranking = quality_analysis['quality_ranking']
            # æ‰¾åˆ°è´¨é‡æœ€é«˜çš„éèåˆå›ç­”
            best_model_entry = next((item for item in ranking if not item['is_fusion']), None)

            if best_model_entry:
                section += f"### ğŸ† è´¨é‡è¯„åˆ†æœ€é«˜\n\n"
                section += f"**{best_model_entry['source']}**\n\n"

                # ç»¼åˆè¯„åˆ†
                section += f"#### ğŸ“Š ç»¼åˆè¯„åˆ†: {best_model_entry['overall_score']:.1f}/10\n\n"

                # è¯¦ç»†è¯„åˆ†åˆ†è§£
                section += f"#### ğŸ” è¯„åˆ†ç»†èŠ‚\n\n"

                # è·å–LLMè¯„ä¼°è¯¦æƒ…
                llm_evals = quality_analysis.get('llm_evaluations', {})
                model_eval = llm_evals.get(best_model_entry['source'])

                if model_eval:
                    # å®Œæ•´æ€§è¯„åˆ†
                    completeness = best_model_entry['completeness']
                    section += f"**1. å®Œæ•´æ€§è¯„åˆ†: {completeness:.1f}/10**\n"
                    section += f"- è¯„ä¼°æ ‡å‡†: å›ç­”æ˜¯å¦è¦†ç›–é—®é¢˜çš„æ‰€æœ‰å…³é”®æ–¹é¢\n"
                    if hasattr(model_eval, 'completeness_reasoning'):
                        section += f"- è¯„åˆ†ç†ç”±: {model_eval.completeness_reasoning}\n"
                    section += "\n"

                    # å‡†ç¡®æ€§è¯„åˆ†
                    accuracy = best_model_entry['accuracy']
                    section += f"**2. å‡†ç¡®æ€§è¯„åˆ†: {accuracy:.1f}/10**\n"
                    section += f"- è¯„ä¼°æ ‡å‡†: ä¿¡æ¯æ˜¯å¦å‡†ç¡®æ— è¯¯ï¼Œé€»è¾‘æ˜¯å¦ä¸¥å¯†\n"
                    if hasattr(model_eval, 'accuracy_reasoning'):
                        section += f"- è¯„åˆ†ç†ç”±: {model_eval.accuracy_reasoning}\n"
                    section += "\n"

                    # æ¸…æ™°åº¦è¯„åˆ†
                    clarity = best_model_entry['clarity']
                    section += f"**3. æ¸…æ™°åº¦è¯„åˆ†: {clarity:.1f}/10**\n"
                    section += f"- è¯„ä¼°æ ‡å‡†: è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚ï¼Œç»“æ„æ˜¯å¦åˆç†\n"
                    if hasattr(model_eval, 'clarity_reasoning'):
                        section += f"- è¯„åˆ†ç†ç”±: {model_eval.clarity_reasoning}\n"
                    section += "\n"

                    # ç›¸å…³æ€§è¯„åˆ†
                    relevance = best_model_entry['relevance']
                    section += f"**4. ç›¸å…³æ€§è¯„åˆ†: {relevance:.1f}/10**\n"
                    section += f"- è¯„ä¼°æ ‡å‡†: å†…å®¹æ˜¯å¦åˆ‡é¢˜ï¼Œæ˜¯å¦ç›´æ¥å›ç­”äº†é—®é¢˜\n"
                    if hasattr(model_eval, 'relevance_reasoning'):
                        section += f"- è¯„åˆ†ç†ç”±: {model_eval.relevance_reasoning}\n"
                    section += "\n"

                    # ç»¼åˆè¯„åˆ†è®¡ç®—è¯´æ˜
                    section += f"**ç»¼åˆè¯„åˆ†è®¡ç®—æ–¹å¼:**\n"
                    section += f"```\n"
                    section += f"ç»¼åˆè¯„åˆ† = (å®Œæ•´æ€§ + å‡†ç¡®æ€§ + æ¸…æ™°åº¦ + ç›¸å…³æ€§) / 4\n"
                    section += f"         = ({completeness:.1f} + {accuracy:.1f} + {clarity:.1f} + {relevance:.1f}) / 4\n"
                    section += f"         = {best_model_entry['overall_score']:.1f}/10\n"
                    section += f"```\n\n"

                    # æ€»ä½“è¯„ä»·
                    if hasattr(model_eval, 'overall_assessment'):
                        section += f"**ğŸ’­ æ€»ä½“è¯„ä»·:**\n{model_eval.overall_assessment}\n\n"
                else:
                    # å¦‚æœæ²¡æœ‰è¯¦ç»†è¯„ä¼°æ•°æ®ï¼Œæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                    section += f"- å®Œæ•´æ€§: {best_model_entry['completeness']:.1f}/10\n"
                    section += f"- å‡†ç¡®æ€§: {best_model_entry['accuracy']:.1f}/10\n"
                    section += f"- æ¸…æ™°åº¦: {best_model_entry['clarity']:.1f}/10\n"
                    section += f"- ç›¸å…³æ€§: {best_model_entry['relevance']:.1f}/10\n\n"
                    section += f"**ç»¼åˆè¯„åˆ†** = (å®Œæ•´æ€§ + å‡†ç¡®æ€§ + æ¸…æ™°åº¦ + ç›¸å…³æ€§) / 4 = {best_model_entry['overall_score']:.1f}/10\n\n"
        else:
            section += f"### ğŸ† è´¨é‡è¯„åˆ†æœ€é«˜\n\n"
            section += f"_è´¨é‡åˆ†ææ•°æ®ä¸å¯ç”¨_\n\n"

        return section

    def print_summary(self, llm_responses: List[Dict], final_answer: str, quality_analysis: Optional[Dict[str, Any]] = None):
        """æ‰“å°ç®€è¦æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š AI Fusion åˆ†ææ‘˜è¦")
        print("="*60)

        for i, response in enumerate(llm_responses, 1):
            if response['success']:
                print(f"ğŸ¤– æ¨¡å‹ {i}: {response['model_name']}")
                print(f"   â±ï¸  å“åº”æ—¶é—´: {response.get('response_time', 0):.2f}ç§’")
                print(f"   ğŸ“ å›ç­”é•¿åº¦: {len(response['response'])}å­—ç¬¦")
                print(f"   âœ… çŠ¶æ€: æˆåŠŸ")
            else:
                print(f"âŒ æ¨¡å‹ {i}: {response['model_name']} - å¤±è´¥")
                print(f"   â±ï¸  å“åº”æ—¶é—´: {response.get('response_time', 0):.2f}ç§’")
                print(f"   ğŸ“ é”™è¯¯: {response.get('error', 'Unknown')}")
            print()

        print(f"ğŸ¯ èåˆå›ç­”é•¿åº¦: {len(final_answer)}å­—ç¬¦")
        print("="*60)
