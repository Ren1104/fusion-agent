"""
AI Fusion æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆè¯¦ç»†çš„markdownåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«æ¨¡å‹æ€§èƒ½å¯¹æ¯”å’Œå›ç­”è´¨é‡åˆ†æ
"""

import os
from typing import List, Dict, Any, Optional
from datetime import datetime


class AIFusionReporter:
    """AI FusionæŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–æŠ¥å‘Šç›®å½•ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼"reports"
        self.report_dir = os.getenv("AI_FUSION_REPORT_DIR", "reports")
        self._ensure_report_dir()

    def _ensure_report_dir(self):
        """ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.report_dir):
            os.makedirs(self.report_dir)

    def generate_report(
        self,
        question: str,
        question_type: str,
        llm_responses: List[Dict],
        final_answer: str,
        selected_models: List[str],
        quality_analysis: Optional[Dict[str, Any]] = None,
        selection_analysis: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š

        Args:
            question: ç”¨æˆ·é—®é¢˜
            question_type: é—®é¢˜ç±»å‹
            llm_responses: å„æ¨¡å‹çš„å“åº”æ•°æ®
            final_answer: èåˆåçš„æœ€ç»ˆå›ç­”
            selected_models: é€‰æ‹©çš„æ¨¡å‹åˆ—è¡¨

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ai_fusion_report_{timestamp}.md"
        filepath = os.path.join(self.report_dir, filename)

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = self._build_report_content(
            question, question_type, llm_responses, final_answer, selected_models, quality_analysis, selection_analysis
        )

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
        return filepath

    def _build_report_content(
        self,
        question: str,
        question_type: str,
        llm_responses: List[Dict],
        final_answer: str,
        selected_models: List[str],
        quality_analysis: Optional[Dict[str, Any]] = None,
        selection_analysis: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ„å»ºæŠ¥å‘Šå†…å®¹ - ä¼˜åŒ–ç‰ˆï¼Œä¸“æ³¨äºèåˆå›ç­”å’Œæœ€ä½³æ¨¡å‹è¡¨ç°"""

        # ç»Ÿè®¡ä¿¡æ¯
        total_models = len(llm_responses)
        fusion_length = len(final_answer)

        report = f"""# AI Fusion åˆ†ææŠ¥å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **é—®é¢˜ç±»å‹**: {question_type}
- **å‚ä¸æ¨¡å‹æ•°é‡**: {total_models}

## â“ ç”¨æˆ·é—®é¢˜
```
{question}
```

## ğŸ¯ AI Fusion èåˆå›ç­”
**å­—ç¬¦æ•°**: {fusion_length}

```
{final_answer}
```

## ğŸ† æœ€ä½³æ¨¡å‹è¡¨ç°

{self._generate_best_models_section(llm_responses, quality_analysis)}

{self._generate_quality_overview_section(quality_analysis)}

{self._generate_speed_quality_section(quality_analysis)}

---
*æœ¬æŠ¥å‘Šç”± AI Fusion ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        return report

    def _generate_best_models_section(self, llm_responses: List[Dict], quality_analysis: Optional[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæœ€ä½³æ¨¡å‹è¡¨ç°éƒ¨åˆ† - æ˜¾ç¤ºæœ€å¿«å’Œæœ€é«˜è´¨é‡çš„æ¨¡å‹ï¼ŒåŒ…å«è¯¦ç»†çš„è¯„åˆ†è®¡ç®—è¯´æ˜"""
        section = ""

        successful_responses = [r for r in llm_responses if r.get('success')]
        if successful_responses:
            fastest_model = min(successful_responses, key=lambda x: x.get('response_time', float('inf')))
            section += f"### ğŸš€ å“åº”é€Ÿåº¦æœ€å¿«\n\n"
            section += f"**{fastest_model['model_name']}**\n"
            section += f"- å“åº”æ—¶é—´: {fastest_model.get('response_time', 0):.2f}ç§’\n"
            section += f"- å›ç­”é•¿åº¦: {len(fastest_model['response'])}å­—ç¬¦\n\n"

        if quality_analysis and 'quality_ranking' in quality_analysis:
            ranking = quality_analysis['quality_ranking']
            details_map = quality_analysis.get('llm_evaluation_details', {})
            best_model_entry = next((item for item in ranking if not item['is_fusion']), None)

            if best_model_entry:
                section += f"### ğŸ† è´¨é‡è¯„åˆ†æœ€é«˜\n\n"
                section += f"**{best_model_entry['source']}**\n\n"
                section += f"#### ğŸ“Š ç»¼åˆè¯„åˆ†: {best_model_entry['overall_score']:.1f}/10\n\n"
                section += "#### ğŸ” è¯„åˆ†ç»†èŠ‚\n\n"
                section += self._render_dimension_scores(best_model_entry)

                model_details = details_map.get(best_model_entry['source'], {})
                if model_details:
                    section += f"**ğŸ’¡ ç‰¹å¾æ‘˜è¦:** {model_details.get('unique_characteristics', '_æš‚æ— æè¿°_')}\n\n"
                    strengths = []
                    weaknesses = []
                    for dimension in ('completeness', 'accuracy', 'clarity', 'relevance'):
                        strengths.extend(model_details.get(dimension, {}).get('strengths', []))
                        weaknesses.extend(model_details.get(dimension, {}).get('weaknesses', []))
                    section += f"**âœ… å…³é”®ä¼˜åŠ¿:** {self._format_list(strengths)}\n\n"
                    section += f"**âŒ æ”¹è¿›ç©ºé—´:** {self._format_list(weaknesses)}\n\n"
                    suggestions = model_details.get('core_suggestions', [])
                    if suggestions:
                        section += "**ğŸ›  æ”¹è¿›å»ºè®®:**\n"
                        for suggestion in suggestions[:3]:
                            section += f"- {suggestion}\n"
                        section += "\n"
        else:
            section += f"### ğŸ† è´¨é‡è¯„åˆ†æœ€é«˜\n\n"
            section += f"_è´¨é‡åˆ†ææ•°æ®ä¸å¯ç”¨_\n\n"

        return section

    def _format_list(self, items: Optional[List[str]], fallback: str = "_æš‚æ— æ•°æ®_") -> str:
        """æ ¼å¼åŒ–åˆ—è¡¨å†…å®¹ä¸ºå¯è¯»æ–‡æœ¬"""
        if not items:
            return fallback
        cleaned = [item.strip() for item in items if item and item.strip()]
        if not cleaned:
            return fallback
        return "ï¼›".join(cleaned[:4])

    def _render_dimension_scores(self, ranking_entry: Dict[str, Any]) -> str:
        """æ¸²æŸ“ç»´åº¦å¾—åˆ†"""
        completeness = ranking_entry['completeness']
        accuracy = ranking_entry['accuracy']
        clarity = ranking_entry['clarity']
        relevance = ranking_entry['relevance']

        return (
            f"- å®Œæ•´æ€§: {completeness:.1f}/10\n"
            f"- å‡†ç¡®æ€§: {accuracy:.1f}/10\n"
            f"- æ¸…æ™°åº¦: {clarity:.1f}/10\n"
            f"- ç›¸å…³æ€§: {relevance:.1f}/10\n\n"
        )

    def _generate_quality_overview_section(self, quality_analysis: Optional[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ•´ä½“è´¨é‡åˆ†ææ¦‚è§ˆ"""
        if not quality_analysis:
            return ""

        ranking = quality_analysis.get('quality_ranking')
        llm_evaluations = quality_analysis.get('llm_evaluations', {})
        evaluation_details = quality_analysis.get('llm_evaluation_details', {})

        if not ranking or not llm_evaluations:
            return ""

        overview = "## ğŸ” è´¨é‡åˆ†ææ¦‚è§ˆ\n\n"
        overview += self._build_quality_table(ranking, llm_evaluations)
        overview += "\n"
        overview += self._build_model_insights(ranking, evaluation_details)

        fusion_effectiveness = quality_analysis.get('fusion_effectiveness')
        if fusion_effectiveness:
            overview += "\n" + self._build_fusion_effectiveness(fusion_effectiveness)

        return overview

    def _build_quality_table(self, ranking: List[Dict[str, Any]], llm_evaluations: Dict[str, Any]) -> str:
        """æ„å»ºè´¨é‡è¯„åˆ†è¡¨æ ¼"""
        header = "| æ¨¡å‹ | ç»¼åˆ | å®Œæ•´æ€§ | å‡†ç¡®æ€§ | æ¸…æ™°åº¦ | ç›¸å…³æ€§ | å­—ç¬¦æ•° |\n"
        header += "| --- | --- | --- | --- | --- | --- | --- |\n"

        rows = []
        for entry in ranking:
            metrics = llm_evaluations.get(entry['source'])
            char_count = getattr(metrics, "char_count", entry.get('word_count', 0))
            rows.append(
                f"| {entry['source']} | {entry['overall_score']:.1f} | "
                f"{entry['completeness']:.1f} | {entry['accuracy']:.1f} | "
                f"{entry['clarity']:.1f} | {entry['relevance']:.1f} | {char_count} |"
            )

        return header + "\n".join(rows) + "\n"

    def _build_model_insights(
        self,
        ranking: List[Dict[str, Any]],
        evaluation_details: Dict[str, Dict[str, Any]]
    ) -> str:
        """æ±‡æ€»æ¨¡å‹çš„ä¼˜åŠ¿ä¸æ”¹è¿›æ–¹å‘"""
        if not evaluation_details:
            return ""

        insight_text = "### ğŸ“ˆ æ¨¡å‹è¡¨ç°æ´å¯Ÿ\n\n"
        for entry in ranking:
            source = entry['source']
            details = evaluation_details.get(source)
            if not details:
                continue

            strengths = []
            weaknesses = []
            for dimension in ('completeness', 'accuracy', 'clarity', 'relevance'):
                strengths.extend(details.get(dimension, {}).get('strengths', []))
                weaknesses.extend(details.get(dimension, {}).get('weaknesses', []))

            unique = details.get('unique_characteristics', "")
            suggestions = details.get('core_suggestions', [])

            insight_text += f"#### {source}\n\n"
            insight_text += f"- **ç‰¹å¾æ‘˜è¦**: {unique or '_æš‚æ— æè¿°_'}\n"
            insight_text += f"- **ä¸»è¦ä¼˜åŠ¿**: {self._format_list(strengths)}\n"
            insight_text += f"- **ä¸»è¦ä¸è¶³**: {self._format_list(weaknesses)}\n"
            if suggestions:
                insight_text += f"- **æ”¹è¿›å»ºè®®**: {self._format_list(suggestions)}\n"
            insight_text += "\n"

        return insight_text

    def _build_fusion_effectiveness(self, fusion_effectiveness: Dict[str, Any]) -> str:
        """æ¸²æŸ“èåˆæ•ˆæœåˆ†æ"""
        summary = fusion_effectiveness.get('summary')
        dimension_improvements = fusion_effectiveness.get('dimension_improvements', {})
        value = fusion_effectiveness.get('fusion_value_score')

        section = "### ğŸ¤ èåˆæ•ˆæœ\n\n"
        if summary:
            section += f"- **ç»¼åˆç»“è®º**: {summary}\n"

        if dimension_improvements:
            improvements = []
            for dim, stats in dimension_improvements.items():
                delta = stats.get('average_improvement')
                if delta is not None:
                    improvements.append(f"{dim}: {delta:+.2f}")
            if improvements:
                section += f"- **ç»´åº¦æå‡**: {'ï¼›'.join(improvements)}\n"

        if value is not None:
            section += f"- **èåˆä»·å€¼è¯„åˆ†**: {value:.1f}/100\n"

        return section + "\n"

    def _generate_speed_quality_section(self, quality_analysis: Optional[Dict[str, Any]]) -> str:
        """ç”Ÿæˆé€Ÿåº¦-è´¨é‡æƒè¡¡åˆ†æéƒ¨åˆ†"""
        if not quality_analysis:
            return ""

        tradeoff = quality_analysis.get('speed_quality_tradeoff')
        if not tradeoff or not tradeoff.get('available'):
            return ""

        section = "## â±ï¸ é€Ÿåº¦-è´¨é‡æƒè¡¡\n\n"

        fastest = tradeoff.get('fastest_model')
        quality = tradeoff.get('highest_quality_model')
        efficient = tradeoff.get('most_efficient_model')

        if fastest:
            section += (
                f"- **æœ€å¿«å“åº”æ¨¡å‹**: {fastest['name']} "
                f"(è€—æ—¶ {fastest['response_time']:.2f}sï¼Œè´¨é‡ {fastest['quality_score']:.1f})\n"
            )
        if quality:
            section += (
                f"- **è´¨é‡æœ€ä½³æ¨¡å‹**: {quality['name']} "
                f"(è€—æ—¶ {quality['response_time']:.2f}sï¼Œè´¨é‡ {quality['quality_score']:.1f})\n"
            )
        if efficient and efficient.get('efficiency_score') is not None:
            section += (
                f"- **æ€§ä»·æ¯”æœ€ä½³**: {efficient['name']} "
                f"(æ•ˆç‡ {efficient['efficiency_score']:.2f}ï¼Œè´¨é‡ {efficient['quality_score']:.1f})\n"
            )

        correlation = tradeoff.get('correlation_analysis')
        if correlation:
            section += f"- **é€Ÿåº¦ä¸è´¨é‡å…³ç³»**: {correlation.get('description', 'æš‚æ— ç»“è®º')}\n"

        recommendations = tradeoff.get('scenario_recommendations', [])
        if recommendations:
            section += "\n**åœºæ™¯åŒ–å»ºè®®ï¼š**\n"
            if isinstance(recommendations, dict):
                items = list(recommendations.items())
                for key, value in items[:3]:
                    label = {
                        'time_critical': "æ—¶é—´æ•æ„Ÿ",
                        'quality_critical': "è´¨é‡ä¼˜å…ˆ",
                        'balanced': "ç»¼åˆå¹³è¡¡",
                        'production': "ç”Ÿäº§ç¯å¢ƒ"
                    }.get(key, key)
                    section += f"- {label}: {value}\n"
            else:
                for rec in recommendations[:3]:
                    section += f"- {rec}\n"

        return section + "\n"

    def print_summary(self, llm_responses: List[Dict], final_answer: str, quality_analysis: Optional[Dict[str, Any]] = None):
        """æ‰“å°ç®€è¦æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š AI Fusion åˆ†ææ‘˜è¦")
        print("="*60)

        for i, response in enumerate(llm_responses, 1):
            if response['success']:
                print(f"ğŸ¤– æ¨¡å‹ {i}: {response['model_name']}")
                print(f"   â±ï¸  å“åº”æ—¶é—´: {response.get('response_time', 0):.2f}ç§’")
                print(f"   ğŸ“ å›ç­”é•¿åº¦: {len(response['response'])}å­—ç¬¦")
                print(f"   âœ… çŠ¶æ€: æˆåŠŸ")
            else:
                print(f"âŒ æ¨¡å‹ {i}: {response['model_name']} - å¤±è´¥")
                print(f"   â±ï¸  å“åº”æ—¶é—´: {response.get('response_time', 0):.2f}ç§’")
                print(f"   ğŸ“ é”™è¯¯: {response.get('error', 'Unknown')}")
            print()

        print(f"ğŸ¯ èåˆå›ç­”é•¿åº¦: {len(final_answer)}å­—ç¬¦")
        print("="*60)
